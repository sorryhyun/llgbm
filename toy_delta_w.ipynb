{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta-W: Effective Weight Update Supervision\n",
    "\n",
    "Supervise the generator on **DW = B @ A * scaling** instead of raw (A, B) matrices.\n",
    "\n",
    "**Why?** Raw weight matching suffers from gauge ambiguity: many (A, B) pairs produce\n",
    "the same effective update. DW is the canonical representation â€” gauge-invariant and\n",
    "cheap to compute (matrix multiply only, no forward pass needed).\n",
    "\n",
    "```\n",
    "Loss = MSE(DW_pred, DW_teacher)\n",
    "where DW = B @ A * (alpha / rank)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    !pip install -q safetensors accelerate transformers peft sentence-transformers\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/llgbm/checkpoints'\n",
    "    DELTAS_DIR = CHECKPOINT_DIR + '/deltas'\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    DELTAS_DIR = './llgbm/deltas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "from llgbm import (\n",
    "    TrainingConfig,\n",
    "    DeltaWLoss,\n",
    "    MultiTaskLoss,\n",
    "    create_generator,\n",
    "    RealAdapterDataset,\n",
    "    FunctionalLoRA,\n",
    "    train,\n",
    "    evaluate,\n",
    ")\n",
    "from llgbm.ablations import setup_base_components, AblationConfig\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    use_small_model=True,\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_steps=200,\n",
    "    warmup_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    lambda_weight=1.0,\n",
    "    lambda_delta=0.0,  # No hidden-state delta needed\n",
    "    num_probes=10,\n",
    "    max_probe_length=256,\n",
    "    delta_batch_probes=True,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    delta_cache_dir=DELTAS_DIR,\n",
    "    output_dir=\"outputs/toy_delta_w\",\n",
    "    text_encoder_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    freeze_text_encoder=True,\n",
    "    num_prompts_per_adapter=8,\n",
    ")\n",
    "\n",
    "TORCH_DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[config.dtype]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "config.save(f\"{config.output_dir}/config.json\")\n",
    "\n",
    "print(f\"Model: {config.base_model}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Loss: DeltaW (MSE on B@A*scaling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup shared components (base model, probes, dataset, etc.)\n",
    "ablation_config = AblationConfig(\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    deltas_dir=DELTAS_DIR,\n",
    "    output_dir=config.output_dir,\n",
    "    use_small_model=config.use_small_model,\n",
    "    batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    num_probes=config.num_probes,\n",
    "    max_probe_length=config.max_probe_length,\n",
    "    delta_batch_probes=config.delta_batch_probes,\n",
    "    text_encoder_name=config.text_encoder_name,\n",
    "    freeze_text_encoder=config.freeze_text_encoder,\n",
    "    num_prompts_per_adapter=config.num_prompts_per_adapter,\n",
    ")\n",
    "components = setup_base_components(ablation_config, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator + text encoder\n",
    "generator = create_generator(\n",
    "    config,\n",
    "    seed=42,\n",
    "    device=device,\n",
    "    text_encoder=components[\"text_encoder\"],\n",
    ")\n",
    "print(f\"[OK] Generator: {sum(p.numel() for p in generator.parameters() if p.requires_grad):,} trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeltaWLoss + optimizer + scheduler\n",
    "criterion = MultiTaskLoss(lambda_weight=1.0, lambda_delta=0.0)\n",
    "weight_criterion = DeltaWLoss(\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_rank=config.lora_rank,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(generator.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "warmup_steps = min(config.warmup_steps, config.num_steps // 10)\n",
    "cosine_steps = max(1, config.num_steps - warmup_steps)\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    [LinearLR(optimizer, 0.1, 1.0, warmup_steps),\n",
    "     CosineAnnealingLR(optimizer, cosine_steps, config.learning_rate * 0.01)],\n",
    "    [warmup_steps]\n",
    ")\n",
    "\n",
    "print(f\"[OK] DeltaWLoss(scaling={weight_criterion.scaling})\")\n",
    "print(f\"[OK] Optimizer & Scheduler ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    components[\"dataset\"],\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=components[\"dataset\"].collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Train\n",
    "state = train(\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    functional_lora=components[\"functional_lora\"],\n",
    "    base_activation=components[\"base_activation\"],\n",
    "    probe_tokens=components[\"probe_tokens\"],\n",
    "    probe_masks=components[\"probe_masks\"],\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    compute_dtype=TORCH_DTYPE,\n",
    "    weight_criterion=weight_criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (delta cosine similarity)\n",
    "eval_dataloader = DataLoader(\n",
    "    components[\"dataset\"],\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=components[\"dataset\"].collate_fn,\n",
    ")\n",
    "\n",
    "# Use MultiTaskLoss with lambda_delta=1 for eval (needs delta computation)\n",
    "eval_criterion = MultiTaskLoss(lambda_weight=0.0, lambda_delta=1.0)\n",
    "eval_results = evaluate(\n",
    "    generator=generator,\n",
    "    dataloader=eval_dataloader,\n",
    "    functional_lora=components[\"functional_lora\"],\n",
    "    base_activation=components[\"base_activation\"],\n",
    "    probe_tokens=components[\"probe_tokens\"],\n",
    "    probe_masks=components[\"probe_masks\"],\n",
    "    criterion=eval_criterion,\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if state.loss_history:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    axes[0].plot(state.loss_history, color='#2c3e50', linewidth=1.5)\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('DeltaW Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(state.grad_norm_history, color='#9b59b6', linewidth=1.5)\n",
    "    axes[1].axhline(config.max_grad_norm, color='r', ls='--', label=f'clip={config.max_grad_norm}')\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_ylabel('Gradient Norm')\n",
    "    axes[1].set_title('Gradient Norms')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].plot(state.lr_history, color='#f39c12', linewidth=1.5)\n",
    "    axes[2].set_xlabel('Step')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('LR Schedule')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/training_curves.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Task Accuracy Evaluation\n\nGenerate LoRA weights for each task and measure downstream accuracy on held-out eval sets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport gc\nfrom llgbm.evaluation import compute_accuracy_with_lora_batched, compute_base_accuracy\n\n# Discover tasks from the adapter dataset (what we actually trained on)\ndataset = components[\"dataset\"]\ntokenizer = components[\"tokenizer\"]\nadapter_tasks = sorted(set(s[\"task\"] for s in dataset.samples))\nprint(f\"Adapter tasks: {adapter_tasks}\")\n\n# Auto-discover eval files from data/ directory\nDATA_DIR = Path(\"data\") if not IN_COLAB else Path(\"/content/drive/MyDrive/llgbm/data\")\n\n# Infer task type from filename/content\nKNOWN_BOOL_TASKS = {\"boolq\"}\nKNOWN_NUMERIC_TASKS = {\"gsm8k\"}\n\ndef infer_task_type(task_name):\n    t = task_name.lower().replace(\"-\", \"\").replace(\"_\", \"\")\n    if t in {\"boolq\"}:\n        return \"bool\"\n    if t in {\"gsm8k\"}:\n        return \"gsm8k\"\n    return \"mcq\"\n\n# Find eval files matching adapter tasks\neval_data = {}\nif DATA_DIR.exists():\n    eval_files = {f.stem.lower(): f for f in DATA_DIR.glob(\"*_eval.json\")}\n    print(f\"Available eval files: {list(eval_files.keys())}\")\n\n    for task in adapter_tasks:\n        # Try matching: \"arc_e\" -> \"arc-e_eval\", \"boolq\" -> \"boolq_eval\", etc.\n        variants = [\n            task.lower() + \"_eval\",\n            task.lower().replace(\"_\", \"-\") + \"_eval\",\n            task.lower().replace(\"-\", \"_\") + \"_eval\",\n        ]\n        for v in variants:\n            if v in eval_files:\n                with open(eval_files[v]) as f:\n                    samples = json.load(f)\n                task_type = infer_task_type(task)\n                eval_data[task] = (samples, task_type)\n                print(f\"  {task}: {len(samples)} samples ({task_type})\")\n                break\n        else:\n            print(f\"  {task}: [no eval file found]\")\n\nif not eval_data:\n    print(\"[SKIP] No eval data found for any adapter task\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compute task accuracy: generated LoRA vs base model\nMAX_EVAL_SAMPLES = 100\n\naccuracy_results = {}\n\nif eval_data:\n    generator.eval()\n\n    # Base model accuracy\n    print(\"=\" * 60)\n    print(\"BASE MODEL (no LoRA)\")\n    print(\"=\" * 60)\n    base_accuracy = {}\n    for task, (samples, task_type) in eval_data.items():\n        res = compute_base_accuracy(\n            base_model=components[\"base_model\"],\n            eval_samples=samples,\n            tokenizer=tokenizer,\n            task_type=task_type,\n            max_samples=MAX_EVAL_SAMPLES,\n            show_progress=False,\n        )\n        base_accuracy[task] = res\n        print(f\"  {task}: {res['accuracy']:.2%} ({res['correct']}/{res['total']})\")\n\n    # Generated LoRA accuracy\n    print(\"\\n\" + \"=\" * 60)\n    print(\"GENERATED LoRA (delta_w)\")\n    print(\"=\" * 60)\n    gen_accuracy = {}\n    for task, (samples, task_type) in eval_data.items():\n        # Use first adapter of this task as conditioning input\n        task_idx = next(i for i, s in enumerate(dataset.samples) if s[\"task\"] == task)\n        sample = dataset[task_idx]\n        condition_ids = sample[\"condition_ids\"]\n        attention_mask = sample[\"attention_mask\"]\n\n        res = compute_accuracy_with_lora_batched(\n            generator=generator,\n            functional_lora=components[\"functional_lora\"],\n            condition_ids=condition_ids,\n            attention_mask=attention_mask,\n            eval_samples=samples,\n            tokenizer=tokenizer,\n            task_type=task_type,\n            max_samples=MAX_EVAL_SAMPLES,\n            batch_size=8,\n            show_progress=False,\n        )\n        gen_accuracy[task] = res\n        print(f\"  {task}: {res['accuracy']:.2%} ({res['correct']}/{res['total']})\")\n\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    # Summary table\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"{'Task':<14} {'Base':>8} {'Generated':>10} {'Delta':>8}\")\n    print(\"-\" * 42)\n    for task in eval_data:\n        base_acc = base_accuracy[task][\"accuracy\"]\n        gen_acc = gen_accuracy.get(task, {}).get(\"accuracy\", 0)\n        delta = gen_acc - base_acc\n        sign = \"+\" if delta >= 0 else \"\"\n        print(f\"  {task:<12} {base_acc:>7.1%} {gen_acc:>9.1%} {sign}{delta:>7.1%}\")\n    print(\"=\" * 60)\n\n    accuracy_results = {\n        \"base\": {t: r for t, r in base_accuracy.items()},\n        \"generated\": {t: r for t, r in gen_accuracy.items()},\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results\nimport json\nfrom dataclasses import asdict\n\nresults = {\n    \"config\": asdict(config),\n    \"training\": {\n        \"steps\": len(state.loss_history),\n        \"best_loss\": state.best_loss,\n        \"final_loss\": state.loss_history[-1] if state.loss_history else None,\n    },\n    \"eval\": eval_results,\n    \"accuracy\": accuracy_results,\n    \"mode\": \"delta_w\",\n}\n\nwith open(f\"{config.output_dir}/results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"Saved to {config.output_dir}/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Google Drive (Colab only)\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    drive_dir = f\"{DRIVE_OUTPUT_DIR}/toy_delta_w\"\n",
    "    if os.path.exists(drive_dir):\n",
    "        shutil.rmtree(drive_dir)\n",
    "    shutil.copytree(config.output_dir, drive_dir)\n",
    "    print(f\"[Drive] Synced to {drive_dir}\")\n",
    "else:\n",
    "    print(f\"[Local] Outputs saved to {config.output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}