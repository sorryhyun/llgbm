{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 4: Multi-Task Training (Weights + Deltas)\n",
    "\n",
    "Train a LoRA generator with both weight supervision and behavioral (delta) supervision.\n",
    "\n",
    "**Core Equation:** `Loss = λ_w * L_weight + λ_d * L_delta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    !pip install -q safetensors accelerate transformers peft\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import llgbm modules\n",
    "from llgbm import (\n",
    "    create_generic_probes,\n",
    "    DeltaCache,\n",
    "    FunctionalLoRA,\n",
    "    TrainingConfig,\n",
    "    MultiTaskLoss,\n",
    "    train,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "print(\"[OK] llgbm imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig(\n",
    "    use_small_model=True,  # Qwen2.5-0.5B for testing\n",
    "    batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=100,  # Short run for testing\n",
    "    warmup_steps=10,  # Must be < max_steps\n",
    "    lambda_delta=0.1,\n",
    "    lambda_weight=1.0,\n",
    "    output_dir=\"outputs/phase4_multitask\",\n",
    ")\n",
    "\n",
    "TORCH_DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[config.dtype]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "config.save(f\"{config.output_dir}/config.json\")\n",
    "\n",
    "print(f\"Model: {config.base_model}\")\n",
    "print(f\"Loss: {config.lambda_weight}*L_w + {config.lambda_delta}*L_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Base Model & Prepare Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model, torch_dtype=TORCH_DTYPE, device_map=device, trust_remote_code=True\n",
    ")\n",
    "base_model.config.output_hidden_states = True\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"[OK] Base model: {sum(p.numel() for p in base_model.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize probes\n",
    "probes = create_generic_probes()[:config.num_probes]\n",
    "probe_tokens = []\n",
    "probe_masks = []\n",
    "for p in probes:\n",
    "    enc = tokenizer(p, return_tensors=\"pt\", truncation=True, max_length=config.max_probe_length)\n",
    "    probe_tokens.append(enc[\"input_ids\"].to(device))\n",
    "    probe_masks.append(enc[\"attention_mask\"].to(device))\n",
    "\n",
    "# Compute base activation\n",
    "with torch.no_grad():\n",
    "    base_acts = []\n",
    "    for ids, mask in zip(probe_tokens, probe_masks):\n",
    "        out = base_model(input_ids=ids, attention_mask=mask, output_hidden_states=True)\n",
    "        h = out.hidden_states[-1][:, int(mask.sum()) - 1, :].squeeze(0)\n",
    "        base_acts.append(h)\n",
    "    base_activation = torch.stack(base_acts).mean(dim=0)\n",
    "\n",
    "print(f\"[OK] Base activation: {base_activation.shape}, norm={base_activation.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_lora = FunctionalLoRA(\n",
    "    base_model=base_model,\n",
    "    lora_rank=config.lora_rank,\n",
    "    lora_alpha=config.lora_alpha,\n",
    ")\n",
    "print(f\"[OK] FunctionalLoRA: {len(functional_lora._lora_to_base_map)} mappings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Placeholder Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderGenerator(nn.Module):\n",
    "    \"\"\"Simple generator that outputs LoRA weights from text condition.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(50000, 256)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=256, nhead=4, batch_first=True), num_layers=2\n",
    "        )\n",
    "        self.proj = nn.Linear(256, cfg.num_layers * 7 * 2)  # 7 modules, 2 scales (A, B)\n",
    "    \n",
    "    def forward(self, condition_ids, attention_mask=None):\n",
    "        B = condition_ids.shape[0]\n",
    "        x = self.embed(condition_ids)\n",
    "        mask = ~attention_mask.bool() if attention_mask is not None else None\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        if attention_mask is not None:\n",
    "            x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1)\n",
    "        else:\n",
    "            x = x.mean(1)\n",
    "        scales = self.proj(x).view(B, self.cfg.num_layers * 7, 2)\n",
    "        \n",
    "        # Generate LoRA weights\n",
    "        batch_weights = []\n",
    "        for b in range(B):\n",
    "            weights = {}\n",
    "            idx = 0\n",
    "            for layer in range(self.cfg.num_layers):\n",
    "                for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "                    sa, sb = scales[b, idx, 0], scales[b, idx, 1]\n",
    "                    prefix = f\"model.layers.{layer}\"\n",
    "                    mod = \"self_attn\" if proj in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"] else \"mlp\"\n",
    "                    \n",
    "                    if proj in [\"k_proj\", \"v_proj\"]:\n",
    "                        out_d = self.cfg.num_kv_heads * (self.cfg.hidden_size // self.cfg.num_heads)\n",
    "                    elif proj in [\"gate_proj\", \"up_proj\"]:\n",
    "                        out_d = self.cfg.intermediate_size\n",
    "                    elif proj == \"down_proj\":\n",
    "                        out_d = self.cfg.hidden_size\n",
    "                    else:\n",
    "                        out_d = self.cfg.hidden_size\n",
    "                    \n",
    "                    in_d = self.cfg.intermediate_size if proj == \"down_proj\" else self.cfg.hidden_size\n",
    "                    \n",
    "                    A = torch.randn(self.cfg.lora_rank, in_d, device=condition_ids.device) * 0.01 * sa\n",
    "                    B_ = torch.randn(out_d, self.cfg.lora_rank, device=condition_ids.device) * 0.001 * sb\n",
    "                    weights[f\"{prefix}.{mod}.{proj}.lora_A.weight\"] = A\n",
    "                    weights[f\"{prefix}.{mod}.{proj}.lora_B.weight\"] = B_\n",
    "                    idx += 1\n",
    "            batch_weights.append(weights)\n",
    "        return batch_weights\n",
    "\n",
    "generator = PlaceholderGenerator(config).to(device)\n",
    "print(f\"[OK] Generator: {sum(p.numel() for p in generator.parameters() if p.requires_grad):,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "# Create sample data if needed\n",
    "checkpoint_dir = Path(config.checkpoint_dir)\n",
    "delta_cache = DeltaCache(config.delta_cache_dir)\n",
    "\n",
    "if not list(checkpoint_dir.rglob(\"adapter_config.json\")) or delta_cache.summary().get('count', 0) == 0:\n",
    "    print(\"Creating sample data...\")\n",
    "    for name, domain in [(\"math_001\", \"math\"), (\"code_001\", \"code\"), (\"general_001\", \"general\")]:\n",
    "        path = checkpoint_dir / domain / name\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Minimal adapter\n",
    "        torch.manual_seed(hash(name) % 10000)\n",
    "        weights = {f\"layer.0.lora_A\": torch.randn(8, 256) * 0.01}\n",
    "        save_file(weights, path / \"adapter_model.safetensors\")\n",
    "        json.dump({\"r\": 8, \"peft_type\": \"LORA\"}, open(path / \"adapter_config.json\", \"w\"))\n",
    "        json.dump({\"prompts\": [f\"Solve {domain} problem\"]}, open(path / \"prompts.json\", \"w\"))\n",
    "        delta_cache.save_delta(str(path), torch.randn(config.hidden_size).numpy() * 0.1)\n",
    "    delta_cache.save_base_activation(np.zeros(config.hidden_size), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, checkpoint_dir, delta_cache, tokenizer, hidden_size):\n",
    "        all_deltas = delta_cache.get_all_deltas()\n",
    "        self.samples = [\n",
    "            str(p.parent) for p in Path(checkpoint_dir).rglob(\"adapter_config.json\")\n",
    "            if str(p.parent) in all_deltas\n",
    "        ]\n",
    "        self.deltas = all_deltas\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.samples[idx]\n",
    "        prompts_file = Path(path) / \"prompts.json\"\n",
    "        text = json.load(open(prompts_file)).get(\"prompts\", [Path(path).name])[0] if prompts_file.exists() else Path(path).name\n",
    "        enc = self.tokenizer(text, max_length=256, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"condition_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"delta_teacher\": torch.from_numpy(self.deltas[path]).float(),\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = SimpleDataset(config.checkpoint_dir, delta_cache, text_tokenizer, config.hidden_size)\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "print(f\"[OK] Dataset: {len(dataset)} samples, {len(dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MultiTaskLoss(lambda_weight=config.lambda_weight, lambda_delta=config.lambda_delta)\n",
    "\n",
    "optimizer = AdamW(generator.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "# Ensure T_max >= 1 to avoid division by zero\n",
    "cosine_steps = max(1, config.max_steps - config.warmup_steps)\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    [LinearLR(optimizer, 0.1, 1.0, config.warmup_steps),\n",
    "     CosineAnnealingLR(optimizer, cosine_steps, config.learning_rate * 0.01)],\n",
    "    [config.warmup_steps]\n",
    ")\n",
    "\n",
    "print(f\"[OK] Optimizer & Scheduler ready (warmup={config.warmup_steps}, cosine={cosine_steps})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(f\"\\nTraining: {config.max_steps} steps, batch={config.batch_size}x{config.gradient_accumulation_steps}\")\n",
    "\n",
    "state = train(\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    functional_lora=functional_lora,\n",
    "    base_activation=base_activation,\n",
    "    probe_tokens=probe_tokens,\n",
    "    probe_masks=probe_masks,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    compute_dtype=TORCH_DTYPE,\n",
    ")\n",
    "\n",
    "print(f\"\\nDone! Steps: {state.step}, Best loss: {state.best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_results = evaluate(\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    functional_lora=functional_lora,\n",
    "    base_activation=base_activation,\n",
    "    probe_tokens=probe_tokens,\n",
    "    probe_masks=probe_masks,\n",
    "    criterion=criterion,\n",
    ")\n",
    "print(\"Evaluation:\", {k: f\"{v:.4f}\" for k, v in eval_results.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "if state.loss_history:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax[0].plot(state.loss_history, label='Total')\n",
    "    ax[0].plot(state.loss_delta_history, label='Delta', alpha=0.7)\n",
    "    ax[0].set_xlabel('Step'); ax[0].set_ylabel('Loss'); ax[0].legend()\n",
    "    ax[1].plot(state.grad_norm_history)\n",
    "    ax[1].axhline(config.max_grad_norm, color='r', ls='--')\n",
    "    ax[1].set_xlabel('Step'); ax[1].set_ylabel('Grad Norm')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/curves.png\", dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"config\": asdict(config),\n",
    "    \"training\": {\"steps\": state.step, \"best_loss\": state.best_loss},\n",
    "    \"eval\": eval_results,\n",
    "}\n",
    "json.dump(results, open(f\"{config.output_dir}/results.json\", \"w\"), indent=2)\n",
    "print(f\"Saved to {config.output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Google Drive (Colab only)\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    drive_phase4_dir = f\"{DRIVE_OUTPUT_DIR}/phase4_multitask\"\n",
    "    if os.path.exists(drive_phase4_dir):\n",
    "        shutil.rmtree(drive_phase4_dir)\n",
    "    shutil.copytree(config.output_dir, drive_phase4_dir)\n",
    "    print(f\"[Drive] Synced to {drive_phase4_dir}\")\n",
    "else:\n",
    "    print(\"[Local] Outputs saved to\", config.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.12.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
