{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Phase 5: Delta-Guided Training with Delta Prediction Head\n\nTrain a LoRA generator using **behavioral supervision** with a dedicated delta prediction head.\n\n**Architecture:**\n```\nN embeddings ─→ [Shared Encoder] ─┬─→ [LoRA Head] → LoRA weights → δ_computed\n                                  └─→ [Delta Head] → δ_predicted (fast!)\n```\n\n**Loss Function:**\n```\nLoss = λ_pred * L(δ_predicted, δ_teacher)        # Fast delta supervision\n     + λ_computed * L(δ_computed, δ_teacher)     # Real LoRA behavior  \n     + λ_consistency * L(δ_computed, δ_predicted) # Heads must agree\n```\n\n## Key Features\n\n1. **Trainable text encoder** - Frozen MiniLM-L6-v2 + learnable projection\n2. **Delta prediction head** - Predicts delta directly from N embeddings (fast!)\n3. **Attention aggregation** - Learns which prompts matter most for delta\n4. **Consistency loss** - LoRA must produce deltas matching predictions\n5. **Per-embedding deltas** - No averaging, uses all N prompt embeddings"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    !pip install -q safetensors accelerate transformers peft sentence-transformers\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/llgbm/checkpoints'\n",
    "    DELTAS_DIR = CHECKPOINT_DIR + '/deltas'\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    DELTAS_DIR = './llgbm/deltas'\n",
    "\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Import llgbm modules\nfrom llgbm import (\n    create_generic_probes,\n    DeltaCache,\n    FunctionalLoRA,\n    TrainingConfig,\n    DeltaGuidedLoss,  # NEW: 3-part loss with consistency\n    compute_delta_for_batch,\n    save_checkpoint,\n    load_checkpoint,\n    # Trainable text encoder\n    create_trainable_text_encoder,\n    # Generator with delta head\n    create_generator_with_delta_head,\n    RealAdapterDataset,\n    # Evaluation\n    compute_base_eval_loss,\n    compute_accuracy_with_lora_batched,\n)\n\nprint(\"[OK] llgbm imports\")\nprint(\"[INFO] Using LoRAGeneratorWithDeltaHead (dual-head architecture)\")\nprint(\"[INFO] Delta prediction + LoRA generation with consistency loss\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Key difference from Phase 4: `lambda_weight=0` (delta-only supervision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    use_small_model=True,  # Qwen2.5-0.5B for testing\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_steps=200,  # More steps for delta-only training\n",
    "    warmup_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    lambda_delta=1.0,   # Full weight on delta loss\n",
    "    lambda_weight=0.0,  # No weight supervision\n",
    "    num_probes=10,\n",
    "    max_probe_length=256,\n",
    "    delta_batch_probes=True,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    delta_cache_dir=DELTAS_DIR,\n",
    "    output_dir=\"outputs/phase5_delta_only\",\n",
    "    # Text encoder settings\n",
    "    text_encoder_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    freeze_text_encoder=True,\n",
    "    num_prompts_per_adapter=8,\n",
    ")\n",
    "\n",
    "# Derived settings\n",
    "TORCH_DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[config.dtype]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "config.save(f\"{config.output_dir}/config.json\")\n",
    "\n",
    "print(f\"Model: {config.base_model}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Loss: Delta-only (lambda_d={config.lambda_delta}, lambda_w={config.lambda_weight})\")\n",
    "print(f\"Text encoder: {config.text_encoder_name}\")\n",
    "print(f\"Prompts per adapter: {config.num_prompts_per_adapter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Base Model & Prepare Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.config.output_hidden_states = False\n",
    "base_model.config.use_cache = False\n",
    "base_model.eval()\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"[OK] Base model: {sum(p.numel() for p in base_model.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probes from delta manifest (or use generic)\n",
    "checkpoint_dir = Path(CHECKPOINT_DIR)\n",
    "deltas_dir = Path(DELTAS_DIR)\n",
    "\n",
    "delta_manifest_path = deltas_dir / \"delta_manifest.json\"\n",
    "manifest_path = checkpoint_dir / \"manifest.json\"\n",
    "\n",
    "all_probes = []\n",
    "if delta_manifest_path.exists() and manifest_path.exists():\n",
    "    with open(delta_manifest_path) as f:\n",
    "        delta_manifest = json.load(f)\n",
    "    with open(manifest_path) as f:\n",
    "        adapter_manifest = json.load(f)\n",
    "    \n",
    "    adapter_paths = {a[\"name\"]: a[\"path\"] for a in adapter_manifest.get(\"adapters\", [])}\n",
    "    tasks_seen = set()\n",
    "    \n",
    "    for adapter_name, adapter_info in delta_manifest[\"adapters\"].items():\n",
    "        task = adapter_info.get(\"task\", \"unknown\")\n",
    "        if task not in tasks_seen:\n",
    "            remaining = max(0, config.num_probes - len(all_probes))\n",
    "            if remaining == 0:\n",
    "                break\n",
    "            adapter_path = adapter_paths.get(adapter_name)\n",
    "            if adapter_path:\n",
    "                prompts_file = Path(adapter_path) / \"prompts.json\"\n",
    "                if prompts_file.exists():\n",
    "                    with open(prompts_file) as f:\n",
    "                        prompts_data = json.load(f)\n",
    "                    probes = prompts_data.get(\"prompts\", [])[:min(5, remaining)]\n",
    "                    if probes:\n",
    "                        all_probes.extend(probes)\n",
    "                        tasks_seen.add(task)\n",
    "                        print(f\"  Loaded {len(probes)} probes for {task}\")\n",
    "\n",
    "if not all_probes:\n",
    "    print(\"[WARN] No task-specific probes, using generic\")\n",
    "    all_probes = create_generic_probes()[:config.num_probes]\n",
    "\n",
    "print(f\"[OK] {len(all_probes)} probes loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize probes\n",
    "probe_tokens, probe_masks = [], []\n",
    "for p in all_probes:\n",
    "    enc = tokenizer(p, return_tensors=\"pt\", truncation=True, max_length=config.max_probe_length)\n",
    "    probe_tokens.append(enc[\"input_ids\"].to(device))\n",
    "    probe_masks.append(enc[\"attention_mask\"].to(device))\n",
    "\n",
    "# Compute base activation\n",
    "with torch.no_grad():\n",
    "    base_acts = []\n",
    "    for ids, mask in zip(probe_tokens, probe_masks):\n",
    "        backbone = getattr(base_model, \"model\", None)\n",
    "        if backbone is not None:\n",
    "            out = backbone(input_ids=ids, attention_mask=mask, use_cache=False)\n",
    "            hidden = out.last_hidden_state\n",
    "        else:\n",
    "            out = base_model(input_ids=ids, attention_mask=mask, output_hidden_states=True, use_cache=False)\n",
    "            hidden = out.hidden_states[-1]\n",
    "        seq_lens = mask.long().sum(dim=1).clamp(min=1) - 1\n",
    "        batch_idx = torch.arange(hidden.shape[0], device=hidden.device)\n",
    "        h = hidden[batch_idx, seq_lens, :].squeeze(0)\n",
    "        base_acts.append(h)\n",
    "    base_activation = torch.stack(base_acts).mean(dim=0)\n",
    "\n",
    "print(f\"[OK] Base activation: {base_activation.shape}, norm={base_activation.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FunctionalLoRA wrapper\n",
    "functional_lora = FunctionalLoRA(\n",
    "    base_model=base_model,\n",
    "    lora_rank=config.lora_rank,\n",
    "    lora_alpha=config.lora_alpha,\n",
    ")\n",
    "print(f\"[OK] FunctionalLoRA: {len(functional_lora._lora_to_base_map)} mappings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Trainable Text Encoder & Generator with Delta Head\n\n**Text Encoder**: Frozen MiniLM-L6-v2 + trainable 2-layer projection MLP\n\n**Generator**: Dual-head architecture:\n- **LoRA Head**: Generates adapter weights (for deployment)\n- **Delta Head**: Predicts behavioral delta (for fast training)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Create TRAINABLE text encoder (projection layer is trainable)\nprint(\"[1] Loading trainable text encoder...\")\ntext_encoder = create_trainable_text_encoder(\n    model_name=config.text_encoder_name,\n    output_dim=384,  # Keep same dimension as input\n    hidden_dim=512,  # Projection MLP hidden dim\n    num_layers=2,    # 2-layer MLP projection\n    dropout=0.1,\n    device=device,\n)\n\n# Note: text_encoder.projection parameters will be trained!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Create generator with delta prediction head\nprint(\"[2] Creating generator with delta head...\")\ngenerator = create_generator_with_delta_head(\n    config,\n    seed=42,\n    device=device,\n    text_encoder=text_encoder,\n    delta_aggregation=\"attention\",  # Use attention over N embeddings\n)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with real adapters and prompt batches\n",
    "print(\"[3] Loading dataset with prompt batches...\")\n",
    "dataset = RealAdapterDataset(\n",
    "    checkpoint_dir=str(checkpoint_dir),\n",
    "    deltas_dir=str(deltas_dir),\n",
    "    tokenizer=text_encoder.tokenizer,\n",
    "    config=config,\n",
    "    num_prompts=config.num_prompts_per_adapter,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"[OK] Dataset: {len(dataset)} samples, {len(dataloader)} batches\")\n",
    "print(f\"     {config.num_prompts_per_adapter} prompts per adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Training with Delta-Only Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Delta-guided loss with 3 components\ncriterion = DeltaGuidedLoss(\n    lambda_pred=1.0,        # Predicted delta vs teacher\n    lambda_computed=1.0,    # Computed delta (via LoRA) vs teacher\n    lambda_consistency=0.5, # Predicted vs computed consistency\n    normalize=True,\n    loss_type=\"mse\",\n)\n\n# Collect all trainable parameters\ntrainable_params = [\n    {\"params\": generator.parameters(), \"lr\": config.learning_rate},\n    {\"params\": text_encoder.projection.parameters(), \"lr\": config.learning_rate * 0.5},\n]\n\n# Optimizer with parameter groups\noptimizer = AdamW(trainable_params, weight_decay=config.weight_decay)\n\n# LR scheduler with warmup\nwarmup_steps = min(config.warmup_steps, config.num_steps // 10)\ncosine_steps = max(1, config.num_steps - warmup_steps)\nscheduler = SequentialLR(\n    optimizer,\n    [LinearLR(optimizer, 0.1, 1.0, warmup_steps),\n     CosineAnnealingLR(optimizer, cosine_steps, config.learning_rate * 0.01)],\n    [warmup_steps]\n)\n\n# Count trainable parameters\ngen_params = sum(p.numel() for p in generator.parameters() if p.requires_grad)\nenc_params = sum(p.numel() for p in text_encoder.projection.parameters() if p.requires_grad)\ndelta_head_params = sum(p.numel() for p in generator.delta_head.parameters() if p.requires_grad)\n\nprint(\"[OK] Optimizer & Scheduler ready\")\nprint(f\"     Generator total: {gen_params:,}\")\nprint(f\"     - Delta head: {delta_head_params:,}\")\nprint(f\"     - LoRA head: {gen_params - delta_head_params:,}\")\nprint(f\"     Encoder projection: {enc_params:,}\")\nprint(f\"     Total trainable: {gen_params + enc_params:,}\")\nprint(f\"[OK] Using DeltaGuidedLoss (3-part: pred + computed + consistency)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Custom training loop for dual-head generator\nfrom tqdm.auto import tqdm\nfrom itertools import cycle\nfrom llgbm import compute_delta_differentiable\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training: {config.num_steps} steps\")\nprint(f\"Batch size: {config.batch_size} x {config.gradient_accumulation_steps}\")\nprint(f\"Mode: Delta-guided (prediction + computed + consistency)\")\nprint(f\"{'='*60}\\n\")\n\ngenerator.train()\ndata_iter = cycle(dataloader)\npbar = tqdm(total=config.num_steps, desc=\"Training\")\n\n# Training state\nloss_history = []\nloss_pred_history = []\nloss_computed_history = []\nloss_consistency_history = []\ngrad_norm_history = []\nlr_history = []\nbest_loss = float(\"inf\")\n\naccumulation_step = 0\nrunning_losses = {\"loss\": 0, \"loss_pred\": 0, \"loss_computed\": 0, \"loss_consistency\": 0}\nupdate_count = 0\n\nwhile update_count < config.num_steps:\n    batch = next(data_iter)\n    \n    # Move to device\n    delta_teacher = batch[\"delta_teacher\"].to(device)\n    condition_ids = batch[\"condition_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    \n    # Forward pass through generator (returns both delta_pred and lora_weights)\n    with torch.autocast(device_type=\"cuda\" if device.type == \"cuda\" else \"cpu\", dtype=TORCH_DTYPE):\n        results = generator(condition_ids, attention_mask, return_delta=True, return_lora=True)\n        \n        delta_predicted = results[\"delta_pred\"]\n        lora_weights_batch = results[\"lora_weights\"]\n        \n        # Compute actual delta from LoRA weights (expensive but necessary for consistency)\n        deltas_computed = []\n        for i in range(len(lora_weights_batch)):\n            delta_i = compute_delta_differentiable(\n                functional_lora=functional_lora,\n                lora_weights=lora_weights_batch[i],\n                base_activation=base_activation,\n                probe_tokens=probe_tokens,\n                probe_masks=probe_masks,\n                batch_probes=True,\n            )\n            deltas_computed.append(delta_i)\n        delta_computed = torch.stack(deltas_computed)\n        \n        # Compute 3-part loss\n        losses = criterion(\n            delta_predicted=delta_predicted.float(),\n            delta_computed=delta_computed.float(),\n            delta_teacher=delta_teacher.float(),\n        )\n    \n    # Backward\n    scaled_loss = losses[\"loss\"] / config.gradient_accumulation_steps\n    scaled_loss.backward()\n    \n    # Accumulate losses\n    for k in running_losses:\n        if k in losses:\n            running_losses[k] += losses[k].item()\n    accumulation_step += 1\n    \n    # Gradient update\n    if accumulation_step >= config.gradient_accumulation_steps:\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            list(generator.parameters()) + list(text_encoder.projection.parameters()),\n            config.max_grad_norm,\n        )\n        \n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        # Record metrics\n        avg_losses = {k: v / accumulation_step for k, v in running_losses.items()}\n        loss_history.append(avg_losses[\"loss\"])\n        loss_pred_history.append(avg_losses.get(\"loss_pred\", 0))\n        loss_computed_history.append(avg_losses.get(\"loss_computed\", 0))\n        loss_consistency_history.append(avg_losses.get(\"loss_consistency\", 0))\n        grad_norm_history.append(grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm)\n        lr_history.append(scheduler.get_last_lr()[0])\n        \n        # Track best\n        if avg_losses[\"loss\"] < best_loss:\n            best_loss = avg_losses[\"loss\"]\n            torch.save({\n                \"generator_state_dict\": generator.state_dict(),\n                \"text_encoder_projection_state_dict\": text_encoder.projection.state_dict(),\n            }, f\"{config.output_dir}/checkpoint_best.pt\")\n        \n        # Reset\n        running_losses = {k: 0 for k in running_losses}\n        accumulation_step = 0\n        update_count += 1\n        \n        pbar.set_postfix({\n            \"loss\": f\"{avg_losses['loss']:.4f}\",\n            \"pred\": f\"{avg_losses.get('loss_pred', 0):.4f}\",\n            \"cons\": f\"{avg_losses.get('loss_consistency', 0):.4f}\",\n        })\n        pbar.update(1)\n        \n        # Memory cleanup\n        if update_count % 50 == 0:\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\npbar.close()\nprint(f\"\\nDone! Steps: {update_count}, Best loss: {best_loss:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate using delta prediction (fast) and computed delta (accurate)\nimport torch.nn.functional as F\n\ngenerator.eval()\neval_dataloader = DataLoader(\n    dataset,\n    batch_size=config.batch_size,\n    shuffle=False,\n    collate_fn=dataset.collate_fn\n)\n\ntotal_loss = 0\ncosines_pred = []\ncosines_computed = []\nnum_samples = 0\n\nwith torch.no_grad():\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        delta_teacher = batch[\"delta_teacher\"].to(device)\n        condition_ids = batch[\"condition_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        \n        results = generator(condition_ids, attention_mask, return_delta=True, return_lora=True)\n        delta_predicted = results[\"delta_pred\"]\n        lora_weights_batch = results[\"lora_weights\"]\n        \n        # Compute actual delta\n        deltas_computed = []\n        for lora_w in lora_weights_batch:\n            delta_i = compute_delta_differentiable(\n                functional_lora=functional_lora,\n                lora_weights=lora_w,\n                base_activation=base_activation,\n                probe_tokens=probe_tokens,\n                probe_masks=probe_masks,\n                batch_probes=True,\n            )\n            deltas_computed.append(delta_i)\n        delta_computed = torch.stack(deltas_computed)\n        \n        # Cosine similarities\n        cos_pred = F.cosine_similarity(delta_predicted, delta_teacher, dim=-1)\n        cos_computed = F.cosine_similarity(delta_computed, delta_teacher, dim=-1)\n        \n        cosines_pred.extend(cos_pred.cpu().tolist())\n        cosines_computed.extend(cos_computed.cpu().tolist())\n        num_samples += len(delta_teacher)\n\neval_results = {\n    \"mean_cosine_pred\": float(np.mean(cosines_pred)),\n    \"std_cosine_pred\": float(np.std(cosines_pred)),\n    \"mean_cosine_computed\": float(np.mean(cosines_computed)),\n    \"std_cosine_computed\": float(np.std(cosines_computed)),\n}\n\nprint(\"\\nEvaluation Results:\")\nprint(f\"  Delta Predicted vs Teacher:\")\nprint(f\"    mean cosine: {eval_results['mean_cosine_pred']:.4f} +/- {eval_results['std_cosine_pred']:.4f}\")\nprint(f\"  Delta Computed (via LoRA) vs Teacher:\")\nprint(f\"    mean cosine: {eval_results['mean_cosine_computed']:.4f} +/- {eval_results['std_cosine_computed']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Plot training curves with all loss components\nif loss_history:\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Total loss\n    axes[0, 0].plot(loss_history, label='Total', color='#2c3e50', linewidth=2)\n    axes[0, 0].set_xlabel('Step')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].set_title('Total Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Loss components\n    axes[0, 1].plot(loss_pred_history, label='Predicted→Teacher', color='#3498db', linewidth=1.5)\n    axes[0, 1].plot(loss_computed_history, label='Computed→Teacher', color='#e74c3c', linewidth=1.5)\n    axes[0, 1].plot(loss_consistency_history, label='Consistency', color='#2ecc71', linewidth=1.5)\n    axes[0, 1].set_xlabel('Step')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].set_title('Loss Components')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Gradient norm\n    axes[1, 0].plot(grad_norm_history, color='#9b59b6', linewidth=1.5)\n    axes[1, 0].axhline(config.max_grad_norm, color='r', ls='--', label=f'Clip={config.max_grad_norm}')\n    axes[1, 0].set_xlabel('Step')\n    axes[1, 0].set_ylabel('Gradient Norm')\n    axes[1, 0].set_title('Gradient Norms')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Learning rate\n    if lr_history:\n        axes[1, 1].plot(lr_history, color='#f39c12', linewidth=2)\n        axes[1, 1].set_xlabel('Step')\n        axes[1, 1].set_ylabel('Learning Rate')\n        axes[1, 1].set_title('LR Schedule')\n        axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f\"{config.output_dir}/training_curves.png\", dpi=150)\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Save results\nresults = {\n    \"config\": asdict(config),\n    \"training\": {\n        \"steps\": len(loss_history),\n        \"best_loss\": best_loss,\n        \"final_loss\": loss_history[-1] if loss_history else None,\n        \"final_loss_pred\": loss_pred_history[-1] if loss_pred_history else None,\n        \"final_loss_computed\": loss_computed_history[-1] if loss_computed_history else None,\n        \"final_loss_consistency\": loss_consistency_history[-1] if loss_consistency_history else None,\n    },\n    \"eval\": eval_results,\n    \"mode\": \"delta_guided\",\n    \"architecture\": \"LoRAGeneratorWithDeltaHead\",\n}\n\nwith open(f\"{config.output_dir}/results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"Saved to {config.output_dir}/\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Task Performance Evaluation\n",
    "\n",
    "Evaluate generated LoRAs on downstream tasks using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task configuration\n",
    "TASKS = [\"arc_e\", \"arc_c\", \"boolq\", \"obqa\", \"piqa\", \"winogrande\"]\n",
    "TASK_TYPES = {\n",
    "    \"arc_e\": \"mcq\",\n",
    "    \"arc_c\": \"mcq\",\n",
    "    \"boolq\": \"bool\",\n",
    "    \"obqa\": \"mcq\",\n",
    "    \"piqa\": \"mcq\",\n",
    "    \"winogrande\": \"mcq\",\n",
    "}\n",
    "\n",
    "# Load eval data\n",
    "print(\"[1] Loading eval data...\")\n",
    "eval_splits_dir = checkpoint_dir / \"eval_splits\"\n",
    "eval_data = {}\n",
    "for task in TASKS:\n",
    "    eval_file = eval_splits_dir / f\"{task}_eval.json\"\n",
    "    if eval_file.exists():\n",
    "        with open(eval_file) as f:\n",
    "            eval_data[task] = json.load(f)\n",
    "        print(f\"    {task}: {len(eval_data[task])} samples\")\n",
    "    else:\n",
    "        print(f\"    {task}: [NOT FOUND]\")\n",
    "\n",
    "if not eval_data:\n",
    "    print(\"[SKIP] No eval data found, skipping task evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "if eval_data:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASE MODEL (no LoRA)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    base_results = {}\n",
    "    for task, samples in eval_data.items():\n",
    "        loss = compute_base_eval_loss(base_model, samples[:50], tokenizer)\n",
    "        base_results[task] = loss\n",
    "        print(f\"  {task}: loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate delta-guided model on downstream tasks\nif eval_data:\n    print(\"\\n\" + \"=\"*60)\n    print(\"DELTA-GUIDED MODEL (LoRAGeneratorWithDeltaHead)\")\n    print(\"=\"*60)\n    \n    # Load best checkpoint\n    best_checkpoint = Path(config.output_dir) / \"checkpoint_best.pt\"\n    if best_checkpoint.exists():\n        ckpt = torch.load(best_checkpoint, map_location=device)\n        generator.load_state_dict(ckpt[\"generator_state_dict\"])\n        if \"text_encoder_projection_state_dict\" in ckpt:\n            text_encoder.projection.load_state_dict(ckpt[\"text_encoder_projection_state_dict\"])\n        print(f\"[OK] Loaded checkpoint: {best_checkpoint}\")\n    \n    generator.eval()\n    delta_guided_results = {}\n    \n    for task, samples in eval_data.items():\n        # Find adapter for this task\n        task_indices = [i for i, s in enumerate(dataset.samples) if s.get(\"task\") == task]\n        if not task_indices:\n            print(f\"  {task}: [SKIP] No adapter found\")\n            continue\n        \n        sample = dataset[task_indices[0]]\n        condition_ids = sample[\"condition_ids\"].unsqueeze(0).to(device)\n        attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n        \n        try:\n            # Generate LoRA weights\n            with torch.no_grad():\n                lora_weights = generator.generate_lora(condition_ids, attention_mask)[0]\n            \n            # Apply LoRA and evaluate\n            functional_lora.apply_lora_weights(lora_weights)\n            \n            correct = 0\n            total = 0\n            for s in samples[:100]:\n                # Simple eval: check if model produces expected output\n                prompt = s.get(\"question\", s.get(\"prompt\", \"\"))\n                expected = s.get(\"answer\", s.get(\"label\", \"\"))\n                \n                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n                with torch.no_grad():\n                    outputs = base_model.generate(\n                        **inputs, max_new_tokens=32, do_sample=False, pad_token_id=tokenizer.pad_token_id\n                    )\n                response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n                \n                if str(expected).lower() in response.lower():\n                    correct += 1\n                total += 1\n            \n            functional_lora.remove_lora_weights()\n            \n            acc = correct / total if total > 0 else 0\n            delta_guided_results[task] = acc\n            print(f\"  {task}: {acc:.2%} ({correct}/{total})\")\n            \n        except Exception as e:\n            print(f\"  {task}: [ERROR] {e}\")\n            functional_lora.remove_lora_weights()\n    \n    # Clean up\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Comparison with Phase 4.5 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phase 4.5 results for comparison\n",
    "phase45_results_path = Path(\"outputs/phase4_5_ablations/ablation_results.json\")\n",
    "\n",
    "if phase45_results_path.exists():\n",
    "    with open(phase45_results_path) as f:\n",
    "        phase45_data = json.load(f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Comparison: Phase 4.5 vs Phase 5\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract Phase 4.5 delta_only results\n",
    "    p45_summary = phase45_data.get(\"summary\", {})\n",
    "    \n",
    "    print(\"\\nPhase 4.5 Ablations Summary:\")\n",
    "    for config_name, stats in p45_summary.items():\n",
    "        loss_mean = stats.get(\"final_loss_mean\", \"N/A\")\n",
    "        loss_std = stats.get(\"final_loss_std\", 0)\n",
    "        cos_mean = stats.get(\"mean_cosine_mean\", \"N/A\")\n",
    "        cos_std = stats.get(\"mean_cosine_std\", 0)\n",
    "        \n",
    "        if isinstance(loss_mean, float):\n",
    "            print(f\"  {config_name:12s}: loss={loss_mean:.4f}+/-{loss_std:.4f}, cos={cos_mean:.4f}+/-{cos_std:.4f}\")\n",
    "    \n",
    "    print(\"\\nPhase 5 (Extended Delta-Only):\")\n",
    "    print(f\"  final_loss: {state.loss_history[-1]:.4f}\")\n",
    "    print(f\"  mean_cosine: {eval_results.get('mean_cosine', 'N/A')}\")\n",
    "    \n",
    "    # Compare delta_only specifically\n",
    "    if \"delta_only\" in p45_summary:\n",
    "        p45_delta = p45_summary[\"delta_only\"]\n",
    "        p5_loss = state.loss_history[-1] if state.loss_history else None\n",
    "        p5_cosine = eval_results.get(\"mean_cosine\")\n",
    "        \n",
    "        print(\"\\nDelta-Only Comparison:\")\n",
    "        print(f\"  Phase 4.5 (100 steps): loss={p45_delta.get('final_loss_mean', 'N/A'):.4f}\")\n",
    "        print(f\"  Phase 5 ({config.num_steps} steps): loss={p5_loss:.4f}\")\n",
    "        if p5_cosine is not None:\n",
    "            print(f\"  Cosine improvement: {p5_cosine - p45_delta.get('mean_cosine_mean', 0):.4f}\")\n",
    "else:\n",
    "    print(\"[Info] Phase 4.5 results not found. Run phase_4_5_ablations.ipynb first for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Sync to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Google Drive (Colab only)\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    drive_phase5_dir = f\"{DRIVE_OUTPUT_DIR}/phase5_delta_only\"\n",
    "    if os.path.exists(drive_phase5_dir):\n",
    "        shutil.rmtree(drive_phase5_dir)\n",
    "    shutil.copytree(config.output_dir, drive_phase5_dir)\n",
    "    print(f\"[Drive] Synced to {drive_phase5_dir}\")\n",
    "else:\n",
    "    print(f\"[Local] Outputs saved to {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": "## Analysis: Delta-Guided Training Insights\n\nKey advantages of the dual-head architecture:\n\n1. **Fast training signal** - Delta predictor provides immediate supervision without LoRA application\n2. **No embedding averaging** - Uses attention over N embeddings (learns which prompts matter)\n3. **Consistency constraint** - LoRA head must produce deltas matching predictions\n4. **Per-embedding supervision** - Can supervise individual prompt→delta mappings\n\n**Expected behavior:**\n- `loss_pred` should decrease quickly (fast learning path)\n- `loss_computed` should follow (LoRA produces correct behavior)\n- `loss_consistency` should stay low (heads agree)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": "# Final summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"Phase 5 Complete!\")\nprint(\"=\"*60)\nprint(f\"\\nArchitecture: LoRAGeneratorWithDeltaHead\")\nprint(f\"Training mode: Delta-guided (3-part loss)\")\nprint(f\"Total steps: {len(loss_history)}\")\nprint(f\"Best loss: {best_loss:.6f}\")\n\nif loss_history:\n    print(f\"\\nFinal losses:\")\n    print(f\"  Total: {loss_history[-1]:.6f}\")\n    print(f\"  Predicted→Teacher: {loss_pred_history[-1]:.6f}\")\n    print(f\"  Computed→Teacher: {loss_computed_history[-1]:.6f}\")\n    print(f\"  Consistency: {loss_consistency_history[-1]:.6f}\")\n\nprint(f\"\\nEvaluation (cosine similarity):\")\nprint(f\"  Delta Predicted: {eval_results['mean_cosine_pred']:.4f} +/- {eval_results['std_cosine_pred']:.4f}\")\nprint(f\"  Delta Computed:  {eval_results['mean_cosine_computed']:.4f} +/- {eval_results['std_cosine_computed']:.4f}\")\n\nif eval_data and delta_guided_results:\n    print(f\"\\nTask Performance:\")\n    for task, acc in delta_guided_results.items():\n        print(f\"  {task}: {acc:.2%}\")\n\nprint(f\"\\nOutputs: {config.output_dir}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}