{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 4.5: Ablation Studies\n",
    "\n",
    "Run multiple trials across different configurations to compare:\n",
    "1. **Multi-task** (λ_w=1.0, λ_d=0.1)\n",
    "2. **Delta-only** (λ_w=0.0, λ_d=1.0)\n",
    "3. **Weight-only** (λ_w=1.0, λ_d=0.0) - baseline\n",
    "\n",
    "Each configuration runs 3 trials with different seeds for statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    !pip install -q safetensors accelerate transformers peft\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llgbm import (\n",
    "    create_generic_probes,\n",
    "    DeltaCache,\n",
    "    FunctionalLoRA,\n",
    "    TrainingConfig,\n",
    "    MultiTaskLoss,\n",
    "    DeltaOnlyLoss,\n",
    "    train,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "print(\"[OK] llgbm imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation configurations\n",
    "CONFIGS = {\n",
    "    \"multitask\": {\"lambda_weight\": 1.0, \"lambda_delta\": 0.1},\n",
    "    \"delta_only\": {\"lambda_weight\": 0.0, \"lambda_delta\": 1.0},\n",
    "    \"weight_only\": {\"lambda_weight\": 1.0, \"lambda_delta\": 0.0},\n",
    "}\n",
    "\n",
    "NUM_TRIALS = 3\n",
    "SEEDS = [42, 123, 456]\n",
    "MAX_STEPS = 100  # Short runs for ablation\n",
    "\n",
    "OUTPUT_DIR = Path(\"outputs/phase4_5_ablations\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configurations: {list(CONFIGS.keys())}\")\n",
    "print(f\"Trials per config: {NUM_TRIALS}\")\n",
    "print(f\"Total runs: {len(CONFIGS) * NUM_TRIALS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Setup (shared across runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base config\n",
    "base_config = TrainingConfig(\n",
    "    use_small_model=True,\n",
    "    batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=MAX_STEPS,\n",
    "    warmup_steps=10,\n",
    ")\n",
    "\n",
    "TORCH_DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[base_config.dtype]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Model: {base_config.base_model}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_config.base_model, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_config.base_model, torch_dtype=TORCH_DTYPE, device_map=device, trust_remote_code=True\n",
    ")\n",
    "base_model.config.output_hidden_states = True\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"[OK] Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probes & base activation (shared)\n",
    "probes = create_generic_probes()[:base_config.num_probes]\n",
    "probe_tokens, probe_masks = [], []\n",
    "for p in probes:\n",
    "    enc = tokenizer(p, return_tensors=\"pt\", truncation=True, max_length=base_config.max_probe_length)\n",
    "    probe_tokens.append(enc[\"input_ids\"].to(device))\n",
    "    probe_masks.append(enc[\"attention_mask\"].to(device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_acts = []\n",
    "    for ids, mask in zip(probe_tokens, probe_masks):\n",
    "        out = base_model(input_ids=ids, attention_mask=mask, output_hidden_states=True)\n",
    "        h = out.hidden_states[-1][:, int(mask.sum()) - 1, :].squeeze(0)\n",
    "        base_acts.append(h)\n",
    "    base_activation = torch.stack(base_acts).mean(dim=0)\n",
    "\n",
    "functional_lora = FunctionalLoRA(base_model, base_config.lora_rank, base_config.lora_alpha)\n",
    "print(f\"[OK] Probes & FunctionalLoRA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (shared)\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "checkpoint_dir = Path(base_config.checkpoint_dir)\n",
    "delta_cache = DeltaCache(base_config.delta_cache_dir)\n",
    "\n",
    "if not list(checkpoint_dir.rglob(\"adapter_config.json\")) or delta_cache.summary().get('count', 0) == 0:\n",
    "    print(\"Creating sample data...\")\n",
    "    for name, domain in [(\"math_001\", \"math\"), (\"code_001\", \"code\"), (\"general_001\", \"general\")]:\n",
    "        path = checkpoint_dir / domain / name\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        torch.manual_seed(hash(name) % 10000)\n",
    "        weights = {f\"layer.0.lora_A\": torch.randn(8, 256) * 0.01}\n",
    "        save_file(weights, path / \"adapter_model.safetensors\")\n",
    "        json.dump({\"r\": 8, \"peft_type\": \"LORA\"}, open(path / \"adapter_config.json\", \"w\"))\n",
    "        json.dump({\"prompts\": [f\"Solve {domain} problem\"]}, open(path / \"prompts.json\", \"w\"))\n",
    "        delta_cache.save_delta(str(path), torch.randn(base_config.hidden_size).numpy() * 0.1)\n",
    "    delta_cache.save_base_activation(np.zeros(base_config.hidden_size), {})\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, checkpoint_dir, delta_cache, tokenizer, hidden_size):\n",
    "        all_deltas = delta_cache.get_all_deltas()\n",
    "        self.samples = [str(p.parent) for p in Path(checkpoint_dir).rglob(\"adapter_config.json\") if str(p.parent) in all_deltas]\n",
    "        self.deltas = all_deltas\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_size = hidden_size\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.samples[idx]\n",
    "        pf = Path(path) / \"prompts.json\"\n",
    "        text = json.load(open(pf)).get(\"prompts\", [Path(path).name])[0] if pf.exists() else Path(path).name\n",
    "        enc = self.tokenizer(text, max_length=256, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        return {\"condition_ids\": enc[\"input_ids\"].squeeze(0), \"attention_mask\": enc[\"attention_mask\"].squeeze(0), \"delta_teacher\": torch.from_numpy(self.deltas[path]).float()}\n",
    "    @staticmethod\n",
    "    def collate_fn(batch): return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = SimpleDataset(base_config.checkpoint_dir, delta_cache, text_tokenizer, base_config.hidden_size)\n",
    "print(f\"[OK] Dataset: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Generator Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderGenerator(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(50000, 256)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=256, nhead=4, batch_first=True), num_layers=2\n",
    "        )\n",
    "        self.proj = nn.Linear(256, cfg.num_layers * 7 * 2)\n",
    "    \n",
    "    def forward(self, condition_ids, attention_mask=None):\n",
    "        B = condition_ids.shape[0]\n",
    "        x = self.embed(condition_ids)\n",
    "        mask = ~attention_mask.bool() if attention_mask is not None else None\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        if attention_mask is not None:\n",
    "            x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1)\n",
    "        else:\n",
    "            x = x.mean(1)\n",
    "        scales = self.proj(x).view(B, self.cfg.num_layers * 7, 2)\n",
    "        \n",
    "        batch_weights = []\n",
    "        for b in range(B):\n",
    "            weights = {}\n",
    "            idx = 0\n",
    "            for layer in range(self.cfg.num_layers):\n",
    "                for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "                    sa, sb = scales[b, idx, 0], scales[b, idx, 1]\n",
    "                    prefix = f\"model.layers.{layer}\"\n",
    "                    mod = \"self_attn\" if proj in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"] else \"mlp\"\n",
    "                    if proj in [\"k_proj\", \"v_proj\"]: out_d = self.cfg.num_kv_heads * (self.cfg.hidden_size // self.cfg.num_heads)\n",
    "                    elif proj in [\"gate_proj\", \"up_proj\"]: out_d = self.cfg.intermediate_size\n",
    "                    elif proj == \"down_proj\": out_d = self.cfg.hidden_size\n",
    "                    else: out_d = self.cfg.hidden_size\n",
    "                    in_d = self.cfg.intermediate_size if proj == \"down_proj\" else self.cfg.hidden_size\n",
    "                    A = torch.randn(self.cfg.lora_rank, in_d, device=condition_ids.device) * 0.01 * sa\n",
    "                    B_ = torch.randn(out_d, self.cfg.lora_rank, device=condition_ids.device) * 0.001 * sb\n",
    "                    weights[f\"{prefix}.{mod}.{proj}.lora_A.weight\"] = A\n",
    "                    weights[f\"{prefix}.{mod}.{proj}.lora_B.weight\"] = B_\n",
    "                    idx += 1\n",
    "            batch_weights.append(weights)\n",
    "        return batch_weights\n",
    "\n",
    "def create_generator(cfg, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    return PlaceholderGenerator(cfg).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Run Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(config_name: str, lambda_weight: float, lambda_delta: float, seed: int, trial_idx: int) -> Dict[str, Any]:\n",
    "    \"\"\"Run a single trial and return results.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config: {config_name} | Trial {trial_idx+1}/{NUM_TRIALS} | Seed: {seed}\")\n",
    "    print(f\"λ_w={lambda_weight}, λ_d={lambda_delta}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create config for this trial\n",
    "    config = TrainingConfig(\n",
    "        use_small_model=True,\n",
    "        batch_size=base_config.batch_size,\n",
    "        gradient_accumulation_steps=base_config.gradient_accumulation_steps,\n",
    "        max_steps=MAX_STEPS,\n",
    "        warmup_steps=10,\n",
    "        lambda_weight=lambda_weight,\n",
    "        lambda_delta=lambda_delta,\n",
    "        output_dir=str(OUTPUT_DIR / f\"{config_name}_trial{trial_idx}\"),\n",
    "    )\n",
    "    Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Fresh generator\n",
    "    generator = create_generator(config, seed)\n",
    "    \n",
    "    # Loss function\n",
    "    if lambda_weight == 0:\n",
    "        criterion = DeltaOnlyLoss()\n",
    "    else:\n",
    "        criterion = MultiTaskLoss(lambda_weight=lambda_weight, lambda_delta=lambda_delta)\n",
    "    \n",
    "    # Optimizer & scheduler\n",
    "    optimizer = AdamW(generator.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    cosine_steps = max(1, config.max_steps - config.warmup_steps)\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        [LinearLR(optimizer, 0.1, 1.0, config.warmup_steps),\n",
    "         CosineAnnealingLR(optimizer, cosine_steps, config.learning_rate * 0.01)],\n",
    "        [config.warmup_steps]\n",
    "    )\n",
    "    \n",
    "    # Dataloader (fresh for each trial)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    state = train(\n",
    "        generator=generator,\n",
    "        dataloader=dataloader,\n",
    "        functional_lora=functional_lora,\n",
    "        base_activation=base_activation,\n",
    "        probe_tokens=probe_tokens,\n",
    "        probe_masks=probe_masks,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        config=config,\n",
    "        compute_dtype=TORCH_DTYPE,\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = evaluate(\n",
    "        generator=generator,\n",
    "        dataloader=dataloader,\n",
    "        functional_lora=functional_lora,\n",
    "        base_activation=base_activation,\n",
    "        probe_tokens=probe_tokens,\n",
    "        probe_masks=probe_masks,\n",
    "        criterion=criterion,\n",
    "    )\n",
    "    \n",
    "    # Cleanup\n",
    "    del generator, optimizer, scheduler\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    result = {\n",
    "        \"config_name\": config_name,\n",
    "        \"trial\": trial_idx,\n",
    "        \"seed\": seed,\n",
    "        \"lambda_weight\": lambda_weight,\n",
    "        \"lambda_delta\": lambda_delta,\n",
    "        \"final_loss\": state.loss_history[-1] if state.loss_history else None,\n",
    "        \"best_loss\": state.best_loss,\n",
    "        \"train_time\": train_time,\n",
    "        **eval_results,\n",
    "    }\n",
    "    \n",
    "    print(f\"Result: loss={result['final_loss']:.4f}, cosine_sim={result.get('cosine_sim', 'N/A')}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all trials\n",
    "all_results = []\n",
    "\n",
    "for config_name, params in CONFIGS.items():\n",
    "    for trial_idx, seed in enumerate(SEEDS[:NUM_TRIALS]):\n",
    "        result = run_trial(\n",
    "            config_name=config_name,\n",
    "            lambda_weight=params[\"lambda_weight\"],\n",
    "            lambda_delta=params[\"lambda_delta\"],\n",
    "            seed=seed,\n",
    "            trial_idx=trial_idx,\n",
    "        )\n",
    "        all_results.append(result)\n",
    "\n",
    "print(f\"\\n\\nCompleted {len(all_results)} trials!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Aggregate by config\n",
    "summary = df.groupby(\"config_name\").agg({\n",
    "    \"final_loss\": [\"mean\", \"std\"],\n",
    "    \"best_loss\": [\"mean\", \"std\"],\n",
    "    \"cosine_sim\": [\"mean\", \"std\"] if \"cosine_sim\" in df.columns else [],\n",
    "    \"train_time\": [\"mean\"],\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION SUMMARY (mean ± std over 3 trials)\")\n",
    "print(\"=\"*70)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "configs = list(CONFIGS.keys())\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "# Final Loss comparison\n",
    "means = [df[df['config_name']==c]['final_loss'].mean() for c in configs]\n",
    "stds = [df[df['config_name']==c]['final_loss'].std() for c in configs]\n",
    "axes[0].bar(configs, means, yerr=stds, color=colors, capsize=5, alpha=0.8)\n",
    "axes[0].set_ylabel('Final Loss')\n",
    "axes[0].set_title('Final Loss by Configuration')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cosine similarity comparison (if available)\n",
    "if 'cosine_sim' in df.columns:\n",
    "    means = [df[df['config_name']==c]['cosine_sim'].mean() for c in configs]\n",
    "    stds = [df[df['config_name']==c]['cosine_sim'].std() for c in configs]\n",
    "    axes[1].bar(configs, means, yerr=stds, color=colors, capsize=5, alpha=0.8)\n",
    "    axes[1].set_ylabel('Cosine Similarity')\n",
    "    axes[1].set_title('Delta Cosine Similarity by Configuration')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Cosine sim not computed', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"ablation_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df.to_csv(OUTPUT_DIR / \"all_trials.csv\", index=False)\n",
    "summary.to_csv(OUTPUT_DIR / \"summary.csv\")\n",
    "\n",
    "final_results = {\n",
    "    \"configs\": CONFIGS,\n",
    "    \"num_trials\": NUM_TRIALS,\n",
    "    \"seeds\": SEEDS[:NUM_TRIALS],\n",
    "    \"summary\": {c: {\n",
    "        \"final_loss_mean\": float(df[df['config_name']==c]['final_loss'].mean()),\n",
    "        \"final_loss_std\": float(df[df['config_name']==c]['final_loss'].std()),\n",
    "        \"cosine_sim_mean\": float(df[df['config_name']==c]['cosine_sim'].mean()) if 'cosine_sim' in df.columns else None,\n",
    "        \"cosine_sim_std\": float(df[df['config_name']==c]['cosine_sim'].std()) if 'cosine_sim' in df.columns else None,\n",
    "    } for c in configs},\n",
    "    \"all_trials\": all_results,\n",
    "}\n",
    "json.dump(final_results, open(OUTPUT_DIR / \"ablation_results.json\", \"w\"), indent=2)\n",
    "print(f\"Saved to {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Drive\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    drive_dir = f\"{DRIVE_OUTPUT_DIR}/phase4_5_ablations\"\n",
    "    if os.path.exists(drive_dir):\n",
    "        shutil.rmtree(drive_dir)\n",
    "    shutil.copytree(str(OUTPUT_DIR), drive_dir)\n",
    "    print(f\"[Drive] Synced to {drive_dir}\")\n",
    "else:\n",
    "    print(\"[Local] Outputs saved to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Phase 4.5 Ablations Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nKey findings:\")\n",
    "for config_name in configs:\n",
    "    mean_loss = df[df['config_name']==config_name]['final_loss'].mean()\n",
    "    std_loss = df[df['config_name']==config_name]['final_loss'].std()\n",
    "    print(f\"  {config_name:12s}: {mean_loss:.4f} ± {std_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
