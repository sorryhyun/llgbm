{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Phase 4.5: Ablation Studies\n\n**Prerequisites:** Run `train_lora_adapters.ipynb` first to create real LoRA adapters and deltas.\n\nThis notebook compares different training configurations:\n1. **Multi-task** (λ_w=1.0, λ_d=0.1) - Both weight and delta supervision\n2. **Delta-only** (λ_w=0.0, λ_d=1.0) - Behavioral supervision only\n3. **Weight-only** (λ_w=1.0, λ_d=0.0) - Traditional DnD baseline\n\nEach configuration runs 3 trials with different seeds for statistical significance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    !pip install -q safetensors accelerate transformers peft\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llgbm import (\n",
    "    create_generic_probes,\n",
    "    DeltaCache,\n",
    "    FunctionalLoRA,\n",
    "    TrainingConfig,\n",
    "    MultiTaskLoss,\n",
    "    DeltaOnlyLoss,\n",
    "    train,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "print(\"[OK] llgbm imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation configurations\n",
    "CONFIGS = {\n",
    "    \"multitask\": {\"lambda_weight\": 1.0, \"lambda_delta\": 0.1},\n",
    "    \"delta_only\": {\"lambda_weight\": 0.0, \"lambda_delta\": 1.0},\n",
    "    \"weight_only\": {\"lambda_weight\": 1.0, \"lambda_delta\": 0.0},\n",
    "}\n",
    "\n",
    "NUM_TRIALS = 3\n",
    "SEEDS = [42, 123, 456]\n",
    "MAX_STEPS = 100  # Short runs for ablation\n",
    "\n",
    "OUTPUT_DIR = Path(\"outputs/phase4_5_ablations\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configurations: {list(CONFIGS.keys())}\")\n",
    "print(f\"Trials per config: {NUM_TRIALS}\")\n",
    "print(f\"Total runs: {len(CONFIGS) * NUM_TRIALS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Setup (shared across runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base config\n",
    "base_config = TrainingConfig(\n",
    "    use_small_model=True,\n",
    "    batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=MAX_STEPS,\n",
    "    warmup_steps=10,\n",
    ")\n",
    "\n",
    "TORCH_DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[base_config.dtype]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Model: {base_config.base_model}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_config.base_model, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_config.base_model, torch_dtype=TORCH_DTYPE, device_map=device, trust_remote_code=True\n",
    ")\n",
    "base_model.config.output_hidden_states = True\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"[OK] Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probes & base activation (shared)\n",
    "probes = create_generic_probes()[:base_config.num_probes]\n",
    "probe_tokens, probe_masks = [], []\n",
    "for p in probes:\n",
    "    enc = tokenizer(p, return_tensors=\"pt\", truncation=True, max_length=base_config.max_probe_length)\n",
    "    probe_tokens.append(enc[\"input_ids\"].to(device))\n",
    "    probe_masks.append(enc[\"attention_mask\"].to(device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_acts = []\n",
    "    for ids, mask in zip(probe_tokens, probe_masks):\n",
    "        out = base_model(input_ids=ids, attention_mask=mask, output_hidden_states=True)\n",
    "        h = out.hidden_states[-1][:, int(mask.sum()) - 1, :].squeeze(0)\n",
    "        base_acts.append(h)\n",
    "    base_activation = torch.stack(base_acts).mean(dim=0)\n",
    "\n",
    "functional_lora = FunctionalLoRA(base_model, base_config.lora_rank, base_config.lora_alpha)\n",
    "print(f\"[OK] Probes & FunctionalLoRA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Dataset - Load real adapters and deltas from checkpoints/\nfrom safetensors.torch import load_file\n\ncheckpoint_dir = Path(base_config.checkpoint_dir)\ndeltas_dir = checkpoint_dir / \"deltas\"\n\n# Check if real data exists\nmanifest_path = checkpoint_dir / \"manifest.json\"\ndelta_manifest_path = deltas_dir / \"delta_manifest.json\"\n\nif manifest_path.exists() and delta_manifest_path.exists():\n    print(\"[OK] Found real adapter data\")\n    with open(manifest_path) as f:\n        adapter_manifest = json.load(f)\n    with open(delta_manifest_path) as f:\n        delta_manifest = json.load(f)\n    \n    # Load base activation\n    base_act_file = deltas_dir / delta_manifest[\"base_activation_file\"]\n    cached_base_activation = np.load(base_act_file)\n    print(f\"  Adapters: {len(adapter_manifest['adapters'])}\")\n    print(f\"  Deltas: {len(delta_manifest['adapters'])}\")\nelse:\n    print(\"[WARNING] No real data found. Run train_lora_adapters.ipynb first!\")\n    print(\"  Creating minimal fake data for testing...\")\n    adapter_manifest = {\"adapters\": []}\n    delta_manifest = {\"adapters\": {}}\n    cached_base_activation = None\n\n\nclass RealAdapterDataset(Dataset):\n    \"\"\"Dataset that loads real LoRA adapters and their deltas.\"\"\"\n    \n    def __init__(self, checkpoint_dir, deltas_dir, tokenizer, config):\n        self.checkpoint_dir = Path(checkpoint_dir)\n        self.deltas_dir = Path(deltas_dir)\n        self.tokenizer = tokenizer\n        self.config = config\n        \n        # Load manifests\n        manifest_path = self.checkpoint_dir / \"manifest.json\"\n        delta_manifest_path = self.deltas_dir / \"delta_manifest.json\"\n        \n        if manifest_path.exists() and delta_manifest_path.exists():\n            with open(manifest_path) as f:\n                self.adapter_manifest = json.load(f)\n            with open(delta_manifest_path) as f:\n                self.delta_manifest = json.load(f)\n            \n            # Build sample list - only adapters that have deltas\n            self.samples = []\n            for adapter in self.adapter_manifest[\"adapters\"]:\n                name = adapter[\"name\"]\n                if name in self.delta_manifest[\"adapters\"]:\n                    self.samples.append({\n                        \"name\": name,\n                        \"path\": adapter[\"path\"],\n                        \"task\": adapter[\"task\"],\n                        \"delta_file\": self.delta_manifest[\"adapters\"][name][\"delta_file\"],\n                    })\n        else:\n            self.samples = []\n            self.adapter_manifest = {\"adapters\": []}\n            self.delta_manifest = {\"adapters\": {}}\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Load prompts for conditioning\n        prompts_file = Path(sample[\"path\"]) / \"prompts.json\"\n        if prompts_file.exists():\n            with open(prompts_file) as f:\n                prompts_data = json.load(f)\n            # Use first prompt as condition\n            text = prompts_data[\"prompts\"][0] if prompts_data[\"prompts\"] else sample[\"name\"]\n        else:\n            text = sample[\"name\"]\n        \n        # Tokenize condition\n        enc = self.tokenizer(\n            text, \n            max_length=256, \n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n        \n        # Load delta\n        delta_path = self.deltas_dir / sample[\"delta_file\"]\n        delta = np.load(delta_path)\n        \n        # Load LoRA weights (for weight supervision)\n        adapter_weights_file = Path(sample[\"path\"]) / \"adapter_model.safetensors\"\n        if adapter_weights_file.exists():\n            lora_weights = load_file(adapter_weights_file)\n        else:\n            lora_weights = {}\n        \n        return {\n            \"condition_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"delta_teacher\": torch.from_numpy(delta).float(),\n            \"adapter_name\": sample[\"name\"],\n            \"lora_weights\": lora_weights,\n        }\n    \n    @staticmethod\n    def collate_fn(batch):\n        return {\n            \"condition_ids\": torch.stack([b[\"condition_ids\"] for b in batch]),\n            \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n            \"delta_teacher\": torch.stack([b[\"delta_teacher\"] for b in batch]),\n            \"adapter_names\": [b[\"adapter_name\"] for b in batch],\n            \"lora_weights\": [b[\"lora_weights\"] for b in batch],\n        }\n\n\n# Create dataset\ntext_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ndataset = RealAdapterDataset(checkpoint_dir, deltas_dir, text_tokenizer, base_config)\nprint(f\"[OK] Dataset: {len(dataset)} samples\")\n\nif len(dataset) == 0:\n    print(\"\\n⚠️  No samples found! Please run train_lora_adapters.ipynb first to create adapters.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Generator Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "class LoRAGenerator(nn.Module):\n    \"\"\"\n    Simplified LoRA generator for ablation studies.\n    \n    Takes text condition and generates LoRA weights (A and B matrices) for all layers.\n    Uses a transformer encoder to process condition, then projects to LoRA weights.\n    \"\"\"\n    \n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        \n        # Text encoder\n        self.embed = nn.Embedding(50000, 256)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=256, nhead=4, batch_first=True, dropout=0.1),\n            num_layers=3\n        )\n        \n        # Compute total LoRA parameters needed\n        # 7 projections per layer: q, k, v, o, gate, up, down\n        self.num_projections = cfg.num_layers * 7\n        \n        # For each projection, we need to generate A (rank x in_dim) and B (out_dim x rank)\n        # We'll use a hypernetwork approach: generate per-projection embeddings, then decode to weights\n        \n        # Per-projection embedding dimension\n        self.proj_embed_dim = 512\n        \n        # Generate projection embeddings from condition\n        self.proj_embeddings = nn.Linear(256, self.num_projections * self.proj_embed_dim)\n        \n        # Weight decoders - shared across projections but scaled\n        self.lora_rank = cfg.lora_rank\n        \n        # Small MLPs to decode A and B matrices\n        # A: (rank, in_dim), B: (out_dim, rank)\n        # We'll generate low-rank factors and compose\n        self.A_decoder = nn.Sequential(\n            nn.Linear(self.proj_embed_dim, 256),\n            nn.GELU(),\n            nn.Linear(256, cfg.lora_rank * 64),  # Generate in chunks\n        )\n        self.B_decoder = nn.Sequential(\n            nn.Linear(self.proj_embed_dim, 256),\n            nn.GELU(), \n            nn.Linear(256, cfg.lora_rank * 64),\n        )\n        \n        # Learnable scale factors per layer/projection type\n        self.scales = nn.Parameter(torch.ones(self.num_projections, 2) * 0.01)\n        \n        # Cache dimension info\n        self._build_dim_info()\n    \n    def _build_dim_info(self):\n        \"\"\"Build dimension info for each projection.\"\"\"\n        self.dim_info = []\n        for layer in range(self.cfg.num_layers):\n            for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]:\n                if proj in [\"k_proj\", \"v_proj\"]:\n                    out_d = self.cfg.num_kv_heads * (self.cfg.hidden_size // self.cfg.num_heads)\n                elif proj in [\"gate_proj\", \"up_proj\"]:\n                    out_d = self.cfg.intermediate_size\n                elif proj == \"down_proj\":\n                    out_d = self.cfg.hidden_size\n                else:\n                    out_d = self.cfg.hidden_size\n                \n                in_d = self.cfg.intermediate_size if proj == \"down_proj\" else self.cfg.hidden_size\n                \n                mod = \"self_attn\" if proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] else \"mlp\"\n                prefix = f\"model.layers.{layer}.{mod}.{proj}\"\n                \n                self.dim_info.append({\n                    \"layer\": layer,\n                    \"proj\": proj,\n                    \"in_dim\": in_d,\n                    \"out_dim\": out_d,\n                    \"A_key\": f\"{prefix}.lora_A.weight\",\n                    \"B_key\": f\"{prefix}.lora_B.weight\",\n                })\n    \n    def forward(self, condition_ids, attention_mask=None):\n        B = condition_ids.shape[0]\n        \n        # Encode condition\n        x = self.embed(condition_ids)\n        if attention_mask is not None:\n            key_padding_mask = ~attention_mask.bool()\n        else:\n            key_padding_mask = None\n        \n        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n        \n        # Pool to single vector\n        if attention_mask is not None:\n            x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1)\n        else:\n            x = x.mean(1)\n        \n        # Generate per-projection embeddings: (B, num_proj, proj_embed_dim)\n        proj_embeds = self.proj_embeddings(x).view(B, self.num_projections, self.proj_embed_dim)\n        \n        # Decode to weights\n        batch_weights = []\n        for b in range(B):\n            weights = {}\n            for idx, info in enumerate(self.dim_info):\n                embed = proj_embeds[b, idx]\n                \n                # Decode A and B base patterns\n                A_base = self.A_decoder(embed).view(self.lora_rank, -1)\n                B_base = self.B_decoder(embed).view(-1, self.lora_rank)\n                \n                # Expand to full dimensions via outer product with learned patterns\n                # This is more parameter-efficient than generating full matrices\n                in_d, out_d = info[\"in_dim\"], info[\"out_dim\"]\n                \n                # Use periodic extension to match dimensions\n                A_full = A_base[:, :in_d % 64 or 64].repeat(1, (in_d // 64) + 1)[:, :in_d]\n                B_full = B_base[:out_d % 64 or 64, :].repeat((out_d // 64) + 1, 1)[:out_d, :]\n                \n                # Apply learned scales\n                scale_a, scale_b = self.scales[idx]\n                A = A_full * scale_a\n                B = B_full * scale_b\n                \n                weights[info[\"A_key\"]] = A\n                weights[info[\"B_key\"]] = B\n            \n            batch_weights.append(weights)\n        \n        return batch_weights\n\n\ndef create_generator(cfg, seed):\n    torch.manual_seed(seed)\n    gen = LoRAGenerator(cfg).to(device)\n    # Print param count\n    num_params = sum(p.numel() for p in gen.parameters() if p.requires_grad)\n    print(f\"  Generator params: {num_params:,}\")\n    return gen"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Run Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "def run_trial(config_name: str, lambda_weight: float, lambda_delta: float, seed: int, trial_idx: int) -> Dict[str, Any]:\n    \"\"\"Run a single trial and return results.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Config: {config_name} | Trial {trial_idx+1}/{NUM_TRIALS} | Seed: {seed}\")\n    print(f\"λ_w={lambda_weight}, λ_d={lambda_delta}\")\n    print(f\"{'='*60}\")\n    \n    # Create config for this trial\n    config = TrainingConfig(\n        use_small_model=True,\n        batch_size=base_config.batch_size,\n        gradient_accumulation_steps=base_config.gradient_accumulation_steps,\n        max_steps=MAX_STEPS,\n        warmup_steps=10,\n        lambda_weight=lambda_weight,\n        lambda_delta=lambda_delta,\n        output_dir=str(OUTPUT_DIR / f\"{config_name}_trial{trial_idx}\"),\n    )\n    Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n    \n    # Fresh generator\n    generator = create_generator(config, seed)\n    \n    # Loss function\n    if lambda_weight == 0:\n        criterion = DeltaOnlyLoss()\n    else:\n        criterion = MultiTaskLoss(lambda_weight=lambda_weight, lambda_delta=lambda_delta)\n    \n    # Optimizer & scheduler\n    optimizer = AdamW(generator.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n    cosine_steps = max(1, config.max_steps - config.warmup_steps)\n    scheduler = SequentialLR(\n        optimizer,\n        [LinearLR(optimizer, 0.1, 1.0, config.warmup_steps),\n         CosineAnnealingLR(optimizer, cosine_steps, config.learning_rate * 0.01)],\n        [config.warmup_steps]\n    )\n    \n    # Dataloader (fresh for each trial)\n    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n    \n    # Train\n    start_time = time.time()\n    state = train(\n        generator=generator,\n        dataloader=dataloader,\n        functional_lora=functional_lora,\n        base_activation=base_activation,\n        probe_tokens=probe_tokens,\n        probe_masks=probe_masks,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        config=config,\n        compute_dtype=TORCH_DTYPE,\n    )\n    train_time = time.time() - start_time\n    \n    # Evaluate\n    eval_results = evaluate(\n        generator=generator,\n        dataloader=dataloader,\n        functional_lora=functional_lora,\n        base_activation=base_activation,\n        probe_tokens=probe_tokens,\n        probe_masks=probe_masks,\n        criterion=criterion,\n    )\n    \n    # Cleanup\n    del generator, optimizer, scheduler\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    result = {\n        \"config_name\": config_name,\n        \"trial\": trial_idx,\n        \"seed\": seed,\n        \"lambda_weight\": lambda_weight,\n        \"lambda_delta\": lambda_delta,\n        \"final_loss\": state.loss_history[-1] if state.loss_history else None,\n        \"best_loss\": state.best_loss,\n        \"train_time\": train_time,\n        **eval_results,\n    }\n    \n    # Print results - use correct metric key 'mean_cosine' from evaluate()\n    cosine = result.get('mean_cosine', None)\n    cosine_str = f\"{cosine:.4f}\" if cosine is not None else \"N/A\"\n    print(f\"Result: loss={result['final_loss']:.4f}, mean_cosine={cosine_str}\")\n    \n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all trials\n",
    "all_results = []\n",
    "\n",
    "for config_name, params in CONFIGS.items():\n",
    "    for trial_idx, seed in enumerate(SEEDS[:NUM_TRIALS]):\n",
    "        result = run_trial(\n",
    "            config_name=config_name,\n",
    "            lambda_weight=params[\"lambda_weight\"],\n",
    "            lambda_delta=params[\"lambda_delta\"],\n",
    "            seed=seed,\n",
    "            trial_idx=trial_idx,\n",
    "        )\n",
    "        all_results.append(result)\n",
    "\n",
    "print(f\"\\n\\nCompleted {len(all_results)} trials!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\ndf = pd.DataFrame(all_results)\n\n# Aggregate by config\nagg_dict = {\n    \"final_loss\": [\"mean\", \"std\"],\n    \"best_loss\": [\"mean\", \"std\"],\n    \"train_time\": [\"mean\"],\n}\n\n# Add cosine metrics if available (evaluate() returns 'mean_cosine')\nif \"mean_cosine\" in df.columns:\n    agg_dict[\"mean_cosine\"] = [\"mean\", \"std\"]\n\nsummary = df.groupby(\"config_name\").agg(agg_dict).round(4)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ABLATION SUMMARY (mean ± std over 3 trials)\")\nprint(\"=\"*70)\nprint(summary.to_string())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nconfigs = list(CONFIGS.keys())\ncolors = ['#2ecc71', '#3498db', '#e74c3c']\n\n# Final Loss comparison\nmeans = [df[df['config_name']==c]['final_loss'].mean() for c in configs]\nstds = [df[df['config_name']==c]['final_loss'].std() for c in configs]\naxes[0].bar(configs, means, yerr=stds, color=colors, capsize=5, alpha=0.8)\naxes[0].set_ylabel('Final Loss')\naxes[0].set_title('Final Loss by Configuration')\naxes[0].grid(axis='y', alpha=0.3)\n\n# Cosine similarity comparison - use correct key 'mean_cosine'\nif 'mean_cosine' in df.columns and df['mean_cosine'].notna().any():\n    means = [df[df['config_name']==c]['mean_cosine'].mean() for c in configs]\n    stds = [df[df['config_name']==c]['mean_cosine'].std() for c in configs]\n    axes[1].bar(configs, means, yerr=stds, color=colors, capsize=5, alpha=0.8)\n    axes[1].set_ylabel('Mean Cosine Similarity')\n    axes[1].set_title('Delta Cosine Similarity by Configuration')\n    axes[1].grid(axis='y', alpha=0.3)\n    axes[1].set_ylim(-1, 1)  # Cosine range\nelse:\n    axes[1].text(0.5, 0.5, 'Cosine similarity not available', ha='center', va='center', transform=axes[1].transAxes)\n    axes[1].set_title('Delta Cosine Similarity')\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / \"ablation_comparison.png\", dpi=150)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Save results\ndf.to_csv(OUTPUT_DIR / \"all_trials.csv\", index=False)\nsummary.to_csv(OUTPUT_DIR / \"summary.csv\")\n\nfinal_results = {\n    \"configs\": CONFIGS,\n    \"num_trials\": NUM_TRIALS,\n    \"seeds\": SEEDS[:NUM_TRIALS],\n    \"summary\": {c: {\n        \"final_loss_mean\": float(df[df['config_name']==c]['final_loss'].mean()),\n        \"final_loss_std\": float(df[df['config_name']==c]['final_loss'].std()),\n        \"mean_cosine_mean\": float(df[df['config_name']==c]['mean_cosine'].mean()) if 'mean_cosine' in df.columns else None,\n        \"mean_cosine_std\": float(df[df['config_name']==c]['mean_cosine'].std()) if 'mean_cosine' in df.columns else None,\n    } for c in configs},\n    \"all_trials\": all_results,\n}\n\nwith open(OUTPUT_DIR / \"ablation_results.json\", \"w\") as f:\n    json.dump(final_results, f, indent=2)\n\nprint(f\"Saved to {OUTPUT_DIR}/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Drive\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    drive_dir = f\"{DRIVE_OUTPUT_DIR}/phase4_5_ablations\"\n",
    "    if os.path.exists(drive_dir):\n",
    "        shutil.rmtree(drive_dir)\n",
    "    shutil.copytree(str(OUTPUT_DIR), drive_dir)\n",
    "    print(f\"[Drive] Synced to {drive_dir}\")\n",
    "else:\n",
    "    print(\"[Local] Outputs saved to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"Phase 4.5 Ablations Complete!\")\nprint(\"=\"*70)\n\nprint(f\"\\nDataset: {len(dataset)} samples\")\nprint(f\"Trials per config: {NUM_TRIALS}\")\nprint(f\"Steps per trial: {MAX_STEPS}\")\n\nprint(f\"\\nKey findings (loss | cosine):\")\nfor config_name in configs:\n    mean_loss = df[df['config_name']==config_name]['final_loss'].mean()\n    std_loss = df[df['config_name']==config_name]['final_loss'].std()\n    \n    if 'mean_cosine' in df.columns:\n        mean_cos = df[df['config_name']==config_name]['mean_cosine'].mean()\n        std_cos = df[df['config_name']==config_name]['mean_cosine'].std()\n        print(f\"  {config_name:12s}: {mean_loss:.4f} ± {std_loss:.4f} | {mean_cos:.4f} ± {std_cos:.4f}\")\n    else:\n        print(f\"  {config_name:12s}: {mean_loss:.4f} ± {std_loss:.4f}\")\n\nprint(f\"\\nOutputs saved to: {OUTPUT_DIR}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}