{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.6: Delta-Guided Training Ablation\n",
    "\n",
    "**Prerequisites:** Run `train_lora_adapters.ipynb` first to create real LoRA adapters and cached `delta_teacher`.\n",
    "\n",
    "## What This Notebook Tests\n",
    "\n",
    "We compare two ways of training a prompt-conditioned LoRA generator using **delta supervision**:\n",
    "1. **Delta-only** — compute Δ(base + generated LoRA) on probes every step.\n",
    "2. **Delta-guided** — add a cheap “delta head” trained every step, and only compute the expensive probe-based Δ every N steps.\n",
    "\n",
    "Both use:\n",
    "- Pretrained text encoder: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Prompt batches: 8 prompts per adapter\n",
    "\n",
    "Goal: reduce expensive base-model/probe forwards while keeping generated weights behavior-grounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Colab setup\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/llgbm/checkpoints'\n",
    "    DELTAS_DIR = CHECKPOINT_DIR + '/deltas'\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    DELTAS_DIR = './llgbm/deltas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llgbm import (\n",
    "    AblationConfig,\n",
    "    phase4_6_configs,\n",
    "    run_ablations,\n",
    "    plot_ablation_results,\n",
    ")\n",
    "\n",
    "print(\"[OK] llgbm imports\")\n",
    "print(\"[INFO] Phase 4.6: delta-only vs delta-guided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration - modify these parameters as needed\n",
    "config = AblationConfig(\n",
    "    # Ablation configurations to compare\n",
    "    configs=phase4_6_configs(\n",
    "        compute_delta_every_n_steps=2,  # N inner delta head steps per outer step\n",
    "        lambda_pred=1.0,\n",
    "        lambda_computed=1.0,\n",
    "        lambda_consistency=0.5,\n",
    "        delta_aggregation=\"attention\",\n",
    "    ),\n",
    "    \n",
    "    # Trial settings\n",
    "    num_trials=3,\n",
    "    seeds=[42, 123, 456],\n",
    "    num_steps=100,  # Outer steps (same for all configs)\n",
    "    \n",
    "    # Paths\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    deltas_dir=DELTAS_DIR,\n",
    "    output_dir=\"outputs/phase4_6_ablations\",\n",
    "    \n",
    "    # Model settings\n",
    "    use_small_model=True,  # Qwen2.5-0.5B\n",
    "    batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,  # Adjustable learning rate\n",
    "    warmup_steps=25,\n",
    "    shuffle_task_prompts=True,\n",
    "    \n",
    "    # Text encoder settings\n",
    "    text_encoder_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    freeze_text_encoder=True,\n",
    "    num_prompts_per_adapter=8,  # Sample 8 prompts instead of just first one\n",
    "    \n",
    "    # Colab settings\n",
    "    in_colab=IN_COLAB,\n",
    "    drive_output_dir=DRIVE_OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "print(f\"Configurations: {list(config.configs.keys())}\")\n",
    "print(f\"Total runs: {len(config.configs) * config.num_trials}\")\n",
    "print(f\"Steps per trial: {config.num_steps}\")\n",
    "print(f\"Delta head inner steps (delta_guided): {config.configs['delta_guided']['compute_delta_every_n_steps']}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Text encoder: {config.text_encoder_name}\")\n",
    "print(f\"Prompts per adapter: {config.num_prompts_per_adapter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all ablation experiments\n",
    "results = run_ablations(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = results[\"dataframe\"]\n",
    "\n",
    "# Aggregate by config\n",
    "agg_dict = {\n",
    "    \"final_loss\": [\"mean\", \"std\"],\n",
    "    \"best_loss\": [\"mean\", \"std\"],\n",
    "    \"train_time\": [\"mean\"],\n",
    "}\n",
    "if \"mean_cosine\" in df.columns:\n",
    "    agg_dict[\"mean_cosine\"] = [\"mean\", \"std\"]\n",
    "if \"mean_cosine_pred\" in df.columns:\n",
    "    agg_dict[\"mean_cosine_pred\"] = [\"mean\", \"std\"]\n",
    "\n",
    "summary = df.groupby(\"config_name\").agg(agg_dict).round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION SUMMARY (mean +/- std over 3 trials)\")\n",
    "print(\"=\"*70)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from pathlib import Path\n",
    "plot_ablation_results(df, Path(config.output_dir), list(config.configs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Drive if in Colab\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    import shutil\n",
    "    drive_dir = f\"{DRIVE_OUTPUT_DIR}/phase4_6_ablations\"\n",
    "    if os.path.exists(drive_dir):\n",
    "        shutil.rmtree(drive_dir)\n",
    "    shutil.copytree(str(config.output_dir), drive_dir)\n",
    "    print(f\"[Drive] Synced to {drive_dir}\")\n",
    "else:\n",
    "    print(\"[Local] Outputs saved to\", config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Phase 4.6 Ablations Complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nKey findings (loss | cosine):\")\n",
    "for config_name in config.configs.keys():\n",
    "    mean_loss = df[df['config_name']==config_name]['final_loss'].mean()\n",
    "    std_loss = df[df['config_name']==config_name]['final_loss'].std()\n",
    "    \n",
    "    if 'mean_cosine' in df.columns:\n",
    "        mean_cos = df[df['config_name']==config_name]['mean_cosine'].mean()\n",
    "        std_cos = df[df['config_name']==config_name]['mean_cosine'].std()\n",
    "        extra = \"\"\n",
    "        if 'mean_cosine_pred' in df.columns:\n",
    "            mean_cos_pred = df[df['config_name']==config_name]['mean_cosine_pred'].mean()\n",
    "            std_cos_pred = df[df['config_name']==config_name]['mean_cosine_pred'].std()\n",
    "            if not pd.isna(mean_cos_pred):\n",
    "                extra = f\" | pred={mean_cos_pred:.4f} +/- {std_cos_pred:.4f}\"\n",
    "        print(f\"  {config_name:12s}: {mean_loss:.4f} +/- {std_loss:.4f} | {mean_cos:.4f} +/- {std_cos:.4f}{extra}\")\n",
    "    else:\n",
    "        print(f\"  {config_name:12s}: {mean_loss:.4f} +/- {std_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nOutputs saved to: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Performance Evaluation (Optional)\n",
    "\n",
    "Evaluate generated LoRAs on held-out task data.\n",
    "\n",
    "**Tasks from manifest:** arc_e, arc_c, boolq, obqa, piqa, winogrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from llgbm import (\n    compute_base_eval_loss,\n    compute_accuracy_with_lora_batched,\n    create_generator,\n    create_generator_with_delta_head,\n    load_checkpoint,\n    TrainingConfig,\n    FunctionalLoRA,\n    RealAdapterDataset,\n    create_text_encoder,\n)\nfrom llgbm.ablations import _infer_mode, phase4_6_configs\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport json\nimport torch\nfrom pathlib import Path\n\n# Configuration\nCONFIGS_TO_EVAL = [\"delta_only\", \"delta_guided\"]\nNUM_TRIALS = 3\n\n# Define configs directly (self-contained)\nCONFIGS = phase4_6_configs()\n\n# Tasks from manifest (excluding hellaswag)\nTASKS = [\"arc_e\", \"arc_c\", \"boolq\", \"obqa\", \"piqa\", \"winogrande\"]\ntask_types = {\n    \"arc_e\": \"mcq\",\n    \"arc_c\": \"mcq\",\n    \"boolq\": \"bool\",\n    \"obqa\": \"mcq\",\n    \"piqa\": \"mcq\",\n    \"winogrande\": \"mcq\",\n}\n\n# Load base model and tokenizer\nbase_config = TrainingConfig(use_small_model=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"[1] Loading base model: {base_config.base_model}\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_config.base_model,\n    torch_dtype=torch.bfloat16,\n    device_map=device,\n    trust_remote_code=True,\n)\nbase_model.eval()\ntokenizer = AutoTokenizer.from_pretrained(base_config.base_model, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Create pretrained text encoder (same as training)\nprint(\"[2] Loading pretrained text encoder\")\ntext_encoder = create_text_encoder(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    freeze=True,\n    device=device,\n)\n\n# Load dataset with new settings\nprint(\"[3] Loading dataset with prompt batches\")\ndataset = RealAdapterDataset(\n    CHECKPOINT_DIR,\n    DELTAS_DIR,\n    text_encoder.tokenizer,  # Use text encoder's tokenizer\n    base_config,\n    num_prompts=8,  # Sample 8 prompts per adapter\n)\nprint(f\"    {len(dataset)} samples, 8 prompts per adapter\")\n\n# Create FunctionalLoRA wrapper\nfunctional_lora = FunctionalLoRA(\n    base_model,\n    lora_rank=base_config.lora_rank,\n    lora_alpha=base_config.lora_alpha,\n)\n\n# Load eval data for all tasks from manifest\nprint(\"[4] Loading eval data\")\neval_splits_dir = Path('/content/drive/MyDrive/llgbm/data')\neval_data = {}\nfor task in TASKS:\n    eval_file = eval_splits_dir / f\"{task}_eval.json\"\n    if eval_file.exists():\n        with open(eval_file) as f:\n            eval_data[task] = json.load(f)\n        print(f\"    {task}: {len(eval_data[task])} samples\")\n    else:\n        print(f\"    {task}: [NOT FOUND] {eval_file}\")\n\n# Evaluate base model first\nprint(\"\\n\" + \"=\"*60)\nprint(\"BASE MODEL (no LoRA)\")\nprint(\"=\"*60)\nfor task, samples in eval_data.items():\n    loss = compute_base_eval_loss(base_model, samples, tokenizer)\n    print(f\"  {task}: loss={loss:.4f}\")\n\n# Collect all results\nall_results = []\n\nclass _LoraOnlyWrapper(torch.nn.Module):\n    \"\"\"Adapter for delta-guided models (forward() -> List[Dict[str, Tensor]]).\"\"\"\n    def __init__(self, gen):\n        super().__init__()\n        self.gen = gen\n\n    def forward(self, condition_ids, attention_mask=None):\n        return self.gen.generate_lora(condition_ids, attention_mask)\n\n# Evaluate each config and trial\nfor config_name in CONFIGS_TO_EVAL:\n    # Get the actual mode from config to determine model architecture\n    config_params = CONFIGS.get(config_name, {})\n    mode = _infer_mode(config_name, config_params)\n    uses_delta_head = (mode == \"delta_guided\")\n    \n    for trial in range(NUM_TRIALS):\n        checkpoint_path = Path(f\"outputs/phase4_6_ablations/{config_name}_trial{trial}/checkpoint_best.pt\")\n        if not checkpoint_path.exists():\n            continue\n        \n        print(f\"\\n\" + \"=\"*60)\n        print(f\"{config_name.upper()} TRIAL {trial} (mode={mode})\")\n        print(\"=\"*60)\n        \n        # Load generator with pretrained text encoder\n        # Use correct architecture based on actual mode, not config name\n        if uses_delta_head:\n            gen_core = create_generator_with_delta_head(\n                base_config,\n                seed=42,\n                device=device,\n                text_encoder=text_encoder,\n            )\n            load_checkpoint(str(checkpoint_path), gen_core)\n            generator = _LoraOnlyWrapper(gen_core)\n        else:\n            generator = create_generator(\n                base_config,\n                seed=42,\n                device=device,\n                text_encoder=text_encoder,  # Use pretrained encoder\n            )\n            load_checkpoint(str(checkpoint_path), generator)\n\n        generator.eval()\n        \n        trial_results = {\"config\": config_name, \"trial\": trial}\n        \n        for task, samples in eval_data.items():\n            task_indices = [i for i, s in enumerate(dataset.samples) if s[\"task\"] == task]\n            if not task_indices:\n                print(f\"  {task}: [SKIP]\")\n                continue\n            \n            sample = dataset[task_indices[0]]\n            condition_ids = sample[\"condition_ids\"]  # (N, seq_len)\n            attention_mask = sample[\"attention_mask\"]  # (N, seq_len)\n            \n            result = compute_accuracy_with_lora_batched(\n                generator=generator,\n                functional_lora=functional_lora,\n                condition_ids=condition_ids,\n                attention_mask=attention_mask,\n                eval_samples=samples,\n                tokenizer=tokenizer,\n                task_type=task_types[task],\n                max_samples=100,\n                batch_size=8,\n                device=device,\n            )\n            trial_results[task] = result[\"accuracy\"]\n            print(f\"  {task}: {result['accuracy']:.2%} ({result['correct']}/{result['total']})\")\n        \n        all_results.append(trial_results)\n        del generator\n        torch.cuda.empty_cache()\n\n# Summary table\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY\")\nprint(\"=\"*60)\nimport pandas as pd\nif all_results:\n    df_eval = pd.DataFrame(all_results)\n    task_cols = [t for t in TASKS if t in df_eval.columns]\n    summary = df_eval.groupby(\"config\")[task_cols].agg([\"mean\", \"std\"])\n    print(summary.round(4).to_string())"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}