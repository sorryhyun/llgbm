{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.5: Ablation Studies\n",
    "\n",
    "**Prerequisites:** Run `train_lora_adapters.ipynb` first to create real LoRA adapters and deltas.\n",
    "\n",
    "This notebook compares different training configurations:\n",
    "1. **Multi-task** (λ_w=1.0, λ_d=0.1) - Both weight and delta supervision\n",
    "2. **Multi-task balanced** (λ_w=0.5, λ_d=0.5) - Equal weight/delta supervision\n",
    "3. **Delta-only** (λ_w=0.0, λ_d=1.0) - Behavioral supervision only\n",
    "4. **Weight-only** (λ_w=1.0, λ_d=0.0) - Traditional DnD baseline\n",
    "\n",
    "Each configuration runs 3 trials with different seeds for statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Colab setup\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/llgbm/checkpoints'\n",
    "    DELTAS_DIR = CHECKPOINT_DIR + '/deltas'\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    DELTAS_DIR = './llgbm/deltas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llgbm import AblationConfig, run_ablations, plot_ablation_results\n",
    "\n",
    "print(\"[OK] llgbm imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration - modify these parameters as needed\n",
    "config = AblationConfig(\n",
    "    # Ablation configurations to compare\n",
    "    configs={\n",
    "        \"multitask\": {\"lambda_weight\": 1.0, \"lambda_delta\": 0.1},\n",
    "        \"multitask2\": {\"lambda_weight\": 0.5, \"lambda_delta\": 0.5},\n",
    "        \"delta_only\": {\"lambda_weight\": 0.0, \"lambda_delta\": 1.0},\n",
    "        \"weight_only\": {\"lambda_weight\": 1.0, \"lambda_delta\": 0.0},\n",
    "    },\n",
    "    \n",
    "    # Trial settings\n",
    "    num_trials=3,\n",
    "    seeds=[42, 123, 456],\n",
    "    num_steps=100,\n",
    "    \n",
    "    # Paths\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    deltas_dir=DELTAS_DIR,\n",
    "    output_dir=\"outputs/phase4_5_ablations\",\n",
    "    \n",
    "    # Model settings\n",
    "    use_small_model=True,  # Qwen2.5-0.5B\n",
    "    batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=50,\n",
    "    \n",
    "    # Colab settings\n",
    "    in_colab=IN_COLAB,\n",
    "    drive_output_dir=DRIVE_OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "print(f\"Configurations: {list(config.configs.keys())}\")\n",
    "print(f\"Total runs: {len(config.configs) * config.num_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all ablation experiments\n",
    "results = run_ablations(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = results[\"dataframe\"]\n",
    "\n",
    "# Aggregate by config\n",
    "agg_dict = {\n",
    "    \"final_loss\": [\"mean\", \"std\"],\n",
    "    \"best_loss\": [\"mean\", \"std\"],\n",
    "    \"train_time\": [\"mean\"],\n",
    "}\n",
    "if \"mean_cosine\" in df.columns:\n",
    "    agg_dict[\"mean_cosine\"] = [\"mean\", \"std\"]\n",
    "\n",
    "summary = df.groupby(\"config_name\").agg(agg_dict).round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION SUMMARY (mean +/- std over 3 trials)\")\n",
    "print(\"=\"*70)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from pathlib import Path\n",
    "plot_ablation_results(df, Path(config.output_dir), list(config.configs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Drive if in Colab\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    import shutil\n",
    "    drive_dir = f\"{DRIVE_OUTPUT_DIR}/phase4_5_ablations\"\n",
    "    if os.path.exists(drive_dir):\n",
    "        shutil.rmtree(drive_dir)\n",
    "    shutil.copytree(str(config.output_dir), drive_dir)\n",
    "    print(f\"[Drive] Synced to {drive_dir}\")\n",
    "else:\n",
    "    print(\"[Local] Outputs saved to\", config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Phase 4.5 Ablations Complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nKey findings (loss | cosine):\")\n",
    "for config_name in config.configs.keys():\n",
    "    mean_loss = df[df['config_name']==config_name]['final_loss'].mean()\n",
    "    std_loss = df[df['config_name']==config_name]['final_loss'].std()\n",
    "    \n",
    "    if 'mean_cosine' in df.columns:\n",
    "        mean_cos = df[df['config_name']==config_name]['mean_cosine'].mean()\n",
    "        std_cos = df[df['config_name']==config_name]['mean_cosine'].std()\n",
    "        print(f\"  {config_name:12s}: {mean_loss:.4f} +/- {std_loss:.4f} | {mean_cos:.4f} +/- {std_cos:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {config_name:12s}: {mean_loss:.4f} +/- {std_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nOutputs saved to: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Performance Evaluation (Optional)\n",
    "\n",
    "Evaluate generated LoRAs using **eval loss** on held-out task data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from llgbm import (\n    compute_base_eval_loss,\n    compute_accuracy_with_lora_batched,\n    create_generator,\n    load_checkpoint,\n    TrainingConfig,\n    FunctionalLoRA,\n    RealAdapterDataset,\n)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport json\nimport torch\nfrom pathlib import Path\n\n# Configuration\nCONFIGS_TO_EVAL = [\"delta_only\", \"multitask\", \"weight_only\"]\nNUM_TRIALS = 3\ntask_types = {\"arc_e\": \"mcq\", \"boolq\": \"bool\", \"gsm8k\": \"gsm8k\"}\n\n# Load base model and tokenizer\nbase_config = TrainingConfig(use_small_model=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"[1] Loading base model: {base_config.base_model}\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_config.base_model,\n    torch_dtype=torch.bfloat16,\n    device_map=device,\n    trust_remote_code=True,\n)\nbase_model.eval()\ntokenizer = AutoTokenizer.from_pretrained(base_config.base_model, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load dataset for conditioning\nprint(\"[2] Loading dataset\")\ntext_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ndataset = RealAdapterDataset(CHECKPOINT_DIR, DELTAS_DIR, text_tokenizer, base_config)\nprint(f\"    {len(dataset)} samples\")\n\n# Create FunctionalLoRA wrapper\nfunctional_lora = FunctionalLoRA(\n    base_model,\n    lora_rank=base_config.lora_rank,\n    lora_alpha=base_config.lora_alpha,\n)\n\n# Load eval data\nprint(\"[3] Loading eval data\")\neval_splits_dir = Path(CHECKPOINT_DIR) / \"eval_splits\"\neval_data = {}\nfor task in [\"arc_e\", \"boolq\", \"gsm8k\"]:\n    eval_file = eval_splits_dir / f\"{task}_eval.json\"\n    if eval_file.exists():\n        with open(eval_file) as f:\n            eval_data[task] = json.load(f)\n        print(f\"    {task}: {len(eval_data[task])} samples\")\n\n# Evaluate base model first\nprint(\"\\n\" + \"=\"*60)\nprint(\"BASE MODEL (no LoRA)\")\nprint(\"=\"*60)\nfor task, samples in eval_data.items():\n    loss = compute_base_eval_loss(base_model, samples, tokenizer)\n    print(f\"  {task}: loss={loss:.4f}\")\n\n# Collect all results\nall_results = []\n\n# Evaluate each config and trial\nfor config_name in CONFIGS_TO_EVAL:\n    for trial in range(NUM_TRIALS):\n        checkpoint_path = Path(f\"outputs/phase4_5_ablations/{config_name}_trial{trial}/checkpoint_best.pt\")\n        if not checkpoint_path.exists():\n            continue\n        \n        print(f\"\\n\" + \"=\"*60)\n        print(f\"{config_name.upper()} TRIAL {trial}\")\n        print(\"=\"*60)\n        \n        # Load generator\n        generator = create_generator(base_config, seed=42, device=device)\n        load_checkpoint(str(checkpoint_path), generator)\n        generator.eval()\n        \n        trial_results = {\"config\": config_name, \"trial\": trial}\n        \n        for task, samples in eval_data.items():\n            task_indices = [i for i, s in enumerate(dataset.samples) if s[\"task\"] == task]\n            if not task_indices:\n                print(f\"  {task}: [SKIP]\")\n                continue\n            \n            sample = dataset[task_indices[0]]\n            condition_ids = sample[\"condition_ids\"].long()\n            attention_mask = sample[\"attention_mask\"].float()\n            \n            result = compute_accuracy_with_lora_batched(\n                generator=generator,\n                functional_lora=functional_lora,\n                condition_ids=condition_ids,\n                attention_mask=attention_mask,\n                eval_samples=samples,\n                tokenizer=tokenizer,\n                task_type=task_types[task],\n                max_samples=100,\n                batch_size=8,\n                device=device,\n            )\n            trial_results[task] = result[\"accuracy\"]\n            print(f\"  {task}: {result['accuracy']:.2%} ({result['correct']}/{result['total']})\")\n        \n        all_results.append(trial_results)\n        del generator\n        torch.cuda.empty_cache()\n\n# Summary table\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY\")\nprint(\"=\"*60)\nimport pandas as pd\nif all_results:\n    df_eval = pd.DataFrame(all_results)\n    summary = df_eval.groupby(\"config\")[[\"arc_e\", \"boolq\", \"gsm8k\"]].agg([\"mean\", \"std\"])\n    print(summary.round(4).to_string())"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}