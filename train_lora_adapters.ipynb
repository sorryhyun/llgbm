{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LoRA Adapters with Non-Overlapping Batches\n",
    "\n",
    "Adapter training using HuggingFace's `SFTTrainer` from `trl`, following the DnD approach of training adapters on **non-overlapping data batches**.\n",
    "\n",
    "**Non-overlapping batch approach:**\n",
    "- Each task's data is shuffled (with fixed seed) then divided into N equal parts\n",
    "- Each adapter is trained on a completely separate subset of the data\n",
    "- This ensures the generator learns from adapters that have seen different subsets\n",
    "\n",
    "**Improvements over manual training:**\n",
    "- Built-in evaluation during training\n",
    "- Automatic gradient checkpointing\n",
    "- Cleaner configuration via `SFTConfig`\n",
    "- Better memory management\n",
    "\n",
    "**Tasks:** ARC-e, ARC-c, BoolQ, OBQA, HellaSwag, PIQA, WinoGrande (7 commonsense tasks)\n",
    "**Output:** 5 adapters per task (35 total) + deltas + eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/llgbm'\n",
    "    DATA_DIR = f'{DRIVE_ROOT}/data'\n",
    "    OUTPUT_DIR = f'{DRIVE_ROOT}/checkpoints'\n",
    "    sys.path.insert(0, DRIVE_ROOT)\n",
    "else:\n",
    "    DATA_DIR = 'data'\n",
    "    OUTPUT_DIR = 'checkpoints'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Model - use Instruct version to match ablation pipeline\n",
    "    model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    \n",
    "    # LoRA\n",
    "    lora_rank: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0\n",
    "    target_modules: tuple = (\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\")\n",
    "    \n",
    "    # Training (SFTConfig compatible)\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 1e-4\n",
    "    max_length: int = 384\n",
    "    warmup_ratio: float = 0.1\n",
    "    gradient_checkpointing: bool = False\n",
    "    eval_ratio: float = 0.1  # 10% of training data for eval\n",
    "    \n",
    "    # Data - Non-overlapping batches (DnD style)\n",
    "    num_batches: int = 5  # Number of non-overlapping batches per task\n",
    "    shuffle_seed: int = 42  # Seed for shuffling before splitting\n",
    "\n",
    "config = Config()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"LoRA: rank={config.lora_rank}, alpha={config.lora_alpha}\")\n",
    "print(f\"Training: {config.num_epochs} epochs, batch={config.batch_size}\")\n",
    "print(f\"Data: {config.num_batches} non-overlapping batches per task (seed={config.shuffle_seed})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task definitions (7 commonsense reasoning tasks)\n",
    "TASKS = {\n",
    "    \"arc_e\": {\"file\": \"ARC-e_train.json\", \"delta_probes\": 32},\n",
    "    \"arc_c\": {\"file\": \"ARC-c_train.json\", \"delta_probes\": 32},\n",
    "    \"boolq\": {\"file\": \"BoolQ_train.json\", \"delta_probes\": 32},\n",
    "    \"obqa\": {\"file\": \"OBQA_train.json\", \"delta_probes\": 32},\n",
    "    # \"hellaswag\": {\"file\": \"HellaSwag_train.json\", \"delta_probes\": 16},\n",
    "    \"piqa\": {\"file\": \"PIQA_train.json\", \"delta_probes\": 32},\n",
    "    \"winogrande\": {\"file\": \"WinoGrande_train.json\", \"delta_probes\": 32},\n",
    "}\n",
    "\n",
    "# Load and verify data\n",
    "task_data_cache = {}\n",
    "for task, info in TASKS.items():\n",
    "    path = Path(DATA_DIR) / info[\"file\"]\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            task_data_cache[task] = json.load(f)\n",
    "        # Show non-overlapping batch distribution\n",
    "        total = len(task_data_cache[task])\n",
    "        per_batch = total // config.num_batches\n",
    "        print(f\"{task}: {total} total → {per_batch} per batch × {config.num_batches} batches\")\n",
    "    else:\n",
    "        print(f\"{task}: MISSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat(example: dict) -> dict:\n",
    "    \"\"\"Format example as Qwen chat template for SFTTrainer.\"\"\"\n",
    "    system = example.get(\"system\", \"You are a helpful assistant.\")\n",
    "    text = (\n",
    "        f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example['prompt']}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "def create_datasets(data: list[dict], eval_ratio: float = 0.1) -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"Create train/eval HuggingFace Datasets from raw data.\"\"\"\n",
    "    # Split data\n",
    "    n_eval = max(1, int(len(data) * eval_ratio))\n",
    "    train_data = data[:-n_eval]\n",
    "    eval_data = data[-n_eval:]\n",
    "    \n",
    "    # Create datasets with text formatting\n",
    "    train_ds = Dataset.from_list(train_data).map(format_chat, remove_columns=[\"prompt\", \"response\"])\n",
    "    eval_ds = Dataset.from_list(eval_data).map(format_chat, remove_columns=[\"prompt\", \"response\"])\n",
    "    \n",
    "    return train_ds, eval_ds\n",
    "\n",
    "\n",
    "def create_lora_config(cfg: Config) -> LoraConfig:\n",
    "    \"\"\"Create PEFT LoRA configuration.\"\"\"\n",
    "    return LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=cfg.lora_alpha,\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=list(cfg.target_modules),\n",
    "        bias=\"none\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_adapter(\n",
    "    task_name: str,\n",
    "    adapter_idx: int,\n",
    "    train_data: list[dict],\n",
    "    tokenizer,\n",
    "    cfg: Config,\n",
    ") -> dict:\n",
    "    \"\"\"Train a single LoRA adapter using SFTTrainer with evaluation.\"\"\"\n",
    "    \n",
    "    adapter_name = f\"{task_name}_{adapter_idx:03d}\"\n",
    "    output_path = Path(OUTPUT_DIR) / task_name / adapter_name\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {adapter_name} ({len(train_data)} samples)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create train/eval datasets\n",
    "    train_ds, eval_ds = create_datasets(train_data, cfg.eval_ratio)\n",
    "    print(f\"  Train: {len(train_ds)}, Eval: {len(eval_ds)}\")\n",
    "    \n",
    "    # Load fresh base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = create_lora_config(cfg)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # SFTConfig: save best 2 checkpoints, load best at end\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=str(output_path),\n",
    "        num_train_epochs=cfg.num_epochs,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=cfg.batch_size,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        max_length=cfg.max_length,\n",
    "        gradient_checkpointing=cfg.gradient_checkpointing,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        completion_only_loss=True,\n",
    "        # Evaluation & Checkpointing\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        # Logging\n",
    "        logging_steps=20,\n",
    "        report_to=\"none\",\n",
    "        # Optimization\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        max_grad_norm=1.0,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train (best model loaded automatically at end)\n",
    "    result = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # Save only the best adapter\n",
    "    final_path = output_path / \"adapter\"\n",
    "    model.save_pretrained(final_path)\n",
    "    \n",
    "    # Save prompts & metrics\n",
    "    with open(final_path / \"prompts.json\", \"w\") as f:\n",
    "        json.dump({\"prompts\": [d[\"prompt\"] for d in train_data[:128]], \"task\": task_name}, f)\n",
    "    \n",
    "    with open(final_path / \"metrics.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": result.training_loss,\n",
    "            \"eval_loss\": eval_result.get(\"eval_loss\"),\n",
    "            \"train_samples\": len(train_ds),\n",
    "            \"eval_samples\": len(eval_ds),\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"  Train: {result.training_loss:.4f}, Eval: {eval_result.get('eval_loss'):.4f}\")\n",
    "    print(f\"  Saved: {final_path}\")\n",
    "    \n",
    "    # Cleanup checkpoints\n",
    "    import shutil\n",
    "    for ckpt in output_path.glob(\"checkpoint-*\"):\n",
    "        shutil.rmtree(ckpt)\n",
    "    \n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"name\": adapter_name,\n",
    "        \"task\": task_name,\n",
    "        \"path\": str(final_path),\n",
    "        \"train_loss\": result.training_loss,\n",
    "        \"eval_loss\": eval_result.get(\"eval_loss\"),\n",
    "        \"samples\": len(train_data),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Tokenizer: {config.model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Adapters (Non-Overlapping Batches)\n",
    "\n",
    "For each task:\n",
    "1. Shuffle data with fixed seed for reproducibility\n",
    "2. Divide into `num_batches` equal, non-overlapping parts\n",
    "3. Train one adapter per batch\n",
    "\n",
    "This follows the DnD paper's approach where adapters are trained on disjoint data subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adapters = []\n",
    "\n",
    "for task_name, task_info in TASKS.items():\n",
    "    task_data = task_data_cache[task_name].copy()  # Copy to avoid modifying original\n",
    "    \n",
    "    # Shuffle data with fixed seed for reproducibility before splitting\n",
    "    random.seed(config.shuffle_seed)\n",
    "    random.shuffle(task_data)\n",
    "    \n",
    "    # Compute samples per batch (strict non-overlapping)\n",
    "    total_samples = len(task_data)\n",
    "    samples_per_batch = total_samples // config.num_batches\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task: {task_name} ({total_samples} samples, shuffled with seed={config.shuffle_seed})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for batch_idx in range(config.num_batches):\n",
    "        # Strict non-overlapping batch selection\n",
    "        start_idx = batch_idx * samples_per_batch\n",
    "        end_idx = start_idx + samples_per_batch\n",
    "        \n",
    "        # Last batch gets any remaining samples\n",
    "        if batch_idx == config.num_batches - 1:\n",
    "            end_idx = total_samples\n",
    "        \n",
    "        subset = task_data[start_idx:end_idx]\n",
    "        print(f\"  Batch {batch_idx}: [{start_idx}:{end_idx}] = {len(subset)} samples\")\n",
    "        \n",
    "        adapter_info = train_adapter(\n",
    "            task_name=task_name,\n",
    "            adapter_idx=batch_idx,\n",
    "            train_data=subset,\n",
    "            tokenizer=tokenizer,\n",
    "            cfg=config,\n",
    "        )\n",
    "        \n",
    "        # Add batch metadata\n",
    "        adapter_info[\"batch_idx\"] = batch_idx\n",
    "        adapter_info[\"batch_range\"] = [start_idx, end_idx]\n",
    "        \n",
    "        all_adapters.append(adapter_info)\n",
    "\n",
    "print(f\"\\nTrained {len(all_adapters)} adapters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = {\n",
    "    \"model_name\": config.model_name,\n",
    "    \"lora_config\": {\n",
    "        \"rank\": config.lora_rank,\n",
    "        \"alpha\": config.lora_alpha,\n",
    "        \"target_modules\": list(config.target_modules),\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"non_overlapping_batches\": True,\n",
    "        \"num_batches_per_task\": config.num_batches,\n",
    "        \"shuffle_seed\": config.shuffle_seed,\n",
    "    },\n",
    "    \"adapters\": all_adapters,\n",
    "}\n",
    "\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "with open(output_path / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Manifest saved: {output_path / 'manifest.json'}\")\n",
    "print(f\"  - {len(all_adapters)} adapters across {len(TASKS)} tasks\")\n",
    "print(f\"  - {config.num_batches} non-overlapping batches per task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Deltas\n",
    "\n",
    "Delta activations measure behavioral differences between base and adapted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_probes(task_data: list[dict], num_probes: int = 16) -> list[str]:\n",
    "    \"\"\"Get task-specific probes for delta computation.\"\"\"\n",
    "    random.seed(42)\n",
    "    indices = random.sample(range(len(task_data)), min(num_probes, len(task_data)))\n",
    "    \n",
    "    probes = []\n",
    "    for idx in indices:\n",
    "        item = task_data[idx]\n",
    "        system = item.get(\"system\", \"You are a helpful assistant.\")\n",
    "        probe = (\n",
    "            f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{item['prompt']}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        probes.append(probe)\n",
    "    return probes\n",
    "\n",
    "\n",
    "def get_activation(model, tokenizer, probes: list[str], device) -> torch.Tensor:\n",
    "    \"\"\"Compute average last-layer, last-token activation over probes.\"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for probe in probes:\n",
    "            inputs = tokenizer(probe, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            \n",
    "            seq_len = inputs[\"attention_mask\"].sum().item()\n",
    "            last_hidden = outputs.hidden_states[-1][0, seq_len - 1, :]\n",
    "            activations.append(last_hidden)\n",
    "    \n",
    "    return torch.stack(activations).mean(dim=0)\n",
    "\n",
    "\n",
    "def compute_all_deltas(adapters: list[dict], task_data_cache: dict, tokenizer, cfg: Config):\n",
    "    \"\"\"Compute delta activations for all trained adapters.\"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    output_path = Path(OUTPUT_DIR)\n",
    "    deltas_dir = output_path / \"deltas\"\n",
    "    deltas_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load base model once\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    base_model.eval()\n",
    "    device = next(base_model.parameters()).device\n",
    "    \n",
    "    # Compute base activation using mixed probes\n",
    "    all_probes = []\n",
    "    for task_name, task_data in task_data_cache.items():\n",
    "        all_probes.extend(get_probes(task_data, 8))\n",
    "    \n",
    "    base_activation = get_activation(base_model, tokenizer, all_probes, device)\n",
    "    np.save(deltas_dir / \"base_activation.npy\", base_activation.cpu().float().numpy())\n",
    "    print(f\"Base activation: {base_activation.shape}\")\n",
    "    \n",
    "    # Compute per-adapter deltas\n",
    "    delta_manifest = {\"base_activation_file\": \"base_activation.npy\", \"adapters\": {}}\n",
    "    \n",
    "    for adapter_info in tqdm(adapters, desc=\"Computing deltas\"):\n",
    "        adapter_name = adapter_info[\"name\"]\n",
    "        task_name = adapter_info[\"task\"]\n",
    "        adapter_path = adapter_info[\"path\"]  # Already includes /adapter\n",
    "        \n",
    "        # Task-specific probes\n",
    "        task_probes = get_probes(task_data_cache[task_name], TASKS[task_name][\"delta_probes\"])\n",
    "        \n",
    "        # Get base activation for these specific probes\n",
    "        base_act = get_activation(base_model, tokenizer, task_probes, device)\n",
    "        \n",
    "        # Load adapter and get activation\n",
    "        adapted = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        adapted.eval()\n",
    "        adapted_act = get_activation(adapted, tokenizer, task_probes, device)\n",
    "        \n",
    "        # Delta\n",
    "        delta = (adapted_act - base_act).cpu().float().numpy()\n",
    "        \n",
    "        # Save\n",
    "        delta_file = f\"{adapter_name}_delta.npy\"\n",
    "        np.save(deltas_dir / delta_file, delta)\n",
    "        \n",
    "        delta_manifest[\"adapters\"][adapter_name] = {\n",
    "            \"adapter_path\": adapter_path,\n",
    "            \"delta_file\": delta_file,\n",
    "            \"task\": task_name,\n",
    "            \"delta_norm\": float(np.linalg.norm(delta)),\n",
    "        }\n",
    "        \n",
    "        # Cleanup\n",
    "        del adapted\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save manifest\n",
    "    with open(deltas_dir / \"delta_manifest.json\", \"w\") as f:\n",
    "        json.dump(delta_manifest, f, indent=2)\n",
    "    \n",
    "    # Cleanup base model\n",
    "    del base_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nDeltas saved to: {deltas_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adapters from manifest (use this if adapters are already trained)\n",
    "manifest_path = Path(OUTPUT_DIR) / \"manifest.json\"\n",
    "\n",
    "if manifest_path.exists():\n",
    "    with open(manifest_path) as f:\n",
    "        manifest = json.load(f)\n",
    "    all_adapters = manifest[\"adapters\"]\n",
    "    print(f\"Loaded {len(all_adapters)} adapters from manifest.json\")\n",
    "    \n",
    "    # Show task breakdown\n",
    "    from collections import Counter\n",
    "    task_counts = Counter(a[\"task\"] for a in all_adapters)\n",
    "    for task, count in sorted(task_counts.items()):\n",
    "        print(f\"  {task}: {count} adapters\")\n",
    "else:\n",
    "    print(f\"[ERROR] manifest.json not found at {manifest_path}\")\n",
    "    print(\"Run the training cells first to create adapters.\")\n",
    "\n",
    "# Compute deltas for all adapters\n",
    "if all_adapters:\n",
    "    compute_all_deltas(all_adapters, task_data_cache, tokenizer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary DataFrame\n",
    "df = pd.DataFrame(all_adapters)\n",
    "print(\"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAdapters trained: {len(all_adapters)}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\nPer-task metrics:\")\n",
    "summary = df.groupby(\"task\").agg({\n",
    "    \"train_loss\": [\"mean\", \"std\"],\n",
    "    \"eval_loss\": [\"mean\", \"std\"],\n",
    "}).round(4)\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  {OUTPUT_DIR}/manifest.json\")\n",
    "print(f\"  {OUTPUT_DIR}/deltas/delta_manifest.json\")\n",
    "for a in all_adapters:\n",
    "    print(f\"  {a['path']}/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
