{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LoRA Adapters with SFTTrainer\n",
    "\n",
    "Simplified adapter training using HuggingFace's `SFTTrainer` from `trl`.\n",
    "\n",
    "**Improvements over manual training:**\n",
    "- Built-in evaluation during training\n",
    "- Automatic gradient checkpointing\n",
    "- Cleaner configuration via `SFTConfig`\n",
    "- Better memory management\n",
    "\n",
    "**Tasks:** ARC-e, BoolQ, GSM8K  \n",
    "**Output:** LoRA adapters + deltas + eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/llgbm'\n",
    "    DATA_DIR = f'{DRIVE_ROOT}/data'\n",
    "    OUTPUT_DIR = f'{DRIVE_ROOT}/checkpoints'\n",
    "    sys.path.insert(0, DRIVE_ROOT)\n",
    "else:\n",
    "    DATA_DIR = 'data'\n",
    "    OUTPUT_DIR = 'checkpoints'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Config:\n    # Model - use Instruct version to match ablation pipeline\n    model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"\n    \n    # LoRA\n    lora_rank: int = 8\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    target_modules: tuple = (\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\")\n    \n    # Training (SFTConfig compatible)\n    num_epochs: int = 2\n    batch_size: int = 4\n    learning_rate: float = 2e-4\n    max_length: int = 384\n    warmup_ratio: float = 0.1\n    gradient_checkpointing: bool = True\n    eval_ratio: float = 0.1  # 10% of training data for eval\n    \n    # Data\n    sample_ratio: float = 0.1  # Use 10% of available data per adapter\n    adapters_per_task: int = 3  # Number of adapters per task\n\nconfig = Config()\nprint(f\"Model: {config.model_name}\")\nprint(f\"LoRA: rank={config.lora_rank}, alpha={config.lora_alpha}\")\nprint(f\"Training: {config.num_epochs} epochs, batch={config.batch_size}\")\nprint(f\"Data: {config.sample_ratio:.0%} of task data per adapter\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task definitions\nTASKS = {\n    \"arc_e\": {\"file\": \"ARC-e_train.json\", \"delta_probes\": 16},\n    \"boolq\": {\"file\": \"BoolQ_train.json\", \"delta_probes\": 16},\n    \"gsm8k\": {\"file\": \"GSM8K_train.json\", \"delta_probes\": 16},\n}\n\n# Load and verify data\ntask_data_cache = {}\nfor task, info in TASKS.items():\n    path = Path(DATA_DIR) / info[\"file\"]\n    if path.exists():\n        with open(path) as f:\n            task_data_cache[task] = json.load(f)\n        # Compute samples per adapter based on ratio\n        n_samples = int(len(task_data_cache[task]) * config.sample_ratio)\n        print(f\"{task}: {len(task_data_cache[task])} total â†’ {n_samples} per adapter ({config.sample_ratio:.0%})\")\n    else:\n        print(f\"{task}: MISSING\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat(example: dict) -> dict:\n",
    "    \"\"\"Format example as Qwen chat template for SFTTrainer.\"\"\"\n",
    "    system = example.get(\"system\", \"You are a helpful assistant.\")\n",
    "    text = (\n",
    "        f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example['prompt']}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "def create_datasets(data: list[dict], eval_ratio: float = 0.1) -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"Create train/eval HuggingFace Datasets from raw data.\"\"\"\n",
    "    # Split data\n",
    "    n_eval = max(1, int(len(data) * eval_ratio))\n",
    "    train_data = data[:-n_eval]\n",
    "    eval_data = data[-n_eval:]\n",
    "    \n",
    "    # Create datasets with text formatting\n",
    "    train_ds = Dataset.from_list(train_data).map(format_chat, remove_columns=[\"prompt\", \"response\"])\n",
    "    eval_ds = Dataset.from_list(eval_data).map(format_chat, remove_columns=[\"prompt\", \"response\"])\n",
    "    \n",
    "    return train_ds, eval_ds\n",
    "\n",
    "\n",
    "def create_lora_config(cfg: Config) -> LoraConfig:\n",
    "    \"\"\"Create PEFT LoRA configuration.\"\"\"\n",
    "    return LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=cfg.lora_alpha,\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=list(cfg.target_modules),\n",
    "        bias=\"none\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_adapter(\n",
    "    task_name: str,\n",
    "    adapter_idx: int,\n",
    "    train_data: list[dict],\n",
    "    tokenizer,\n",
    "    cfg: Config,\n",
    ") -> dict:\n",
    "    \"\"\"Train a single LoRA adapter using SFTTrainer with evaluation.\"\"\"\n",
    "    \n",
    "    adapter_name = f\"{task_name}_{adapter_idx:03d}\"\n",
    "    output_path = Path(OUTPUT_DIR) / task_name / adapter_name\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {adapter_name} ({len(train_data)} samples)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create train/eval datasets\n",
    "    train_ds, eval_ds = create_datasets(train_data, cfg.eval_ratio)\n",
    "    print(f\"  Train: {len(train_ds)}, Eval: {len(eval_ds)}\")\n",
    "    \n",
    "    # Load fresh base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = create_lora_config(cfg)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # SFTConfig: save best 2 checkpoints, load best at end\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=str(output_path),\n",
    "        num_train_epochs=cfg.num_epochs,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=cfg.batch_size,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        max_seq_length=cfg.max_length,\n",
    "        gradient_checkpointing=cfg.gradient_checkpointing,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        completion_only_loss=True,\n",
    "        # Evaluation & Checkpointing\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        # Logging\n",
    "        logging_steps=20,\n",
    "        report_to=\"none\",\n",
    "        # Optimization\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        max_grad_norm=1.0,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train (best model loaded automatically at end)\n",
    "    result = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # Save only the best adapter\n",
    "    final_path = output_path / \"adapter\"\n",
    "    model.save_pretrained(final_path)\n",
    "    \n",
    "    # Save prompts & metrics\n",
    "    with open(final_path / \"prompts.json\", \"w\") as f:\n",
    "        json.dump({\"prompts\": [d[\"prompt\"] for d in train_data[:128]], \"task\": task_name}, f)\n",
    "    \n",
    "    with open(final_path / \"metrics.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": result.training_loss,\n",
    "            \"eval_loss\": eval_result.get(\"eval_loss\"),\n",
    "            \"train_samples\": len(train_ds),\n",
    "            \"eval_samples\": len(eval_ds),\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"  Train: {result.training_loss:.4f}, Eval: {eval_result.get('eval_loss'):.4f}\")\n",
    "    print(f\"  Saved: {final_path}\")\n",
    "    \n",
    "    # Cleanup checkpoints\n",
    "    import shutil\n",
    "    for ckpt in output_path.glob(\"checkpoint-*\"):\n",
    "        shutil.rmtree(ckpt)\n",
    "    \n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"name\": adapter_name,\n",
    "        \"task\": task_name,\n",
    "        \"path\": str(final_path),\n",
    "        \"train_loss\": result.training_loss,\n",
    "        \"eval_loss\": eval_result.get(\"eval_loss\"),\n",
    "        \"samples\": len(train_data),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Tokenizer: {config.model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_adapters = []\n\nfor task_name, task_info in TASKS.items():\n    task_data = task_data_cache[task_name]\n    \n    # Compute samples per adapter from ratio\n    samples = int(len(task_data) * config.sample_ratio)\n    samples = max(samples, 10)  # Minimum 10 samples\n    \n    for adapter_idx in range(config.adapters_per_task):\n        # Select data subset (non-overlapping when possible)\n        start_idx = adapter_idx * samples\n        end_idx = start_idx + samples\n        \n        if end_idx > len(task_data):\n            # Wrap around if we run out of data\n            subset = task_data[start_idx:] + task_data[:end_idx - len(task_data)]\n        else:\n            subset = task_data[start_idx:end_idx]\n        \n        adapter_info = train_adapter(\n            task_name=task_name,\n            adapter_idx=adapter_idx,\n            train_data=subset,\n            tokenizer=tokenizer,\n            cfg=config,\n        )\n        all_adapters.append(adapter_info)\n\nprint(f\"\\nTrained {len(all_adapters)} adapters!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = {\n",
    "    \"model_name\": config.model_name,\n",
    "    \"lora_config\": {\n",
    "        \"rank\": config.lora_rank,\n",
    "        \"alpha\": config.lora_alpha,\n",
    "        \"target_modules\": list(config.target_modules),\n",
    "    },\n",
    "    \"adapters\": all_adapters,\n",
    "}\n",
    "\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "with open(output_path / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Manifest saved: {output_path / 'manifest.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Deltas\n",
    "\n",
    "Delta activations measure behavioral differences between base and adapted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_probes(task_data: list[dict], num_probes: int = 16) -> list[str]:\n",
    "    \"\"\"Get task-specific probes for delta computation.\"\"\"\n",
    "    random.seed(42)\n",
    "    indices = random.sample(range(len(task_data)), min(num_probes, len(task_data)))\n",
    "    \n",
    "    probes = []\n",
    "    for idx in indices:\n",
    "        item = task_data[idx]\n",
    "        system = item.get(\"system\", \"You are a helpful assistant.\")\n",
    "        probe = (\n",
    "            f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{item['prompt']}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        probes.append(probe)\n",
    "    return probes\n",
    "\n",
    "\n",
    "def get_activation(model, tokenizer, probes: list[str], device) -> torch.Tensor:\n",
    "    \"\"\"Compute average last-layer, last-token activation over probes.\"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for probe in probes:\n",
    "            inputs = tokenizer(probe, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            \n",
    "            seq_len = inputs[\"attention_mask\"].sum().item()\n",
    "            last_hidden = outputs.hidden_states[-1][0, seq_len - 1, :]\n",
    "            activations.append(last_hidden)\n",
    "    \n",
    "    return torch.stack(activations).mean(dim=0)\n",
    "\n",
    "\n",
    "def compute_all_deltas(adapters: list[dict], task_data_cache: dict, tokenizer, cfg: Config):\n",
    "    \"\"\"Compute delta activations for all trained adapters.\"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    output_path = Path(OUTPUT_DIR)\n",
    "    deltas_dir = output_path / \"deltas\"\n",
    "    deltas_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load base model once\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    base_model.eval()\n",
    "    device = next(base_model.parameters()).device\n",
    "    \n",
    "    # Compute base activation using mixed probes\n",
    "    all_probes = []\n",
    "    for task_name, task_data in task_data_cache.items():\n",
    "        all_probes.extend(get_probes(task_data, 8))\n",
    "    \n",
    "    base_activation = get_activation(base_model, tokenizer, all_probes, device)\n",
    "    np.save(deltas_dir / \"base_activation.npy\", base_activation.cpu().float().numpy())\n",
    "    print(f\"Base activation: {base_activation.shape}\")\n",
    "    \n",
    "    # Compute per-adapter deltas\n",
    "    delta_manifest = {\"base_activation_file\": \"base_activation.npy\", \"adapters\": {}}\n",
    "    \n",
    "    for adapter_info in tqdm(adapters, desc=\"Computing deltas\"):\n",
    "        adapter_name = adapter_info[\"name\"]\n",
    "        task_name = adapter_info[\"task\"]\n",
    "        adapter_path = adapter_info[\"path\"]  # Already includes /adapter\n",
    "        \n",
    "        # Task-specific probes\n",
    "        task_probes = get_probes(task_data_cache[task_name], TASKS[task_name][\"delta_probes\"])\n",
    "        \n",
    "        # Get base activation for these specific probes\n",
    "        base_act = get_activation(base_model, tokenizer, task_probes, device)\n",
    "        \n",
    "        # Load adapter and get activation\n",
    "        adapted = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        adapted.eval()\n",
    "        adapted_act = get_activation(adapted, tokenizer, task_probes, device)\n",
    "        \n",
    "        # Delta\n",
    "        delta = (adapted_act - base_act).cpu().float().numpy()\n",
    "        \n",
    "        # Save\n",
    "        delta_file = f\"{adapter_name}_delta.npy\"\n",
    "        np.save(deltas_dir / delta_file, delta)\n",
    "        \n",
    "        delta_manifest[\"adapters\"][adapter_name] = {\n",
    "            \"adapter_path\": adapter_path,\n",
    "            \"delta_file\": delta_file,\n",
    "            \"task\": task_name,\n",
    "            \"delta_norm\": float(np.linalg.norm(delta)),\n",
    "        }\n",
    "        \n",
    "        # Cleanup\n",
    "        del adapted\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save manifest\n",
    "    with open(deltas_dir / \"delta_manifest.json\", \"w\") as f:\n",
    "        json.dump(delta_manifest, f, indent=2)\n",
    "    \n",
    "    # Cleanup base model\n",
    "    del base_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nDeltas saved to: {deltas_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute deltas for all trained adapters\n",
    "compute_all_deltas(all_adapters, task_data_cache, tokenizer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary DataFrame\n",
    "df = pd.DataFrame(all_adapters)\n",
    "print(\"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAdapters trained: {len(all_adapters)}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\nPer-task metrics:\")\n",
    "summary = df.groupby(\"task\").agg({\n",
    "    \"train_loss\": [\"mean\", \"std\"],\n",
    "    \"eval_loss\": [\"mean\", \"std\"],\n",
    "}).round(4)\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  {OUTPUT_DIR}/manifest.json\")\n",
    "print(f\"  {OUTPUT_DIR}/deltas/delta_manifest.json\")\n",
    "for a in all_adapters:\n",
    "    print(f\"  {a['path']}/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}