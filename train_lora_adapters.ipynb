{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LoRA Adapters for Ablation Studies\n",
    "\n",
    "This notebook trains real LoRA adapters by fine-tuning Qwen2.5-0.5B on various tasks.\n",
    "Run this on Colab with GPU, then use the adapters in phase 4.5 ablations.\n",
    "\n",
    "**Tasks:**\n",
    "- ARC-e (science reasoning)\n",
    "- BoolQ (boolean QA)\n",
    "- GSM8K (math)\n",
    "\n",
    "**Output:** ~9 LoRA adapters saved to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport sys\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    !pip install -q transformers peft accelerate safetensors bitsandbytes\n    \n    # Use Drive paths\n    DRIVE_ROOT = '/content/drive/MyDrive/llgbm'\n    DATA_DIR = f'{DRIVE_ROOT}/data'\n    OUTPUT_DIR = f'{DRIVE_ROOT}/checkpoints'\nelse:\n    DATA_DIR = 'data'\n    OUTPUT_DIR = 'checkpoints'\n\nimport os\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Data dir: {DATA_DIR}\")\nprint(f\"Output dir: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Model\n",
    "    model_name: str = \"Qwen/Qwen2.5-0.5B\"\n",
    "    \n",
    "    # LoRA\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: tuple = (\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\")\n",
    "    \n",
    "    # Training\n",
    "    num_epochs: int = 2\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    max_length: int = 384\n",
    "    warmup_ratio: float = 0.1\n",
    "    \n",
    "    # Data\n",
    "    samples_per_adapter: int = 400\n",
    "    adapters_per_task: int = 3\n",
    "\n",
    "config = Config()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"LoRA: rank={config.lora_rank}, alpha={config.lora_alpha}\")\n",
    "print(f\"Training: {config.num_epochs} epochs, batch_size={config.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task definitions\n",
    "TASKS = {\n",
    "    \"arc_e\": {\n",
    "        \"file\": \"ARC-e_train.json\",\n",
    "        \"samples\": 400,\n",
    "        \"adapters\": 3,\n",
    "    },\n",
    "    \"boolq\": {\n",
    "        \"file\": \"BoolQ_train.json\",\n",
    "        \"samples\": 400,\n",
    "        \"adapters\": 3,\n",
    "    },\n",
    "    \"gsm8k\": {\n",
    "        \"file\": \"GSM8K_train.json\",\n",
    "        \"samples\": 300,\n",
    "        \"adapters\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Check data files exist\n",
    "for task, info in TASKS.items():\n",
    "    path = Path(DATA_DIR) / info[\"file\"]\n",
    "    exists = path.exists()\n",
    "    print(f\"{task}: {path.name} {'[OK]' if exists else '[MISSING]'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    \"\"\"Simple SFT dataset for instruction tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # Build chat format\n",
    "        system = item.get(\"system\", \"You are a helpful assistant.\")\n",
    "        prompt = item[\"prompt\"]\n",
    "        response = item[\"response\"]\n",
    "\n",
    "        # Qwen chat format\n",
    "        text = f\"<|im_start|>system\\n{system}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Labels: same as input_ids, with padding tokens set to -100\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adapter(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_data: List[Dict],\n",
    "    output_dir: Path,\n",
    "    adapter_name: str,\n",
    "    config: Config,\n",
    "):\n",
    "    \"\"\"Train a single LoRA adapter and save it.\"\"\"\n",
    "\n",
    "    print(f\"\\n  Training: {adapter_name} ({len(train_data)} samples)\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = SFTDataset(train_data, tokenizer, max_length=config.max_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "    num_training_steps = len(dataloader) * config.num_epochs\n",
    "    num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    global_step = 0\n",
    "\n",
    "    progress = tqdm(total=num_training_steps, desc=f\"  {adapter_name}\", leave=False)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                progress.set_postfix(loss=f\"{total_loss / global_step:.4f}\")\n",
    "            progress.update(1)\n",
    "\n",
    "    progress.close()\n",
    "\n",
    "    # Save adapter\n",
    "    adapter_dir = output_dir / adapter_name\n",
    "    adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(adapter_dir)\n",
    "\n",
    "    # Save training prompts (for conditioning)\n",
    "    prompts = [item[\"prompt\"] for item in train_data[:128]]\n",
    "    with open(adapter_dir / \"prompts.json\", \"w\") as f:\n",
    "        json.dump({\"prompts\": prompts, \"task\": adapter_name}, f, indent=2)\n",
    "\n",
    "    avg_loss = total_loss / global_step\n",
    "    print(f\"  Saved: {adapter_dir} (loss={avg_loss:.4f})\")\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(config: Config):\n",
    "    \"\"\"Load base model and apply LoRA.\"\"\"\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config.lora_rank,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        target_modules=list(config.target_modules),\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adapters = []\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "\n",
    "for task_name, task_info in TASKS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task: {task_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load data\n",
    "    data_file = Path(DATA_DIR) / task_info[\"file\"]\n",
    "    with open(data_file) as f:\n",
    "        task_data = json.load(f)\n",
    "    print(f\"Loaded {len(task_data)} samples\")\n",
    "\n",
    "    # Train multiple adapters\n",
    "    for adapter_idx in range(task_info[\"adapters\"]):\n",
    "        # Fresh model for each adapter\n",
    "        print(f\"\\nAdapter {adapter_idx + 1}/{task_info['adapters']}\")\n",
    "        model = create_lora_model(config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        # Select data subset\n",
    "        samples = task_info[\"samples\"]\n",
    "        start_idx = adapter_idx * samples\n",
    "        end_idx = start_idx + samples\n",
    "        \n",
    "        if end_idx > len(task_data):\n",
    "            subset = task_data[start_idx:] + task_data[:end_idx - len(task_data)]\n",
    "        else:\n",
    "            subset = task_data[start_idx:end_idx]\n",
    "\n",
    "        adapter_name = f\"{task_name}_{adapter_idx:03d}\"\n",
    "\n",
    "        # Train\n",
    "        loss = train_adapter(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_data=subset,\n",
    "            output_dir=output_path / task_name,\n",
    "            adapter_name=adapter_name,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        all_adapters.append({\n",
    "            \"name\": adapter_name,\n",
    "            \"task\": task_name,\n",
    "            \"path\": str(output_path / task_name / adapter_name),\n",
    "            \"loss\": loss,\n",
    "            \"samples\": len(subset),\n",
    "        })\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n\\nTrained {len(all_adapters)} adapters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = {\n",
    "    \"model_name\": config.model_name,\n",
    "    \"lora_config\": {\n",
    "        \"rank\": config.lora_rank,\n",
    "        \"alpha\": config.lora_alpha,\n",
    "        \"target_modules\": list(config.target_modules),\n",
    "    },\n",
    "    \"adapters\": all_adapters,\n",
    "}\n",
    "\n",
    "with open(output_path / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Manifest saved to: {output_path / 'manifest.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Deltas\n",
    "\n",
    "Now compute the delta activations for each adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probes for delta computation\n",
    "PROBES = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"In a hole in the ground there lived a hobbit.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"The only thing we have to fear is fear itself.\",\n",
    "    \"It was the best of times, it was the worst of times.\",\n",
    "    \"All happy families are alike; each unhappy family is unhappy in its own way.\",\n",
    "    \"Call me Ishmael.\",\n",
    "    \"It is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from peft import PeftModel\n",
    "\n",
    "def compute_activation(model, tokenizer, probes: List[str], device: str) -> torch.Tensor:\n",
    "    \"\"\"Compute average last-layer, last-token activation over probes.\"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for probe in probes:\n",
    "            inputs = tokenizer(probe, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            \n",
    "            # Last layer, last token\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "            seq_len = inputs[\"attention_mask\"].sum().item()\n",
    "            last_token_hidden = last_hidden[0, seq_len - 1, :]\n",
    "            activations.append(last_token_hidden)\n",
    "    \n",
    "    return torch.stack(activations).mean(dim=0)\n",
    "\n",
    "\n",
    "def compute_delta(base_model, adapter_path: str, tokenizer, probes: List[str], device: str) -> np.ndarray:\n",
    "    \"\"\"Compute delta = activation(adapted) - activation(base).\"\"\"\n",
    "    \n",
    "    # Base activation\n",
    "    base_act = compute_activation(base_model, tokenizer, probes, device)\n",
    "    \n",
    "    # Load adapter\n",
    "    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    adapted_model.eval()\n",
    "    \n",
    "    # Adapted activation\n",
    "    adapted_act = compute_activation(adapted_model, tokenizer, probes, device)\n",
    "    \n",
    "    # Delta\n",
    "    delta = (adapted_act - base_act).cpu().float().numpy()\n",
    "    \n",
    "    # Cleanup\n",
    "    del adapted_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model for delta computation...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "base_model.config.output_hidden_states = True\n",
    "base_model.eval()\n",
    "\n",
    "# Compute base activation once\n",
    "print(\"Computing base activation...\")\n",
    "base_activation = compute_activation(base_model, tokenizer, PROBES, device)\n",
    "print(f\"Base activation shape: {base_activation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deltas directory\n",
    "deltas_dir = output_path / \"deltas\"\n",
    "deltas_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save base activation\n",
    "np.save(deltas_dir / \"base_activation.npy\", base_activation.cpu().float().numpy())\n",
    "\n",
    "# Compute deltas for each adapter\n",
    "delta_manifest = {\n",
    "    \"base_activation_file\": \"base_activation.npy\",\n",
    "    \"probes\": PROBES,\n",
    "    \"adapters\": {},\n",
    "}\n",
    "\n",
    "for adapter_info in tqdm(all_adapters, desc=\"Computing deltas\"):\n",
    "    adapter_path = adapter_info[\"path\"]\n",
    "    adapter_name = adapter_info[\"name\"]\n",
    "    \n",
    "    print(f\"\\nComputing delta for {adapter_name}...\")\n",
    "    \n",
    "    delta = compute_delta(base_model, adapter_path, tokenizer, PROBES, device)\n",
    "    \n",
    "    # Save delta\n",
    "    delta_file = f\"{adapter_name}_delta.npy\"\n",
    "    np.save(deltas_dir / delta_file, delta)\n",
    "    \n",
    "    delta_manifest[\"adapters\"][adapter_name] = {\n",
    "        \"adapter_path\": adapter_path,\n",
    "        \"delta_file\": delta_file,\n",
    "        \"delta_norm\": float(np.linalg.norm(delta)),\n",
    "    }\n",
    "    \n",
    "    print(f\"  Delta norm: {np.linalg.norm(delta):.4f}\")\n",
    "\n",
    "# Save delta manifest\n",
    "with open(deltas_dir / \"delta_manifest.json\", \"w\") as f:\n",
    "    json.dump(delta_manifest, f, indent=2)\n",
    "\n",
    "print(f\"\\nDeltas saved to: {deltas_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAdapters trained: {len(all_adapters)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"\\nPer-task breakdown:\")\n",
    "for task in TASKS:\n",
    "    task_adapters = [a for a in all_adapters if a[\"task\"] == task]\n",
    "    avg_loss = sum(a[\"loss\"] for a in task_adapters) / len(task_adapters)\n",
    "    print(f\"  {task}: {len(task_adapters)} adapters, avg_loss={avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - {OUTPUT_DIR}/manifest.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/deltas/delta_manifest.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/deltas/base_activation.npy\")\n",
    "for a in all_adapters:\n",
    "    print(f\"  - {a['path']}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify structure\n",
    "!ls -la {OUTPUT_DIR}/\n",
    "print()\n",
    "!ls -la {OUTPUT_DIR}/deltas/ 2>/dev/null || echo \"No deltas dir yet\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}