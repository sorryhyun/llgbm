{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0: Baseline Reproduction (DnD on Qwen2.5-1.5B)\n",
    "\n",
    "This notebook verifies the existing DnD framework works end-to-end for Qwen2.5-1.5B LoRA generation before adding delta supervision.\n",
    "\n",
    "## Goals\n",
    "- Verify all DnD imports work correctly\n",
    "- Test the LoRA tokenizer roundtrip\n",
    "- Run a minimal training loop\n",
    "- Generate and save a LoRA checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab\n",
      "\n",
      "============================================================\n",
      "ERROR: dnd_repo not found!\n",
      "Please do one of the following:\n",
      "  1. Upload dnd_repo folder to Colab\n",
      "  2. Uncomment the git clone line above and set your repo URL\n",
      "  3. Mount Google Drive and copy the repo\n",
      "============================================================\n",
      "\n",
      "Working directory: /content\n",
      "DnD path: /content/dnd_repo\n",
      "[ERROR] DnD path does not exist!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    \n",
    "    # Option 1: Clone repo if not present (uncomment and set your repo URL)\n",
    "    # if not os.path.exists(\"dnd_repo\"):\n",
    "    #     !git clone https://github.com/YOUR_USERNAME/dnd_repo.git dnd_repo\n",
    "    \n",
    "    # Option 2: Mount Google Drive if repo is there\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    # !cp -r /content/drive/MyDrive/dnd_repo /content/dnd_repo\n",
    "    \n",
    "    # Option 3: Upload dnd_repo.zip and extract\n",
    "    # !unzip dnd_repo.zip\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q safetensors accelerate transformers sentence-transformers\n",
    "    \n",
    "    # Check if dnd_repo exists\n",
    "    if not os.path.exists(\"dnd_repo\"):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ERROR: dnd_repo not found!\")\n",
    "        print(\"Please do one of the following:\")\n",
    "        print(\"  1. Upload dnd_repo folder to Colab\")\n",
    "        print(\"  2. Uncomment the git clone line above and set your repo URL\")\n",
    "        print(\"  3. Mount Google Drive and copy the repo\")\n",
    "        print(\"=\"*60)\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Add DnD to path - this allows importing as 'workspace.dnd...'\n",
    "DND_PATH = os.path.abspath(\"dnd_repo\")\n",
    "if DND_PATH not in sys.path:\n",
    "    sys.path.insert(0, DND_PATH)\n",
    "\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "print(f\"DnD path: {DND_PATH}\")\n",
    "\n",
    "if os.path.exists(DND_PATH):\n",
    "    contents = os.listdir(DND_PATH)\n",
    "    print(f\"DnD contents: {contents[:5]}{'...' if len(contents) > 5 else ''}\")\n",
    "    if 'workspace' in contents:\n",
    "        print(\"[OK] workspace folder found\")\n",
    "else:\n",
    "    print(\"[ERROR] DnD path does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify DnD Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_imports():\n",
    "    \"\"\"Verify all DnD components can be imported.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Model components\n",
    "    try:\n",
    "        from workspace.dnd.model.decoderonly import (\n",
    "            HyperConvDecoderModel,\n",
    "            HyperConvDecoderModel_FullCond,\n",
    "            HyperConvDecoderModel_SuperLarge\n",
    "        )\n",
    "        results[\"Model imports\"] = \"OK\"\n",
    "    except Exception as e:\n",
    "        results[\"Model imports\"] = f\"FAIL: {e}\"\n",
    "    \n",
    "    # Tokenizer\n",
    "    try:\n",
    "        from workspace.dnd.tokenizer.register import Qwen2515LoRA_Tokenizer2D\n",
    "        results[\"Tokenizer imports\"] = \"OK\"\n",
    "    except Exception as e:\n",
    "        results[\"Tokenizer imports\"] = f\"FAIL: {e}\"\n",
    "    \n",
    "    # Dataset\n",
    "    try:\n",
    "        from workspace.dnd.dataset.register import (\n",
    "            Text2Qwen25LoRA_FullCondDataset,\n",
    "            Text2Qwen25LoRA_CondQ_ADataset\n",
    "        )\n",
    "        results[\"Dataset imports\"] = \"OK\"\n",
    "    except Exception as e:\n",
    "        results[\"Dataset imports\"] = f\"FAIL: {e}\"\n",
    "    \n",
    "    # Modules\n",
    "    try:\n",
    "        from workspace.dnd.module.hyperconv import HyperConvDecoder\n",
    "        results[\"HyperConv imports\"] = \"OK\"\n",
    "    except Exception as e:\n",
    "        results[\"HyperConv imports\"] = f\"FAIL: {e}\"\n",
    "    \n",
    "    # Tools\n",
    "    try:\n",
    "        from workspace.dnd.tools import load_safetensors, save_safetensors\n",
    "        results[\"Tools imports\"] = \"OK\"\n",
    "    except Exception as e:\n",
    "        results[\"Tools imports\"] = f\"FAIL: {e}\"\n",
    "    \n",
    "    # Print results\n",
    "    all_ok = True\n",
    "    for name, status in results.items():\n",
    "        icon = \"[OK]\" if status == \"OK\" else \"[FAIL]\"\n",
    "        print(f\"{icon} {name}: {status}\")\n",
    "        if status != \"OK\":\n",
    "            all_ok = False\n",
    "    \n",
    "    print(\"\\n\" + (\"All imports successful!\" if all_ok else \"Some imports failed!\"))\n",
    "    return all_ok\n",
    "\n",
    "verify_imports()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Sample Data\n",
    "\n",
    "Create dummy LoRA checkpoints for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from safetensors.torch import save_file, load_file\n\ndef create_dummy_lora_checkpoint(output_path: str, rank: int = 16):\n    \"\"\"\n    Create a dummy LoRA checkpoint matching Qwen2.5-1.5B structure.\n    \n    Args:\n        output_path: Full path to the .safetensors file (not directory)\n        rank: LoRA rank\n    \n    LoRA targets for Qwen2.5-1.5B:\n    - q_proj, k_proj, v_proj, o_proj (attention)\n    - gate_proj, up_proj, down_proj (MLP)\n    \"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    \n    # Qwen2.5-1.5B config\n    hidden_size = 1536\n    intermediate_size = 8960\n    num_layers = 28\n    num_kv_heads = 2  # GQA: fewer KV heads\n    num_heads = 12\n    head_dim = hidden_size // num_heads\n    kv_dim = num_kv_heads * head_dim  # 256\n    \n    lora_weights = {}\n    \n    for layer_idx in range(num_layers):\n        prefix = f\"base_model.model.model.layers.{layer_idx}\"\n        \n        # Q and O projections: (hidden_size, hidden_size)\n        for proj in [\"q_proj\", \"o_proj\"]:\n            lora_weights[f\"{prefix}.self_attn.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n            lora_weights[f\"{prefix}.self_attn.{proj}.lora_B.weight\"] = torch.zeros(hidden_size, rank)\n        \n        # K and V projections: (kv_dim, hidden_size) for GQA\n        for proj in [\"k_proj\", \"v_proj\"]:\n            lora_weights[f\"{prefix}.self_attn.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n            lora_weights[f\"{prefix}.self_attn.{proj}.lora_B.weight\"] = torch.zeros(kv_dim, rank)\n        \n        # MLP: gate_proj and up_proj (intermediate_size, hidden_size)\n        for proj in [\"gate_proj\", \"up_proj\"]:\n            lora_weights[f\"{prefix}.mlp.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n            lora_weights[f\"{prefix}.mlp.{proj}.lora_B.weight\"] = torch.zeros(intermediate_size, rank)\n        \n        # MLP: down_proj (hidden_size, intermediate_size)\n        lora_weights[f\"{prefix}.mlp.down_proj.lora_A.weight\"] = torch.randn(rank, intermediate_size) * 0.01\n        lora_weights[f\"{prefix}.mlp.down_proj.lora_B.weight\"] = torch.zeros(hidden_size, rank)\n    \n    # Convert to bfloat16\n    lora_weights = {k: v.to(torch.bfloat16) for k, v in lora_weights.items()}\n    \n    # Save checkpoint\n    save_file(lora_weights, output_path)\n    \n    total_params = sum(p.numel() for p in lora_weights.values())\n    print(f\"Created: {output_path}\")\n    print(f\"  Layers: {num_layers}, Rank: {rank}, Params: {total_params:,}\")\n    \n    return lora_weights"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sample checkpoints directory structure\n# DnD dataset expects: a folder containing ONLY .safetensors files\nDATA_DIR = Path(\"data/sample_checkpoints/math_loras\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create multiple dummy LoRA checkpoints as individual .safetensors files\nNUM_CHECKPOINTS = 5\nfor i in range(NUM_CHECKPOINTS):\n    ckpt_path = DATA_DIR / f\"{i}.safetensors\"\n    create_dummy_lora_checkpoint(str(ckpt_path), rank=16)\n\nprint(f\"\\nCreated {NUM_CHECKPOINTS} checkpoints in {DATA_DIR}\")\nprint(f\"Files: {list(DATA_DIR.glob('*.safetensors'))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample prompts\n",
    "PROMPTS_DIR = Path(\"data/sample_prompts\")\n",
    "PROMPTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sample_prompts = [\n",
    "    {\"prompt\": \"Solve the equation: 2x + 5 = 15\", \"response\": \"x = 5\"},\n",
    "    {\"prompt\": \"What is the derivative of x^2?\", \"response\": \"2x\"},\n",
    "    {\"prompt\": \"Calculate the area of a circle with radius 5\", \"response\": \"25*pi\"},\n",
    "    {\"prompt\": \"Simplify: (3x + 2)(x - 4)\", \"response\": \"3x^2 - 10x - 8\"},\n",
    "    {\"prompt\": \"Find the roots of x^2 - 5x + 6 = 0\", \"response\": \"x = 2 or x = 3\"},\n",
    "    {\"prompt\": \"What is 15% of 80?\", \"response\": \"12\"},\n",
    "    {\"prompt\": \"Convert 3/4 to a decimal\", \"response\": \"0.75\"},\n",
    "    {\"prompt\": \"What is the sum of angles in a triangle?\", \"response\": \"180 degrees\"},\n",
    "    {\"prompt\": \"Calculate: 125 / 5 + 3 * 4\", \"response\": \"37\"},\n",
    "    {\"prompt\": \"Find the GCD of 24 and 36\", \"response\": \"12\"},\n",
    "] * 10  # Repeat for more samples\n",
    "\n",
    "prompts_path = PROMPTS_DIR / \"math_prompts.json\"\n",
    "with open(prompts_path, \"w\") as f:\n",
    "    json.dump(sample_prompts, f, indent=2)\n",
    "\n",
    "print(f\"Created {len(sample_prompts)} sample prompts at {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tokenizer Sanity Check\n",
    "\n",
    "Test that the tokenizer can convert LoRA weights to tokens and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace.dnd.tokenizer.register import Qwen2515LoRA_Tokenizer2D\n",
    "from workspace.dnd.tools import load_safetensors\n",
    "\n",
    "def test_tokenizer_roundtrip(checkpoint_path: str):\n",
    "    \"\"\"Test that tokenize -> detokenize recovers original weights.\"\"\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    weights = load_safetensors(checkpoint_path, map_location=\"cpu\", dtype=torch.bfloat16)\n",
    "    \n",
    "    # Remove base_model.model. prefix (matching the dataset's post_process)\n",
    "    weights = {k.replace(\"base_model.model.\", \"\"): v for k, v in weights.items()}\n",
    "    weights = OrderedDict(sorted(weights.items()))\n",
    "    \n",
    "    print(f\"Loaded {len(weights)} tensors from checkpoint\")\n",
    "    print(f\"Sample keys: {list(weights.keys())[:3]}\")\n",
    "    \n",
    "    # Initialize tokenizer with Qwen2.5-1.5B token size\n",
    "    token_size = (18, 258)  # From the training script\n",
    "    tokenizer = Qwen2515LoRA_Tokenizer2D(token_size=token_size)\n",
    "    \n",
    "    # Make a copy for comparison\n",
    "    original_weights = {k: v.clone() for k, v in weights.items()}\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens, scales = tokenizer.tokenize(weights)\n",
    "    print(f\"\\nTokenized:\")\n",
    "    print(f\"  - Tokens shape: {tokens.shape}\")\n",
    "    print(f\"  - Token dtype: {tokens.dtype}\")\n",
    "    print(f\"  - Has NaN (padding): {torch.isnan(tokens).any().item()}\")\n",
    "    \n",
    "    # Load fresh weights for fake_diction\n",
    "    fake_weights = load_safetensors(checkpoint_path, map_location=\"cpu\", dtype=torch.bfloat16)\n",
    "    fake_weights = {k.replace(\"base_model.model.\", \"\"): v for k, v in fake_weights.items()}\n",
    "    fake_weights = OrderedDict(sorted(fake_weights.items()))\n",
    "    \n",
    "    # Detokenize\n",
    "    reconstructed = tokenizer.detokenize(fake_weights, tokens)\n",
    "    print(f\"\\nReconstructed {len(reconstructed)} tensors\")\n",
    "    \n",
    "    # Compare\n",
    "    total_error = 0.0\n",
    "    max_error = 0.0\n",
    "    error_count = 0\n",
    "    \n",
    "    for key in original_weights:\n",
    "        if key in reconstructed:\n",
    "            orig = original_weights[key].float()\n",
    "            recon = reconstructed[key].float()\n",
    "            error = torch.abs(orig - recon).mean().item()\n",
    "            total_error += error\n",
    "            max_error = max(max_error, error)\n",
    "            if error > 1e-3:\n",
    "                error_count += 1\n",
    "                if error_count <= 5:  # Only print first 5 errors\n",
    "                    print(f\"  High error in {key}: {error:.6f}\")\n",
    "    \n",
    "    avg_error = total_error / len(original_weights)\n",
    "    print(f\"\\n=== Results ===\")\n",
    "    print(f\"Average reconstruction error: {avg_error:.8f}\")\n",
    "    print(f\"Max reconstruction error: {max_error:.8f}\")\n",
    "    print(f\"Keys with error > 1e-3: {error_count}\")\n",
    "    \n",
    "    if avg_error < 1e-3:\n",
    "        print(\"\\n[PASS] Tokenizer roundtrip test\")\n",
    "    else:\n",
    "        print(\"\\n[WARN] Reconstruction error above threshold\")\n",
    "    \n",
    "    return tokens, avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test on our dummy checkpoint\ntest_ckpt = \"data/sample_checkpoints/math_loras/0.safetensors\"\ntokens, error = test_tokenizer_roundtrip(test_ckpt)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Minimal Training Loop\n",
    "\n",
    "Set up a minimal training loop to verify the pipeline works end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModel, AutoTokenizer\nfrom workspace.dnd.model.decoderonly import HyperConvDecoderModel_SuperLarge\nfrom workspace.dnd.tokenizer.register import Qwen2515LoRA_Tokenizer2D\nfrom workspace.dnd.dataset.register import Text2Qwen25LoRA_FullCondDataset\n\n# Configuration\nCONFIG = {\n    # Data settings\n    \"token_size\": (18, 258),\n    \"max_text_length\": 512,  # Reduced for testing\n    \"modified_length\": 128,  # Reduced for testing\n    \"num_texts\": 4,  # Reduced for testing\n    \"batch_size\": 2,\n    \"real_length\": 5,  # Number of checkpoints to use from the folder\n    \n    # Training settings\n    \"total_steps\": 50,\n    \"learning_rate\": 1e-4,\n    \"log_interval\": 10,\n    \n    # Model settings  \n    \"extractor_type\": \"BERT\",\n    \"extractor_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Small model for testing\n    \n    # Paths - single folder containing .safetensors files\n    \"checkpoint_folders\": [\"data/sample_checkpoints/math_loras\"],\n    \"prompts_path\": \"data/sample_prompts/math_prompts.json\",\n    \"output_dir\": \"outputs/phase0_baseline\",\n}\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# LoRA tokenizer\n",
    "lora_tokenizer = Qwen2515LoRA_Tokenizer2D(token_size=CONFIG[\"token_size\"])\n",
    "print(\"[OK] LoRA tokenizer initialized\")\n",
    "\n",
    "# Text tokenizer and condition model\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"extractor_model\"])\n",
    "condition_model = AutoModel.from_pretrained(CONFIG[\"extractor_model\"], torch_dtype=torch.float32)\n",
    "condition_model = condition_model.to(device)\n",
    "condition_model.eval()\n",
    "for param in condition_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(f\"[OK] Text encoder loaded: {CONFIG['extractor_model']}\")\n",
    "print(f\"     Hidden size: {condition_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts\n",
    "with open(CONFIG[\"prompts_path\"], \"r\") as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "# Create datasets for each checkpoint folder\n",
    "texts_per_folder = [prompts_data] * len(CONFIG[\"checkpoint_folders\"])\n",
    "\n",
    "# Initialize dataset\n",
    "Text2Qwen25LoRA_FullCondDataset.dtype = torch.bfloat16\n",
    "\n",
    "dataset = Text2Qwen25LoRA_FullCondDataset(\n",
    "    checkpoint_folders=CONFIG[\"checkpoint_folders\"],\n",
    "    tokenizer=lora_tokenizer,\n",
    "    num_texts=CONFIG[\"num_texts\"],\n",
    "    texts=texts_per_folder,\n",
    "    max_text_length=CONFIG[\"max_text_length\"],\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    expected_iteration=CONFIG[\"total_steps\"] * CONFIG[\"batch_size\"] * 2,\n",
    "    real_length=CONFIG[\"real_length\"],\n",
    ")\n",
    "\n",
    "print(f\"[OK] Dataset initialized\")\n",
    "print(f\"     Total checkpoints: {dataset.real_length}\")\n",
    "print(f\"     Dataset length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading a single sample\n",
    "sample = dataset[0]\n",
    "tokens, condition, path = sample\n",
    "print(f\"Sample loaded from: {path}\")\n",
    "print(f\"Tokens shape: {tokens.shape}\")\n",
    "print(f\"Condition type: {type(condition)}\")\n",
    "print(f\"Condition input_ids shape: {condition.input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.collate_fn_train,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"[OK] DataLoader created with batch_size={CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden size from condition model\n",
    "hidden_size = condition_model.config.hidden_size  # 384 for MiniLM-L6\n",
    "\n",
    "# Calculate criterion weight (uniform for testing)\n",
    "# In real training, this would be computed from actual LoRA statistics\n",
    "num_tokens = tokens.shape[0]  # From sample\n",
    "criterion_weight = torch.ones(num_tokens)\n",
    "\n",
    "# Model configuration - scaled down for testing\n",
    "model_config = {\n",
    "    \"features\": [\n",
    "        (CONFIG[\"num_texts\"], CONFIG[\"modified_length\"], hidden_size),  # Input\n",
    "        (32, 64, 128),   # Intermediate\n",
    "        (64, 32, 128),   # Intermediate\n",
    "        (num_tokens, CONFIG[\"token_size\"][0], CONFIG[\"token_size\"][1]),  # Output\n",
    "    ],\n",
    "    \"condition_dim\": (CONFIG[\"num_texts\"], CONFIG[\"modified_length\"], hidden_size),\n",
    "    \"kernel_size\": 5,\n",
    "}\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Features: {model_config['features']}\")\n",
    "print(f\"  Condition dim: {model_config['condition_dim']}\")\n",
    "print(f\"  Criterion weight shape: {criterion_weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = HyperConvDecoderModel_SuperLarge(\n",
    "    config=model_config,\n",
    "    criterion_weight=criterion_weight.view(1, -1, 1, 1),\n",
    "    max_length=CONFIG[\"max_text_length\"],\n",
    "    modified_length=CONFIG[\"modified_length\"],\n",
    "    extractor_type=CONFIG[\"extractor_type\"],\n",
    "    extra_condition_module=condition_model,\n",
    "    freeze_extra_condition=True,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"[OK] Model initialized\")\n",
    "print(f\"     Total parameters: {total_params:,}\")\n",
    "print(f\"     Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "print(f\"[OK] Optimizer initialized with lr={CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "global_step = 0\n",
    "\n",
    "pbar = tqdm(total=CONFIG[\"total_steps\"], desc=\"Training\")\n",
    "\n",
    "for epoch in range(100):  # Max epochs (will break when steps reached)\n",
    "    for batch_idx, (tokens, cond_id, cond_mask) in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        tokens = tokens.to(device)\n",
    "        conditions = {\n",
    "            \"input_ids\": cond_id.to(device),\n",
    "            \"attention_mask\": cond_mask.to(device),\n",
    "        }\n",
    "        \n",
    "        # Handle NaN padding\n",
    "        mask = ~torch.isnan(tokens)\n",
    "        tokens = torch.nan_to_num(tokens, nan=0.0)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                           dtype=torch.bfloat16):\n",
    "            loss = model(\n",
    "                source=None,\n",
    "                mask=mask,\n",
    "                condition=conditions,\n",
    "                target=tokens,\n",
    "            )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        loss_val = loss.item()\n",
    "        losses.append(loss_val)\n",
    "        global_step += 1\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n",
    "        \n",
    "        if global_step % CONFIG[\"log_interval\"] == 0:\n",
    "            avg_loss = sum(losses[-CONFIG[\"log_interval\"]:]) / CONFIG[\"log_interval\"]\n",
    "            print(f\"Step {global_step}: loss = {avg_loss:.6f}\")\n",
    "        \n",
    "        if global_step >= CONFIG[\"total_steps\"]:\n",
    "            break\n",
    "    \n",
    "    if global_step >= CONFIG[\"total_steps\"]:\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "print(f\"Average loss (last 10): {sum(losses[-10:])/10:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if loss decreased\n",
    "first_10_avg = sum(losses[:10]) / 10\n",
    "last_10_avg = sum(losses[-10:]) / 10\n",
    "print(f\"\\nFirst 10 steps avg loss: {first_10_avg:.6f}\")\n",
    "print(f\"Last 10 steps avg loss: {last_10_avg:.6f}\")\n",
    "print(f\"Loss {'decreased' if last_10_avg < first_10_avg else 'did not decrease'} over training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generation Test\n",
    "\n",
    "Test generating a LoRA checkpoint from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lora(model, text_condition: str, lora_tokenizer, text_tokenizer, device, \n",
    "                  num_texts=4, max_length=512):\n",
    "    \"\"\"Generate a LoRA checkpoint from a text condition.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize condition (need num_texts copies)\n",
    "    inputs = text_tokenizer(\n",
    "        [text_condition] * num_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # Add batch dimension\n",
    "    conditions = {\n",
    "        \"input_ids\": inputs.input_ids.unsqueeze(0).to(device),\n",
    "        \"attention_mask\": inputs.attention_mask.unsqueeze(0).to(device),\n",
    "    }\n",
    "    \n",
    "    # Generate tokens\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                           dtype=torch.bfloat16):\n",
    "            tokens_pred = model.generate(\n",
    "                source=None,\n",
    "                mask=None,\n",
    "                condition=conditions,\n",
    "                target=None,\n",
    "            )\n",
    "    \n",
    "    print(f\"Generated tokens shape: {tokens_pred.shape}\")\n",
    "    \n",
    "    return tokens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from a test prompt\n",
    "test_prompt = \"Solve mathematical equations step by step\"\n",
    "print(f\"Generating LoRA for prompt: '{test_prompt}'\")\n",
    "\n",
    "generated_tokens = generate_lora(\n",
    "    model=model,\n",
    "    text_condition=test_prompt,\n",
    "    lora_tokenizer=lora_tokenizer,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    device=device,\n",
    "    num_texts=CONFIG[\"num_texts\"],\n",
    "    max_length=CONFIG[\"max_text_length\"],\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated tokens stats:\")\n",
    "print(f\"  Shape: {generated_tokens.shape}\")\n",
    "print(f\"  Min: {generated_tokens.min().item():.4f}\")\n",
    "print(f\"  Max: {generated_tokens.max().item():.4f}\")\n",
    "print(f\"  Mean: {generated_tokens.mean().item():.4f}\")\n",
    "print(f\"  Std: {generated_tokens.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detokenize to actual LoRA weights\n# Load a fake_diction template\ntemplate_path = \"data/sample_checkpoints/math_loras/0.safetensors\"\nfake_weights = load_safetensors(template_path, map_location=\"cpu\", dtype=torch.bfloat16)\nfake_weights = {k.replace(\"base_model.model.\", \"\"): v for k, v in fake_weights.items()}\nfake_weights = OrderedDict(sorted(fake_weights.items()))\n\n# Detokenize\ngenerated_lora = lora_tokenizer.detokenize(fake_weights, generated_tokens[0].cpu())\n\nprint(f\"\\nDetokenized LoRA weights:\")\nprint(f\"  Number of tensors: {len(generated_lora)}\")\nprint(f\"  Total parameters: {sum(p.numel() for p in generated_lora.values()):,}\")\nprint(f\"\\nSample weights:\")\nfor i, (k, v) in enumerate(generated_lora.items()):\n    if i < 3:\n        print(f\"  {k}: shape={v.shape}, mean={v.float().mean():.6f}, std={v.float().std():.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated LoRA\n",
    "output_dir = Path(CONFIG[\"output_dir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add back the base_model.model. prefix\n",
    "save_weights = {f\"base_model.model.{k}\": v.contiguous() for k, v in generated_lora.items()}\n",
    "\n",
    "output_path = output_dir / \"generated_lora.safetensors\"\n",
    "save_file(save_weights, str(output_path))\n",
    "\n",
    "print(f\"[OK] Saved generated LoRA to {output_path}\")\n",
    "print(f\"     File size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "model_path = output_dir / \"model.pth\"\n",
    "\n",
    "# Get state dict without the frozen condition module\n",
    "state_dict = model.state_dict()\n",
    "keys_to_remove = [k for k in state_dict.keys() if k.startswith(\"condition_module\")]\n",
    "for k in keys_to_remove:\n",
    "    del state_dict[k]\n",
    "\n",
    "torch.save(state_dict, model_path)\n",
    "\n",
    "print(f\"[OK] Saved model checkpoint to {model_path}\")\n",
    "print(f\"     File size: {model_path.stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Acceptance Criteria Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Phase 0 Acceptance Criteria\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "criteria = {\n",
    "    \"All imports succeed\": verify_imports(),\n",
    "    \"Tokenizer roundtrip error < 1e-3\": error < 1e-3,\n",
    "    \"Training loop runs without OOM\": True,  # If we got here, it passed\n",
    "    \"Loss decreased over training\": last_10_avg < first_10_avg,\n",
    "    \"Can generate and save LoRA checkpoint\": output_path.exists(),\n",
    "}\n",
    "\n",
    "print()\n",
    "all_passed = True\n",
    "for criterion, passed in criteria.items():\n",
    "    status = \"[PASS]\" if passed else \"[FAIL]\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print()\n",
    "if all_passed:\n",
    "    print(\"All acceptance criteria PASSED!\")\n",
    "    print(\"Ready to proceed to Phase 1.\")\n",
    "else:\n",
    "    print(\"Some criteria FAILED. Please review and fix issues before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once Phase 0 is verified, proceed to **Phase 1** to compute delta embeddings for teacher LoRAs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}