{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 1: Define Delta Targets (Teacher Signals) Offline\n",
    "\n",
    "This notebook computes and caches delta embeddings for all teacher LoRA checkpoints. These serve as supervision targets for behavioral matching.\n",
    "\n",
    "## Goals\n",
    "- Compute base model activation on probe texts\n",
    "- Compute delta embeddings for each teacher LoRA\n",
    "- Cache results for use in training\n",
    "- Visualize delta embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport shutil\n\n# Detect environment\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab\")\n    \n    # Mount Google Drive for persistence\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Create project directory on Drive\n    DRIVE_PROJECT_DIR = '/content/drive/MyDrive/llgbm'\n    os.makedirs(DRIVE_PROJECT_DIR, exist_ok=True)\n    print(f\"Drive project dir: {DRIVE_PROJECT_DIR}\")\n    \n    # Install dependencies\n    !pip install -q safetensors accelerate transformers peft\n    !pip install -q scikit-learn matplotlib seaborn\n    \n    # Upload or clone your repo containing llgbm package\n    # Option 1: Upload llgbm folder\n    # Option 2: Clone from git\n    # !git clone https://github.com/YOUR_USERNAME/llgbm.git\n    \n    if not os.path.exists(\"llgbm\"):\n        print(\"\\n\" + \"=\"*60)\n        print(\"ERROR: llgbm package not found!\")\n        print(\"Please upload the llgbm folder or clone your repo.\")\n        print(\"=\"*60)\nelse:\n    print(\"Running locally\")\n    DRIVE_PROJECT_DIR = None\n\n# Add project root to path\nPROJECT_ROOT = os.path.abspath(\".\")\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\nprint(f\"\\nWorking directory: {os.getcwd()}\")\nprint(f\"Project root: {PROJECT_ROOT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import llgbm modules\n",
    "from llgbm.probes import create_generic_probes, create_domain_probes, create_mixed_probes\n",
    "from llgbm.delta import (\n",
    "    get_average_activation,\n",
    "    compute_base_activation,\n",
    "    compute_adapter_delta,\n",
    "    DeltaCache,\n",
    ")\n",
    "\n",
    "print(\"[OK] llgbm imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"base_model\": \"Qwen/Qwen2.5-1.5B\",\n",
    "    \"dtype\": \"bfloat16\",  # or \"float16\"\n",
    "    \n",
    "    # Probe settings\n",
    "    \"probe_type\": \"generic\",  # \"generic\", \"math\", \"code\", \"commonsense\", \"mixed\"\n",
    "    \"max_length\": 256,\n",
    "    \n",
    "    # Paths\n",
    "    \"checkpoint_dir\": \"data/teacher_checkpoints\",  # Directory with LoRA adapters\n",
    "    \"cache_dir\": \"deltas\",  # Where to cache delta embeddings\n",
    "    \"output_dir\": \"outputs/phase1_delta\",\n",
    "    \n",
    "    # Options\n",
    "    \"force_recompute\": False,  # Set True to recompute even if cached\n",
    "}\n",
    "\n",
    "# Resolve dtype\n",
    "DTYPE_MAP = {\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float32\": torch.float32,\n",
    "}\n",
    "CONFIG[\"torch_dtype\"] = DTYPE_MAP[CONFIG[\"dtype\"]]\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    if k != \"torch_dtype\":\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "32adx7sb3uc",
   "source": "# Drive persistence helpers\ndef get_persistent_path(local_path: str) -> str:\n    \"\"\"Get persistent path (Drive in Colab, local otherwise).\"\"\"\n    if IN_COLAB and DRIVE_PROJECT_DIR:\n        return os.path.join(DRIVE_PROJECT_DIR, local_path)\n    return local_path\n\ndef sync_to_drive(local_path: str, description: str = \"\"):\n    \"\"\"Copy local path to Drive for persistence.\"\"\"\n    if IN_COLAB and DRIVE_PROJECT_DIR:\n        drive_path = get_persistent_path(local_path)\n        os.makedirs(os.path.dirname(drive_path) if os.path.dirname(drive_path) else \".\", exist_ok=True)\n        if os.path.isdir(local_path):\n            if os.path.exists(drive_path):\n                shutil.rmtree(drive_path)\n            shutil.copytree(local_path, drive_path)\n        else:\n            shutil.copy2(local_path, drive_path)\n        print(f\"[Drive] Synced {description or local_path} -> {drive_path}\")\n        return drive_path\n    return local_path\n\ndef sync_from_drive(local_path: str, description: str = \"\"):\n    \"\"\"Restore local path from Drive if it exists there.\"\"\"\n    if IN_COLAB and DRIVE_PROJECT_DIR:\n        drive_path = get_persistent_path(local_path)\n        if os.path.exists(drive_path):\n            if os.path.isdir(drive_path):\n                if os.path.exists(local_path):\n                    shutil.rmtree(local_path)\n                shutil.copytree(drive_path, local_path)\n            else:\n                os.makedirs(os.path.dirname(local_path) if os.path.dirname(local_path) else \".\", exist_ok=True)\n                shutil.copy2(drive_path, local_path)\n            print(f\"[Drive] Restored {description or local_path} <- {drive_path}\")\n            return True\n    return False\n\n# Restore any previously computed data from Drive\nsync_from_drive(CONFIG[\"cache_dir\"], \"delta cache\")\nsync_from_drive(CONFIG[\"checkpoint_dir\"], \"teacher checkpoints\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Create Sample Teacher Checkpoints (for testing)\n",
    "\n",
    "Skip this step if you already have teacher checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "def create_dummy_lora_adapter(output_dir: str, rank: int = 16, domain: str = \"math\"):\n",
    "    \"\"\"\n",
    "    Create a dummy LoRA adapter for testing.\n",
    "    \n",
    "    In practice, these would be real fine-tuned LoRA adapters.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Qwen2.5-1.5B config\n",
    "    hidden_size = 1536\n",
    "    intermediate_size = 8960\n",
    "    num_layers = 28\n",
    "    num_kv_heads = 2\n",
    "    num_heads = 12\n",
    "    head_dim = hidden_size // num_heads\n",
    "    kv_dim = num_kv_heads * head_dim\n",
    "    \n",
    "    lora_weights = {}\n",
    "    \n",
    "    # Add some domain-specific bias to make deltas different\n",
    "    domain_seed = hash(domain) % 1000\n",
    "    torch.manual_seed(domain_seed)\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        prefix = f\"base_model.model.model.layers.{layer_idx}\"\n",
    "        \n",
    "        # Attention projections\n",
    "        for proj in [\"q_proj\", \"o_proj\"]:\n",
    "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n",
    "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_B.weight\"] = torch.randn(hidden_size, rank) * 0.001\n",
    "        \n",
    "        for proj in [\"k_proj\", \"v_proj\"]:\n",
    "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n",
    "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_B.weight\"] = torch.randn(kv_dim, rank) * 0.001\n",
    "        \n",
    "        # MLP projections\n",
    "        for proj in [\"gate_proj\", \"up_proj\"]:\n",
    "            lora_weights[f\"{prefix}.mlp.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n",
    "            lora_weights[f\"{prefix}.mlp.{proj}.lora_B.weight\"] = torch.randn(intermediate_size, rank) * 0.001\n",
    "        \n",
    "        lora_weights[f\"{prefix}.mlp.down_proj.lora_A.weight\"] = torch.randn(rank, intermediate_size) * 0.01\n",
    "        lora_weights[f\"{prefix}.mlp.down_proj.lora_B.weight\"] = torch.randn(hidden_size, rank) * 0.001\n",
    "    \n",
    "    # Convert to bfloat16 and save\n",
    "    lora_weights = {k: v.to(torch.bfloat16) for k, v in lora_weights.items()}\n",
    "    save_file(lora_weights, os.path.join(output_dir, \"adapter_model.safetensors\"))\n",
    "    \n",
    "    # Create adapter_config.json\n",
    "    config = {\n",
    "        \"base_model_name_or_path\": \"Qwen/Qwen2.5-1.5B\",\n",
    "        \"r\": rank,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.0,\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"bias\": \"none\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        \"peft_type\": \"LORA\",\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"adapter_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Created adapter at {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample teacher checkpoints if they don't exist\n",
    "checkpoint_dir = Path(CONFIG[\"checkpoint_dir\"])\n",
    "\n",
    "if not checkpoint_dir.exists() or not list(checkpoint_dir.rglob(\"adapter_config.json\")):\n",
    "    print(\"Creating sample teacher checkpoints for testing...\")\n",
    "    \n",
    "    # Create a few dummy adapters with different \"domains\"\n",
    "    sample_adapters = [\n",
    "        (\"math_adapter_001\", \"math\"),\n",
    "        (\"math_adapter_002\", \"math\"),\n",
    "        (\"code_adapter_001\", \"code\"),\n",
    "        (\"code_adapter_002\", \"code\"),\n",
    "        (\"general_adapter_001\", \"general\"),\n",
    "    ]\n",
    "    \n",
    "    for name, domain in sample_adapters:\n",
    "        adapter_path = checkpoint_dir / domain / name\n",
    "        create_dummy_lora_adapter(str(adapter_path), domain=domain)\n",
    "    \n",
    "    print(f\"\\nCreated {len(sample_adapters)} sample adapters\")\n",
    "else:\n",
    "    print(f\"Using existing checkpoints in {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Probes and Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probes based on configuration\n",
    "if CONFIG[\"probe_type\"] == \"mixed\":\n",
    "    probes = create_mixed_probes()\n",
    "elif CONFIG[\"probe_type\"] == \"generic\":\n",
    "    probes = create_generic_probes()\n",
    "else:\n",
    "    probes = create_domain_probes(CONFIG[\"probe_type\"])\n",
    "\n",
    "print(f\"Using {len(probes)} {CONFIG['probe_type']} probes\")\n",
    "print(f\"\\nSample probe:\")\n",
    "print(\"-\" * 40)\n",
    "print(probes[0][:200] + \"...\" if len(probes[0]) > 200 else probes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cache\n",
    "cache = DeltaCache(CONFIG[\"cache_dir\"])\n",
    "\n",
    "# Check existing cache\n",
    "summary = cache.summary()\n",
    "print(f\"Cache directory: {CONFIG['cache_dir']}\")\n",
    "print(f\"Existing cached deltas: {summary.get('count', 0)}\")\n",
    "if summary.get('count', 0) > 0:\n",
    "    print(f\"  Norm range: [{summary['norm_min']:.4f}, {summary['norm_max']:.4f}]\")\n",
    "    print(f\"  Norm mean: {summary['norm_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 5: Compute Base Model Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check for cached base activation\n",
    "base_activation = cache.get_base_activation()\n",
    "\n",
    "if base_activation is None or CONFIG[\"force_recompute\"]:\n",
    "    print(\"\\nComputing base model activation...\")\n",
    "    base_activation, tokenizer = compute_base_activation(\n",
    "        CONFIG[\"base_model\"],\n",
    "        probes,\n",
    "        device,\n",
    "        CONFIG[\"max_length\"],\n",
    "        CONFIG[\"torch_dtype\"],\n",
    "    )\n",
    "    \n",
    "    # Save to cache\n",
    "    cache.save_base_activation(base_activation, {\n",
    "        \"base_model\": CONFIG[\"base_model\"],\n",
    "        \"probe_type\": CONFIG[\"probe_type\"],\n",
    "        \"num_probes\": len(probes),\n",
    "        \"max_length\": CONFIG[\"max_length\"],\n",
    "        \"dtype\": CONFIG[\"dtype\"],\n",
    "    })\n",
    "    print(f\"[OK] Base activation computed and cached\")\n",
    "else:\n",
    "    print(\"[OK] Loaded cached base activation\")\n",
    "    # Still need tokenizer for adapter computation\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model\"], trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\nBase activation shape: {base_activation.shape}\")\n",
    "print(f\"Base activation norm: {np.linalg.norm(base_activation):.4f}\")\n",
    "print(f\"Base activation range: [{base_activation.min():.4f}, {base_activation.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 6: Find Teacher Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adapter_paths(checkpoint_dir: str) -> list:\n",
    "    \"\"\"Find all LoRA adapter paths in a directory.\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    adapter_paths = []\n",
    "    \n",
    "    # Look for adapter_config.json\n",
    "    for path in checkpoint_dir.rglob(\"adapter_config.json\"):\n",
    "        adapter_paths.append(str(path.parent))\n",
    "    \n",
    "    return sorted(adapter_paths)\n",
    "\n",
    "# Find all adapters\n",
    "adapter_paths = find_adapter_paths(CONFIG[\"checkpoint_dir\"])\n",
    "print(f\"Found {len(adapter_paths)} teacher adapters:\")\n",
    "for path in adapter_paths:\n",
    "    # Check if already cached\n",
    "    cached = cache.get_delta(path) is not None\n",
    "    status = \"[cached]\" if cached else \"[pending]\"\n",
    "    print(f\"  {status} {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 7: Compute Delta Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute deltas for all adapters\n",
    "stats = {\"norms\": [], \"computed\": 0, \"cached\": 0, \"failed\": 0}\n",
    "\n",
    "print(\"Computing delta embeddings...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for adapter_path in tqdm(adapter_paths, desc=\"Processing adapters\"):\n",
    "    adapter_name = Path(adapter_path).name\n",
    "    \n",
    "    # Check cache first\n",
    "    if not CONFIG[\"force_recompute\"]:\n",
    "        cached_delta = cache.get_delta(adapter_path)\n",
    "        if cached_delta is not None:\n",
    "            stats[\"cached\"] += 1\n",
    "            stats[\"norms\"].append(np.linalg.norm(cached_delta))\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        delta = compute_adapter_delta(\n",
    "            CONFIG[\"base_model\"],\n",
    "            adapter_path,\n",
    "            probes,\n",
    "            base_activation,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            CONFIG[\"max_length\"],\n",
    "            CONFIG[\"torch_dtype\"],\n",
    "            show_progress=False,\n",
    "        )\n",
    "        cache.save_delta(adapter_path, delta)\n",
    "        stats[\"computed\"] += 1\n",
    "        stats[\"norms\"].append(np.linalg.norm(delta))\n",
    "        \n",
    "        tqdm.write(f\"  [OK] {adapter_name}: norm={np.linalg.norm(delta):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"  [FAIL] {adapter_name}: {e}\")\n",
    "        stats[\"failed\"] += 1\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Print summary\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Delta Computation Summary\")\nprint(\"=\" * 50)\nprint(f\"Total adapters: {len(adapter_paths)}\")\nprint(f\"Computed: {stats['computed']}\")\nprint(f\"Cached: {stats['cached']}\")\nprint(f\"Failed: {stats['failed']}\")\n\nif stats[\"norms\"]:\n    norms = np.array(stats[\"norms\"])\n    print(f\"\\nDelta norm statistics:\")\n    print(f\"  Min:    {norms.min():.4f}\")\n    print(f\"  Max:    {norms.max():.4f}\")\n    print(f\"  Mean:   {norms.mean():.4f}\")\n    print(f\"  Std:    {norms.std():.4f}\")\n    print(f\"  Median: {np.median(norms):.4f}\")\n\nprint(f\"\\nCache saved to: {CONFIG['cache_dir']}/\")\n\n# Sync to Drive for persistence\nsync_to_drive(CONFIG[\"cache_dir\"], \"delta cache\")\nsync_to_drive(CONFIG[\"checkpoint_dir\"], \"teacher checkpoints\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Delta Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all deltas\n",
    "deltas = cache.get_all_deltas()\n",
    "print(f\"Loaded {len(deltas)} delta embeddings for visualization\")\n",
    "\n",
    "if len(deltas) < 2:\n",
    "    print(\"Need at least 2 deltas for visualization\")\n",
    "else:\n",
    "    # Prepare data\n",
    "    names = list(deltas.keys())\n",
    "    embeddings = np.stack([deltas[n] for n in names])\n",
    "    \n",
    "    # Extract domain from path\n",
    "    domains = []\n",
    "    for name in names:\n",
    "        parts = Path(name).parts\n",
    "        domain = \"unknown\"\n",
    "        for part in parts:\n",
    "            if part in [\"math\", \"code\", \"commonsense\", \"general\", \"legal\", \"medical\"]:\n",
    "                domain = part\n",
    "                break\n",
    "        domains.append(domain)\n",
    "    \n",
    "    print(f\"Domains found: {set(domains)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity matrix\n",
    "if len(deltas) >= 2:\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create labels\n",
    "    short_names = [Path(n).name[:20] for n in names]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        sim_matrix,\n",
    "        xticklabels=short_names,\n",
    "        yticklabels=short_names,\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        annot=True if len(names) <= 10 else False,\n",
    "        fmt='.2f',\n",
    "    )\n",
    "    plt.title(\"Delta Embedding Cosine Similarity\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    output_path = Path(CONFIG[\"output_dir\"]) / \"similarity_matrix.png\"\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "if len(deltas) >= 3:\n",
    "    # Run t-SNE\n",
    "    perplexity = min(5, len(embeddings) - 1)\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    unique_domains = list(set(domains))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, max(len(unique_domains), 1)))\n",
    "    domain_to_color = {d: c for d, c in zip(unique_domains, colors)}\n",
    "    \n",
    "    for i, (x, y) in enumerate(embeddings_2d):\n",
    "        plt.scatter(x, y, c=[domain_to_color[domains[i]]], s=100, alpha=0.7)\n",
    "        plt.annotate(\n",
    "            Path(names[i]).name[:15],\n",
    "            (x, y),\n",
    "            fontsize=8,\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "    \n",
    "    # Legend\n",
    "    for domain, color in domain_to_color.items():\n",
    "        plt.scatter([], [], c=[color], label=domain, s=100)\n",
    "    plt.legend(title=\"Domain\")\n",
    "    \n",
    "    plt.title(\"Delta Embeddings t-SNE\")\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save\n",
    "    output_path = Path(CONFIG[\"output_dir\"]) / \"tsne.png\"\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved to {output_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Delta norm distribution\nif stats[\"norms\"]:\n    plt.figure(figsize=(10, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(stats[\"norms\"], bins=20, edgecolor='black', alpha=0.7)\n    plt.xlabel(\"Delta Norm\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Distribution of Delta Norms\")\n    plt.axvline(np.mean(stats[\"norms\"]), color='r', linestyle='--', label=f'Mean: {np.mean(stats[\"norms\"]):.2f}')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.bar(range(len(stats[\"norms\"])), sorted(stats[\"norms\"]))\n    plt.xlabel(\"Adapter (sorted)\")\n    plt.ylabel(\"Delta Norm\")\n    plt.title(\"Sorted Delta Norms\")\n    \n    plt.tight_layout()\n    \n    output_path = Path(CONFIG[\"output_dir\"]) / \"norm_distribution.png\"\n    plt.savefig(output_path, dpi=150)\n    print(f\"Saved to {output_path}\")\n    plt.show()\n\n# Sync all outputs to Drive\nsync_to_drive(CONFIG[\"output_dir\"], \"phase 1 outputs\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 9: Acceptance Criteria Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Phase 1 Acceptance Criteria\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "criteria = {\n",
    "    \"Can compute base activation\": base_activation is not None and base_activation.shape[0] > 0,\n",
    "    \"Can compute deltas without memory leaks\": stats[\"computed\"] + stats[\"cached\"] > 0,\n",
    "    \"Cache manifest exists\": (Path(CONFIG[\"cache_dir\"]) / \"manifest.json\").exists(),\n",
    "    \"Delta norms are reasonable (not zero)\": all(n > 1e-6 for n in stats[\"norms\"]) if stats[\"norms\"] else False,\n",
    "    \"Delta norms not exploding\": all(n < 1e6 for n in stats[\"norms\"]) if stats[\"norms\"] else False,\n",
    "    \"No failed computations\": stats[\"failed\"] == 0,\n",
    "}\n",
    "\n",
    "print()\n",
    "all_passed = True\n",
    "for criterion, passed in criteria.items():\n",
    "    status = \"[PASS]\" if passed else \"[FAIL]\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print()\n",
    "if all_passed:\n",
    "    print(\"All acceptance criteria PASSED!\")\n",
    "    print(\"Ready to proceed to Phase 2.\")\n",
    "else:\n",
    "    print(\"Some criteria FAILED. Please review and fix issues before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Usage Example: Loading Deltas for Training\n",
    "\n",
    "In Phase 2, you'll use the cached deltas like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading delta for a specific checkpoint\n",
    "if adapter_paths:\n",
    "    example_path = adapter_paths[0]\n",
    "    delta = cache.get_delta(example_path)\n",
    "    \n",
    "    print(f\"Example usage:\")\n",
    "    print(f\"  Adapter: {example_path}\")\n",
    "    print(f\"  Delta shape: {delta.shape}\")\n",
    "    print(f\"  Delta norm: {np.linalg.norm(delta):.4f}\")\n",
    "    print(f\"\\n  # In training:\")\n",
    "    print(f\"  # delta_target = cache.get_delta(checkpoint_path)\")\n",
    "    print(f\"  # loss = mse_loss(predicted_delta, delta_target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once Phase 1 is complete, proceed to **Phase 2** to add delta labels to the training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}