{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Dataset Plumbing - Return Delta Labels Alongside Weight Tokens\n",
        "\n",
        "This notebook creates and tests delta-augmented datasets that return `(tokens, condition, delta)` tuples for training.\n",
        "\n",
        "## Goals\n",
        "- Wrap existing DnD datasets with delta supervision\n",
        "- Verify data loading and collation\n",
        "- Test DataLoader iteration\n",
        "- Persist outputs to Google Drive (for Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup & Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "    \n",
        "    # Mount Google Drive for persistence\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Create project directory on Drive\n",
        "    DRIVE_PROJECT_DIR = '/content/drive/MyDrive/llgbm'\n",
        "    os.makedirs(DRIVE_PROJECT_DIR, exist_ok=True)\n",
        "    print(f\"Drive project dir: {DRIVE_PROJECT_DIR}\")\n",
        "    \n",
        "    # Install dependencies\n",
        "    !pip install -q safetensors accelerate transformers peft\n",
        "    !pip install -q scikit-learn matplotlib seaborn\n",
        "    \n",
        "    # Clone or copy repo if not present\n",
        "    if not os.path.exists(\"llgbm\"):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ERROR: llgbm package not found!\")\n",
        "        print(\"Please upload the llgbm folder or clone your repo.\")\n",
        "        print(\"=\"*60)\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    DRIVE_PROJECT_DIR = None\n",
        "\n",
        "# Add project root to path\n",
        "PROJECT_ROOT = os.path.abspath(\".\")\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import llgbm modules\n",
        "from llgbm.delta import DeltaCache\n",
        "from llgbm.dataset import (\n",
        "    DeltaAugmentedDataset,\n",
        "    Text2Qwen25LoRA_DeltaDataset,\n",
        "    create_dataloader,\n",
        ")\n",
        "\n",
        "print(\"[OK] llgbm imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configuration with Drive Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper for persistent paths\n",
        "def get_persistent_path(local_path: str) -> str:\n",
        "    \"\"\"Get persistent path (Drive in Colab, local otherwise).\"\"\"\n",
        "    if IN_COLAB and DRIVE_PROJECT_DIR:\n",
        "        return os.path.join(DRIVE_PROJECT_DIR, local_path)\n",
        "    return local_path\n",
        "\n",
        "def sync_to_drive(local_path: str, description: str = \"\"):\n",
        "    \"\"\"Copy local path to Drive for persistence.\"\"\"\n",
        "    if IN_COLAB and DRIVE_PROJECT_DIR:\n",
        "        drive_path = get_persistent_path(local_path)\n",
        "        os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "        if os.path.isdir(local_path):\n",
        "            if os.path.exists(drive_path):\n",
        "                shutil.rmtree(drive_path)\n",
        "            shutil.copytree(local_path, drive_path)\n",
        "        else:\n",
        "            shutil.copy2(local_path, drive_path)\n",
        "        print(f\"[Drive] Synced {description or local_path} -> {drive_path}\")\n",
        "        return drive_path\n",
        "    return local_path\n",
        "\n",
        "def sync_from_drive(local_path: str, description: str = \"\"):\n",
        "    \"\"\"Restore local path from Drive if it exists there.\"\"\"\n",
        "    if IN_COLAB and DRIVE_PROJECT_DIR:\n",
        "        drive_path = get_persistent_path(local_path)\n",
        "        if os.path.exists(drive_path):\n",
        "            if os.path.isdir(drive_path):\n",
        "                if os.path.exists(local_path):\n",
        "                    shutil.rmtree(local_path)\n",
        "                shutil.copytree(drive_path, local_path)\n",
        "            else:\n",
        "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "                shutil.copy2(drive_path, local_path)\n",
        "            print(f\"[Drive] Restored {description or local_path} <- {drive_path}\")\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Model settings\n",
        "    \"base_model\": \"Qwen/Qwen2.5-1.5B\",\n",
        "    \n",
        "    # Dataset settings\n",
        "    \"checkpoint_dir\": \"data/teacher_checkpoints\",\n",
        "    \"cache_dir\": \"deltas\",\n",
        "    \"output_dir\": \"outputs/phase2_dataset\",\n",
        "    \n",
        "    # DataLoader settings\n",
        "    \"batch_size\": 4,\n",
        "    \"num_workers\": 0,  # Set to 0 for Colab compatibility\n",
        "    \"max_text_length\": 512,\n",
        "    \"condition_type\": \"prompt\",  # or \"prompt_answer\"\n",
        "    \n",
        "    # Qwen2.5-1.5B specifics\n",
        "    \"hidden_size\": 1536,\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Restore Data from Drive (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore cached data from Drive\n",
        "sync_from_drive(CONFIG[\"cache_dir\"], \"delta cache\")\n",
        "sync_from_drive(CONFIG[\"checkpoint_dir\"], \"teacher checkpoints\")\n",
        "\n",
        "# Check what we have\n",
        "cache_path = Path(CONFIG[\"cache_dir\"])\n",
        "checkpoint_path = Path(CONFIG[\"checkpoint_dir\"])\n",
        "\n",
        "print(f\"\\nDelta cache exists: {cache_path.exists()}\")\n",
        "if cache_path.exists():\n",
        "    cache = DeltaCache(CONFIG[\"cache_dir\"])\n",
        "    summary = cache.summary()\n",
        "    print(f\"  Cached deltas: {summary.get('count', 0)}\")\n",
        "\n",
        "print(f\"\\nCheckpoint dir exists: {checkpoint_path.exists()}\")\n",
        "if checkpoint_path.exists():\n",
        "    adapters = list(checkpoint_path.rglob(\"adapter_config.json\"))\n",
        "    print(f\"  Adapters found: {len(adapters)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Sample Data (if needed)\n",
        "\n",
        "Skip this if you have real data from Phase 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from safetensors.torch import save_file\n",
        "\n",
        "def create_dummy_lora_adapter(output_dir: str, rank: int = 16, domain: str = \"math\"):\n",
        "    \"\"\"Create a dummy LoRA adapter for testing.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    hidden_size = 1536\n",
        "    intermediate_size = 8960\n",
        "    num_layers = 28\n",
        "    num_kv_heads = 2\n",
        "    num_heads = 12\n",
        "    head_dim = hidden_size // num_heads\n",
        "    kv_dim = num_kv_heads * head_dim\n",
        "    \n",
        "    lora_weights = {}\n",
        "    domain_seed = hash(domain) % 1000\n",
        "    torch.manual_seed(domain_seed)\n",
        "    \n",
        "    for layer_idx in range(num_layers):\n",
        "        prefix = f\"base_model.model.model.layers.{layer_idx}\"\n",
        "        \n",
        "        for proj in [\"q_proj\", \"o_proj\"]:\n",
        "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n",
        "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_B.weight\"] = torch.randn(hidden_size, rank) * 0.001\n",
        "        \n",
        "        for proj in [\"k_proj\", \"v_proj\"]:\n",
        "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n",
        "            lora_weights[f\"{prefix}.self_attn.{proj}.lora_B.weight\"] = torch.randn(kv_dim, rank) * 0.001\n",
        "        \n",
        "        for proj in [\"gate_proj\", \"up_proj\"]:\n",
        "            lora_weights[f\"{prefix}.mlp.{proj}.lora_A.weight\"] = torch.randn(rank, hidden_size) * 0.01\n",
        "            lora_weights[f\"{prefix}.mlp.{proj}.lora_B.weight\"] = torch.randn(intermediate_size, rank) * 0.001\n",
        "        \n",
        "        lora_weights[f\"{prefix}.mlp.down_proj.lora_A.weight\"] = torch.randn(rank, intermediate_size) * 0.01\n",
        "        lora_weights[f\"{prefix}.mlp.down_proj.lora_B.weight\"] = torch.randn(hidden_size, rank) * 0.001\n",
        "    \n",
        "    lora_weights = {k: v.to(torch.bfloat16) for k, v in lora_weights.items()}\n",
        "    save_file(lora_weights, os.path.join(output_dir, \"adapter_model.safetensors\"))\n",
        "    \n",
        "    config = {\n",
        "        \"base_model_name_or_path\": \"Qwen/Qwen2.5-1.5B\",\n",
        "        \"r\": rank,\n",
        "        \"lora_alpha\": 32,\n",
        "        \"lora_dropout\": 0.0,\n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        \"bias\": \"none\",\n",
        "        \"task_type\": \"CAUSAL_LM\",\n",
        "        \"peft_type\": \"LORA\",\n",
        "    }\n",
        "    with open(os.path.join(output_dir, \"adapter_config.json\"), \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "    \n",
        "    # Create sample prompts.json\n",
        "    prompts_data = {\n",
        "        \"prompts\": [f\"Solve this {domain} problem: What is 2+2?\"],\n",
        "        \"answers\": [\"The answer is 4.\"],\n",
        "    }\n",
        "    with open(os.path.join(output_dir, \"prompts.json\"), \"w\") as f:\n",
        "        json.dump(prompts_data, f, indent=2)\n",
        "    \n",
        "    return output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data if needed\n",
        "checkpoint_dir = Path(CONFIG[\"checkpoint_dir\"])\n",
        "cache = DeltaCache(CONFIG[\"cache_dir\"])\n",
        "\n",
        "need_sample_data = (\n",
        "    not checkpoint_dir.exists() or \n",
        "    not list(checkpoint_dir.rglob(\"adapter_config.json\")) or\n",
        "    cache.summary().get('count', 0) == 0\n",
        ")\n",
        "\n",
        "if need_sample_data:\n",
        "    print(\"Creating sample teacher checkpoints and fake deltas for testing...\")\n",
        "    \n",
        "    sample_adapters = [\n",
        "        (\"math_adapter_001\", \"math\"),\n",
        "        (\"math_adapter_002\", \"math\"),\n",
        "        (\"code_adapter_001\", \"code\"),\n",
        "        (\"code_adapter_002\", \"code\"),\n",
        "        (\"general_adapter_001\", \"general\"),\n",
        "    ]\n",
        "    \n",
        "    for name, domain in sample_adapters:\n",
        "        adapter_path = checkpoint_dir / domain / name\n",
        "        create_dummy_lora_adapter(str(adapter_path), domain=domain)\n",
        "        \n",
        "        # Create fake delta embedding\n",
        "        torch.manual_seed(hash(name) % 10000)\n",
        "        fake_delta = torch.randn(CONFIG[\"hidden_size\"]).numpy() * 0.1\n",
        "        cache.save_delta(str(adapter_path), fake_delta)\n",
        "    \n",
        "    # Create fake base activation\n",
        "    fake_base = np.zeros(CONFIG[\"hidden_size\"])\n",
        "    cache.save_base_activation(fake_base, {\"note\": \"fake for testing\"})\n",
        "    \n",
        "    print(f\"Created {len(sample_adapters)} sample adapters with fake deltas\")\n",
        "    \n",
        "    # Sync to Drive\n",
        "    sync_to_drive(CONFIG[\"checkpoint_dir\"], \"teacher checkpoints\")\n",
        "    sync_to_drive(CONFIG[\"cache_dir\"], \"delta cache\")\n",
        "else:\n",
        "    print(\"Using existing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Initialize Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize text tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(f\"[OK] Text tokenizer loaded: {text_tokenizer.__class__.__name__}\")\n",
        "print(f\"     Vocab size: {text_tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to load DnD LoRA tokenizer (optional for full integration)\n",
        "DND_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    sys.path.insert(0, \"dnd_repo\")\n",
        "    from workspace.dnd.tokenizer.register import Qwen2515LoRA_Tokenizer2D\n",
        "    lora_tokenizer = Qwen2515LoRA_Tokenizer2D()\n",
        "    DND_AVAILABLE = True\n",
        "    print(\"[OK] DnD LoRA tokenizer loaded\")\n",
        "    print(f\"     Token size: {lora_tokenizer.token_size}\")\n",
        "except ImportError as e:\n",
        "    print(f\"[INFO] DnD tokenizer not available: {e}\")\n",
        "    print(\"       Using simplified dataset without LoRA tokenization\")\n",
        "    lora_tokenizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Create Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload cache\n",
        "cache = DeltaCache(CONFIG[\"cache_dir\"])\n",
        "print(f\"Delta cache summary:\")\n",
        "for k, v in cache.summary().items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if DND_AVAILABLE and lora_tokenizer is not None:\n",
        "    # Full dataset with LoRA tokenization\n",
        "    dataset = Text2Qwen25LoRA_DeltaDataset(\n",
        "        checkpoint_folder=CONFIG[\"checkpoint_dir\"],\n",
        "        lora_tokenizer=lora_tokenizer,\n",
        "        text_tokenizer=text_tokenizer,\n",
        "        delta_cache=cache,\n",
        "        max_text_length=CONFIG[\"max_text_length\"],\n",
        "        condition_type=CONFIG[\"condition_type\"],\n",
        "    )\n",
        "else:\n",
        "    # Simplified dataset without LoRA tokenization\n",
        "    from torch.utils.data import Dataset as TorchDataset\n",
        "    \n",
        "    class SimpleTestDataset(TorchDataset):\n",
        "        \"\"\"Simplified dataset for testing without DnD tokenizer.\"\"\"\n",
        "        def __init__(self, checkpoint_dir, text_tokenizer, delta_cache, max_length=512):\n",
        "            self.checkpoint_dir = Path(checkpoint_dir)\n",
        "            self.text_tokenizer = text_tokenizer\n",
        "            self.delta_cache = delta_cache\n",
        "            self.max_length = max_length\n",
        "            \n",
        "            self.all_deltas = delta_cache.get_all_deltas()\n",
        "            self.checkpoints = [\n",
        "                str(p.parent) for p in self.checkpoint_dir.rglob(\"adapter_config.json\")\n",
        "                if str(p.parent) in self.all_deltas\n",
        "            ]\n",
        "            print(f\"SimpleTestDataset: {len(self.checkpoints)} checkpoints with deltas\")\n",
        "        \n",
        "        def __len__(self):\n",
        "            return len(self.checkpoints)\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            ckpt_path = self.checkpoints[idx]\n",
        "            \n",
        "            # Load prompts\n",
        "            prompts_file = Path(ckpt_path) / \"prompts.json\"\n",
        "            if prompts_file.exists():\n",
        "                with open(prompts_file) as f:\n",
        "                    data = json.load(f)\n",
        "                text = data.get(\"prompts\", [Path(ckpt_path).name])[0]\n",
        "            else:\n",
        "                text = Path(ckpt_path).name\n",
        "            \n",
        "            # Tokenize text\n",
        "            encoded = self.text_tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            \n",
        "            # Create fake tokens (placeholder)\n",
        "            tokens = torch.randn(196, 18, 258)  # DnD token shape for Qwen2.5-1.5B\n",
        "            scales = torch.ones(196)\n",
        "            \n",
        "            # Get delta\n",
        "            delta = torch.from_numpy(self.all_deltas[ckpt_path]).float()\n",
        "            \n",
        "            return {\n",
        "                \"tokens\": tokens,\n",
        "                \"scales\": scales,\n",
        "                \"condition_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "                \"delta\": delta,\n",
        "                \"checkpoint_path\": ckpt_path,\n",
        "            }\n",
        "        \n",
        "        @staticmethod\n",
        "        def collate_fn(batch):\n",
        "            return {\n",
        "                key: torch.stack([b[key] for b in batch]) if isinstance(batch[0][key], torch.Tensor) else [b[key] for b in batch]\n",
        "                for key in batch[0].keys()\n",
        "            }\n",
        "    \n",
        "    dataset = SimpleTestDataset(\n",
        "        checkpoint_dir=CONFIG[\"checkpoint_dir\"],\n",
        "        text_tokenizer=text_tokenizer,\n",
        "        delta_cache=cache,\n",
        "        max_length=CONFIG[\"max_text_length\"],\n",
        "    )\n",
        "\n",
        "print(f\"\\nDataset size: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    collate_fn=dataset.collate_fn,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "print(f\"DataLoader created with batch_size={CONFIG['batch_size']}\")\n",
        "print(f\"Number of batches: {len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test Single Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test single sample\n",
        "sample = dataset[0]\n",
        "\n",
        "print(\"Single sample inspection:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in sample.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"  {key}:\")\n",
        "        print(f\"    shape: {value.shape}\")\n",
        "        print(f\"    dtype: {value.dtype}\")\n",
        "        print(f\"    range: [{value.min().item():.4f}, {value.max().item():.4f}]\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Test Batch Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch iteration\n",
        "batch = next(iter(dataloader))\n",
        "\n",
        "print(\"Batch inspection:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in batch.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"  {key}:\")\n",
        "        print(f\"    shape: {value.shape}\")\n",
        "        print(f\"    dtype: {value.dtype}\")\n",
        "    else:\n",
        "        print(f\"  {key}: list of {len(value)} items\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test delta statistics\n",
        "print(\"\\nDelta statistics for batch:\")\n",
        "print(\"=\" * 50)\n",
        "delta = batch[\"delta\"]\n",
        "print(f\"  Shape: {delta.shape}\")\n",
        "print(f\"  Mean:  {delta.mean().item():.6f}\")\n",
        "print(f\"  Std:   {delta.std().item():.6f}\")\n",
        "print(f\"  Min:   {delta.min().item():.6f}\")\n",
        "print(f\"  Max:   {delta.max().item():.6f}\")\n",
        "print(f\"  Norms: {delta.norm(dim=1).tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test full iteration\n",
        "print(\"\\nFull DataLoader iteration test:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "total_samples = 0\n",
        "delta_norms = []\n",
        "\n",
        "for batch_idx, batch in enumerate(dataloader):\n",
        "    total_samples += batch[\"delta\"].shape[0]\n",
        "    delta_norms.extend(batch[\"delta\"].norm(dim=1).tolist())\n",
        "    print(f\"  Batch {batch_idx}: {batch['delta'].shape[0]} samples\")\n",
        "\n",
        "print(f\"\\nTotal samples iterated: {total_samples}\")\n",
        "print(f\"Expected: {len(dataset)}\")\n",
        "assert total_samples == len(dataset), \"Sample count mismatch!\"\n",
        "print(\"[PASS] Full iteration complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Verify Dtype Consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dtypes match expected\n",
        "batch = next(iter(dataloader))\n",
        "\n",
        "expected_dtypes = {\n",
        "    \"tokens\": torch.float32,\n",
        "    \"scales\": torch.float32,\n",
        "    \"condition_ids\": torch.long,\n",
        "    \"attention_mask\": torch.long,\n",
        "    \"delta\": torch.float32,\n",
        "}\n",
        "\n",
        "print(\"Dtype consistency check:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "all_correct = True\n",
        "for key, expected in expected_dtypes.items():\n",
        "    if key not in batch:\n",
        "        print(f\"  {key}: MISSING\")\n",
        "        continue\n",
        "    if not isinstance(batch[key], torch.Tensor):\n",
        "        continue\n",
        "    actual = batch[key].dtype\n",
        "    status = \"OK\" if actual == expected else \"MISMATCH\"\n",
        "    if actual != expected:\n",
        "        all_correct = False\n",
        "    print(f\"  {key}: {actual} (expected {expected}) [{status}]\")\n",
        "\n",
        "if all_correct:\n",
        "    print(\"\\n[PASS] Dtype consistency check\")\n",
        "else:\n",
        "    print(\"\\n[WARN] Some dtype mismatches detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Visualize Delta Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot delta norm distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(delta_norms, bins=20, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('Delta Norm')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Delta Norms in Dataset')\n",
        "plt.axvline(np.mean(delta_norms), color='r', linestyle='--', label=f'Mean: {np.mean(delta_norms):.4f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Show batch delta correlation\n",
        "if len(delta_norms) > 1:\n",
        "    all_deltas = []\n",
        "    for batch in dataloader:\n",
        "        all_deltas.append(batch[\"delta\"])\n",
        "    all_deltas = torch.cat(all_deltas, dim=0).numpy()\n",
        "    \n",
        "    # Compute pairwise cosine similarity\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    sim_matrix = cosine_similarity(all_deltas)\n",
        "    \n",
        "    plt.imshow(sim_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "    plt.colorbar(label='Cosine Similarity')\n",
        "    plt.title('Delta Pairwise Similarity')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Sample')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "output_path = Path(CONFIG[\"output_dir\"]) / \"delta_distribution.png\"\n",
        "plt.savefig(output_path, dpi=150)\n",
        "print(f\"Saved to {output_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Acceptance Criteria Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Phase 2 Acceptance Criteria\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Gather test data\n",
        "sample = dataset[0]\n",
        "batch = next(iter(dataloader))\n",
        "\n",
        "criteria = {\n",
        "    \"Dataset returns (tokens, condition, delta) structure\": (\n",
        "        \"tokens\" in sample and \"delta\" in sample\n",
        "    ),\n",
        "    f\"Delta shape is (hidden_size,) = ({CONFIG['hidden_size']},)\": (\n",
        "        sample[\"delta\"].shape == (CONFIG[\"hidden_size\"],)\n",
        "    ),\n",
        "    \"Delta dtype is float32\": (\n",
        "        sample[\"delta\"].dtype == torch.float32\n",
        "    ),\n",
        "    f\"Batch delta shape is (B, hidden_size) = ({CONFIG['batch_size']}, {CONFIG['hidden_size']})\": (\n",
        "        batch[\"delta\"].shape == (min(CONFIG['batch_size'], len(dataset)), CONFIG[\"hidden_size\"])\n",
        "    ),\n",
        "    \"DataLoader iteration works without errors\": (\n",
        "        total_samples == len(dataset)\n",
        "    ),\n",
        "    \"Samples with cached deltas are included\": (\n",
        "        len(dataset) > 0\n",
        "    ),\n",
        "    \"Delta norms are reasonable (not zero or inf)\": (\n",
        "        all(0 < n < 1e6 for n in delta_norms)\n",
        "    ),\n",
        "}\n",
        "\n",
        "print()\n",
        "all_passed = True\n",
        "for criterion, passed in criteria.items():\n",
        "    status = \"[PASS]\" if passed else \"[FAIL]\"\n",
        "    print(f\"{status} {criterion}\")\n",
        "    if not passed:\n",
        "        all_passed = False\n",
        "\n",
        "print()\n",
        "if all_passed:\n",
        "    print(\"All acceptance criteria PASSED!\")\n",
        "    print(\"Ready to proceed to Phase 3.\")\n",
        "else:\n",
        "    print(\"Some criteria FAILED. Please review and fix issues.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Save Results and Sync to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save test results\n",
        "results = {\n",
        "    \"dataset_size\": len(dataset),\n",
        "    \"batch_size\": CONFIG[\"batch_size\"],\n",
        "    \"hidden_size\": CONFIG[\"hidden_size\"],\n",
        "    \"delta_stats\": {\n",
        "        \"mean_norm\": float(np.mean(delta_norms)),\n",
        "        \"std_norm\": float(np.std(delta_norms)),\n",
        "        \"min_norm\": float(np.min(delta_norms)),\n",
        "        \"max_norm\": float(np.max(delta_norms)),\n",
        "    },\n",
        "    \"sample_shapes\": {\n",
        "        key: list(value.shape) if isinstance(value, torch.Tensor) else str(type(value))\n",
        "        for key, value in sample.items()\n",
        "    },\n",
        "    \"all_criteria_passed\": all_passed,\n",
        "}\n",
        "\n",
        "results_path = Path(CONFIG[\"output_dir\"]) / \"phase2_results.json\"\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"Results saved to {results_path}\")\n",
        "\n",
        "# Sync outputs to Drive\n",
        "sync_to_drive(CONFIG[\"output_dir\"], \"phase 2 outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Example for Training\n",
        "\n",
        "In Phase 3/4, you'll use the dataset like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example training loop structure\n",
        "print(\"Example usage in training loop:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\"\"\n",
        "from llgbm.dataset import create_dataloader\n",
        "from llgbm.delta import DeltaCache\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = create_dataloader(\n",
        "    checkpoint_folder=\"data/teacher_checkpoints\",\n",
        "    lora_tokenizer=lora_tokenizer,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    delta_cache=DeltaCache(\"deltas\"),\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for batch in dataloader:\n",
        "    tokens_teacher = batch[\"tokens\"]       # (B, num_tokens, H, W)\n",
        "    condition_ids = batch[\"condition_ids\"]  # (B, seq_len)\n",
        "    delta_teacher = batch[\"delta\"]          # (B, hidden_size)\n",
        "    \n",
        "    # Forward pass\n",
        "    tokens_pred = generator(condition_ids)\n",
        "    \n",
        "    # Weight loss (existing DnD loss)\n",
        "    loss_weight = mse_loss(tokens_pred, tokens_teacher)\n",
        "    \n",
        "    # Delta loss (new behavioral loss)\n",
        "    delta_pred = compute_delta_embedding(base_model, detokenize(tokens_pred), probes)\n",
        "    loss_delta = mse_loss(delta_pred, delta_teacher)\n",
        "    \n",
        "    # Combined loss\n",
        "    loss = loss_weight + lambda_delta * loss_delta\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Once Phase 2 is complete, proceed to **Phase 3** to implement differentiable delta computation for generated LoRAs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
