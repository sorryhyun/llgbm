{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Raw Weight Supervision\n",
    "\n",
    "The simplest baseline: MSE between raw (A, B) matrices.\n",
    "\n",
    "**Note:** This suffers from gauge ambiguity â€” many (A, B) pairs produce the same\n",
    "effective weight update DW = B @ A * scaling. Compare with `toy_delta_w.ipynb`\n",
    "which supervises on DW directly.\n",
    "\n",
    "```\n",
    "Loss = MSE(A_pred, A_teacher) + MSE(B_pred, B_teacher)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DRIVE_OUTPUT_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/llgbm/outputs'\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    !pip install -q safetensors accelerate transformers peft sentence-transformers\n",
    "    sys.path.insert(0, '/content/drive/MyDrive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/llgbm/checkpoints'\n",
    "    DELTAS_DIR = CHECKPOINT_DIR + '/deltas'\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    DELTAS_DIR = './llgbm/deltas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "from llgbm import (\n",
    "    TrainingConfig,\n",
    "    WeightLoss,\n",
    "    MultiTaskLoss,\n",
    "    create_generator,\n",
    "    RealAdapterDataset,\n",
    "    FunctionalLoRA,\n",
    "    train,\n",
    "    evaluate,\n",
    ")\n",
    "from llgbm.ablations import setup_base_components, AblationConfig\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    use_small_model=True,\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_steps=200,\n",
    "    warmup_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    lambda_weight=1.0,\n",
    "    lambda_delta=0.0,  # No hidden-state delta needed\n",
    "    num_probes=10,\n",
    "    max_probe_length=256,\n",
    "    delta_batch_probes=True,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    delta_cache_dir=DELTAS_DIR,\n",
    "    output_dir=\"outputs/toy_baseline\",\n",
    "    text_encoder_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    freeze_text_encoder=True,\n",
    "    num_prompts_per_adapter=8,\n",
    ")\n",
    "\n",
    "TORCH_DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[config.dtype]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "config.save(f\"{config.output_dir}/config.json\")\n",
    "\n",
    "print(f\"Model: {config.base_model}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Loss: WeightLoss (raw A, B MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup shared components (base model, probes, dataset, etc.)\n",
    "ablation_config = AblationConfig(\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    deltas_dir=DELTAS_DIR,\n",
    "    output_dir=config.output_dir,\n",
    "    use_small_model=config.use_small_model,\n",
    "    batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    num_probes=config.num_probes,\n",
    "    max_probe_length=config.max_probe_length,\n",
    "    delta_batch_probes=config.delta_batch_probes,\n",
    "    text_encoder_name=config.text_encoder_name,\n",
    "    freeze_text_encoder=config.freeze_text_encoder,\n",
    "    num_prompts_per_adapter=config.num_prompts_per_adapter,\n",
    ")\n",
    "components = setup_base_components(ablation_config, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator + text encoder\n",
    "generator = create_generator(\n",
    "    config,\n",
    "    seed=42,\n",
    "    device=device,\n",
    "    text_encoder=components[\"text_encoder\"],\n",
    ")\n",
    "print(f\"[OK] Generator: {sum(p.numel() for p in generator.parameters() if p.requires_grad):,} trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WeightLoss + optimizer + scheduler\n",
    "criterion = MultiTaskLoss(lambda_weight=1.0, lambda_delta=0.0)\n",
    "weight_criterion = WeightLoss()\n",
    "\n",
    "optimizer = AdamW(generator.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "warmup_steps = min(config.warmup_steps, config.num_steps // 10)\n",
    "cosine_steps = max(1, config.num_steps - warmup_steps)\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    [LinearLR(optimizer, 0.1, 1.0, warmup_steps),\n",
    "     CosineAnnealingLR(optimizer, cosine_steps, config.learning_rate * 0.01)],\n",
    "    [warmup_steps]\n",
    ")\n",
    "\n",
    "print(f\"[OK] WeightLoss (raw A, B MSE)\")\n",
    "print(f\"[OK] Optimizer & Scheduler ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    components[\"dataset\"],\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=components[\"dataset\"].collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Train\n",
    "state = train(\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    functional_lora=components[\"functional_lora\"],\n",
    "    base_activation=components[\"base_activation\"],\n",
    "    probe_tokens=components[\"probe_tokens\"],\n",
    "    probe_masks=components[\"probe_masks\"],\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    compute_dtype=TORCH_DTYPE,\n",
    "    weight_criterion=weight_criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (delta cosine similarity)\n",
    "eval_dataloader = DataLoader(\n",
    "    components[\"dataset\"],\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=components[\"dataset\"].collate_fn,\n",
    ")\n",
    "\n",
    "# Use MultiTaskLoss with lambda_delta=1 for eval (needs delta computation)\n",
    "eval_criterion = MultiTaskLoss(lambda_weight=0.0, lambda_delta=1.0)\n",
    "eval_results = evaluate(\n",
    "    generator=generator,\n",
    "    dataloader=eval_dataloader,\n",
    "    functional_lora=components[\"functional_lora\"],\n",
    "    base_activation=components[\"base_activation\"],\n",
    "    probe_tokens=components[\"probe_tokens\"],\n",
    "    probe_masks=components[\"probe_masks\"],\n",
    "    criterion=eval_criterion,\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if state.loss_history:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    axes[0].plot(state.loss_history, color='#2c3e50', linewidth=1.5)\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Weight Loss (raw A, B)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(state.grad_norm_history, color='#9b59b6', linewidth=1.5)\n",
    "    axes[1].axhline(config.max_grad_norm, color='r', ls='--', label=f'clip={config.max_grad_norm}')\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_ylabel('Gradient Norm')\n",
    "    axes[1].set_title('Gradient Norms')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].plot(state.lr_history, color='#f39c12', linewidth=1.5)\n",
    "    axes[2].set_xlabel('Step')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('LR Schedule')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/training_curves.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "results = {\n",
    "    \"config\": asdict(config),\n",
    "    \"training\": {\n",
    "        \"steps\": len(state.loss_history),\n",
    "        \"best_loss\": state.best_loss,\n",
    "        \"final_loss\": state.loss_history[-1] if state.loss_history else None,\n",
    "    },\n",
    "    \"eval\": eval_results,\n",
    "    \"mode\": \"weight_only\",\n",
    "}\n",
    "\n",
    "with open(f\"{config.output_dir}/results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved to {config.output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync to Google Drive (Colab only)\n",
    "if IN_COLAB and DRIVE_OUTPUT_DIR:\n",
    "    drive_dir = f\"{DRIVE_OUTPUT_DIR}/toy_baseline\"\n",
    "    if os.path.exists(drive_dir):\n",
    "        shutil.rmtree(drive_dir)\n",
    "    shutil.copytree(config.output_dir, drive_dir)\n",
    "    print(f\"[Drive] Synced to {drive_dir}\")\n",
    "else:\n",
    "    print(f\"[Local] Outputs saved to {config.output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
