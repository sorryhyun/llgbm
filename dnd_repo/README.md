

<h1 align="center"> <img src="./assets/DnD.png" alt="Logo" width="36" /> Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights</h1>

<div align="center">
<a href='https://jerryliang24.github.io/DnD/' style="text-decoration: none;"><img src='https://img.shields.io/badge/DnD-Projectpage-orange?style=flat&logo=googlehome&logoColor=%23FFFFFF'></a>
<a href='https://arxiv.org/pdf/2506.16406'><img src='https://img.shields.io/badge/arXiv-2506.16406-%23B31B1B?logo=arxiv'></a>
<a href='https://huggingface.co/datasets/Jerrylz/DnD-checkpoints-and-logs'><img src='https://img.shields.io/badge/Hugging%20Face-Models-blue?style=flat&logo=huggingface&logoColor=%23FFD21E'></a>
<a href='https://huggingface.co/datasets/Jerrylz'><img src='https://img.shields.io/badge/Hugging%20Face-Datasets-blue?style=flat&logo=huggingface&logoColor=%23FFD21E'></a>
<a href='LICENSE'><img src='https://img.shields.io/badge/License-Apache_2.0-green.svg'></a>
<a href='[LICENSE](https://x.com/_akhaliq/status/1937017302999851124)'><img src='https://img.shields.io/twitter/url?url=https%3A%2F%2Fx.com%2F_akhaliq%2Fstatus%2F1937017302999851124
'></a>
</div>

## Updates
* üöÄ **October 23, 2025**: Our **camera ready version** of NeurIPS 2025 is out! We fix some code minors and will update **all DnD's pretrained checkpoints and logs** [here](https://huggingface.co/datasets/Jerrylz/DnD-checkpoints-and-logs)!
* üöÄ **September 19, 2025**: **DnD** has been accepted by **NeurIPS 2025**!

## üß≠ Contents

- üé• [Quick Demo](#-customize-your-llms-wo-training-in-seconds)
- üìñ [Abstract](#abstract)
- üöÄ [Installation](#installation)
- üóÇÔ∏è [Quick Start](#quick-start)
- ü§ñ [Advanced Usage](#advanced-usage)
- üë©‚Äçüë©‚Äçüëß‚Äçüë¶ [Acknowledgement](#acknowledgment)
- üìÑ [License](#license)
- üéì [Citation](#citation)


## üé• Customize Your LLMs w/o Training in seconds!


https://github.com/user-attachments/assets/ec1ea0d1-3e1c-47b7-8c30-3623866d9369


**Explore generating your LLMs for various tasks using our [pretrained checkpoints](https://huggingface.co/datasets/Jerrylz/DnD-checkpoints-and-logs)!**


## üìñ Abstract
Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet
 still require a separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (DnD), a prompt-conditioned parameter generator
 that eliminates per-task training by mapping a handful of unlabeled task prompts
 directly to LoRA weight updates. A lightweight text encoder distills each prompt
 batch into condition embeddings, which are then transformed by a cascaded hyper
convolutional decoder into the full set of LoRA matrices. Once trained in a diverse
 collection of prompt-checkpoint pairs, DnD produces task-specific parameters in
 seconds, yielding **i)** up to 12,000√ó lower overhead than full fine-tuning, **ii)** average
 gains up to 30% in performance over the strongest training LoRAs on unseen
 common-sense reasoning, math, coding, and multimodal benchmarks, and **iii)**
 robust cross-domain generalization despite never seeing the target data or labels.
 Our results demonstrate that prompt-conditioned parameter generation is a viable
 alternative to gradient-based adaptation for rapidly specializing LLMs.


## üöÄ Installation
Before you get started, you need to set up a conda environment first.
1. Create your conda environment.
```shell
conda create -n dnd python=3.12
conda activate dnd
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124
```
2. Install dependencies for DnD.

```shell
git clone https://github.com/jerryliang24/Drag-and-Drop-LLMs.git
cd Drag-and-Drop-LLMs
bash install.sh
```


## üóÇÔ∏è Quick Start
This section covers the entire process from preparing the checkpoint dataset to training and testing the DnD model.

1. Download foundation models and sentenceBERT in the ./model folder.

```shell
hf download <model_name> --local-dir models/<model_name>
# The models you may need for DnD: Qwen/Qwen2.5-0.5/1.5/7B-Instruct, Qwen/Qwen2.5-VL-3B-Instruct, sentence-transformers/all-MiniLM-L12-v2, google-t5/t5-base
```

3. Preparing the checkpoint is laborious, so we recommend using our released LoRA adapters for training (Note that you need to first visit the website and agree the terms of our liscence).

```shell
python download_data.py --lora_type <lora_type>
# please refer to this script to specify the kind of LoRAs you want to download.
```

If you want to enjoy the process of checkpoint collection, see the [Adapt your own dataset](#adapt-your-own-dataset) section for detailed instructions.

4. Register whether you want to use wandb and your wandb key in [./workspace/main/config.json](https://github.com/jerryliang24/Drag-and-Drop-LLMs/blob/main/workspace/main/config.json).

```shell
{"use_wandb": <bool>,
"wandb_api_key":""}
```


5. Use our example scripts to train DnD model for common sense reasoning dataset ARC-c.
```shell
bash scripts/common_sense_reasoning/ARC-c/training_and_generation.sh
```

You can refer to [./scripts](https://github.com/jerryliang24/Drag-and-Drop-LLMs/tree/main/scripts) folder for a variety of experiments, generating LLMs for common sense reasoning, coding, math, and multimodal tasks.


## ü§ñ Advanced Usage


### Reproduce Section 3.4 & Section 3.5: _Ablation Studies and Explorations_

#### condition forms ablation
```shell
bash scripts/ablations/condition_forms/training.sh
bash scripts/ablations/condition_forms/generation.sh
```
This experiment explores different forms of conditions' (Prompts, Prompts+Answers, Answers) influence on DnD's performance.

#### condition extractor ablation

```shell
bash scripts/ablations/extractor_types/<type>_extractor.sh
```

This experiment explores different condition extractors' (Word2Vector, Encoder-Decoder, Decoder-Only) influence on DnD's performance. Note that you need to first download the model in ./models folder.

We also open-source other experiments' code in [./scripts/ablations](https://github.com/jerryliang24/Drag-and-Drop-LLMs/tree/main/scripts/ablations) folder, please feel free to explore!


### Adapt your own dataset
In this section, we will introduce how to train DnD on customized data-checkpoint pairs and further predict novel parameters.

<details>
<summary>Click here for details</summary>


1. Register the dataset

You first need to place your dataset file in [./prepare/data](https://github.com/jerryliang24/Drag-and-Drop-LLMs/tree/main/prepare/data) folder in .json format, and register it in  [./prepare/data/dataset_info.json](https://github.com/jerryliang24/Drag-and-Drop-LLMs/blob/main/prepare/data/dataset_info.json):

```shell
<dataset_name>:
{
  "file_name": "<dataset_name>.json",
  "columns": {"prompt":"prompt",
  "response":"response",
  "system":"system"},
```

Note that the format of your json file should be like:
```
[{ "prompt": "",
  "response": "",
  "system": ""},
  ...,
  ...
  ...,
  { "prompt": "",
  "response": "",
  "system": ""}]
```
  please refer to [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) for more details.


2. Collect checkpoints for this dataset. You need to train LLMs on previous datasets to collect relevent checkpoints, forming data-checkpoint pairs for DnD's training.

We give an example of how to modify the training script:

```diff

### model
-model_name_or_path: ../models/Qwen2.5-0.5B-Instruct
+model_name_or_path: ../models/<your desired base model>


################# line9-21 of training_your_dataset.yaml #################

-lora_rank: 8
+lora_rank: <expected rank>
lora_target: all

### dataset
-dataset: ARC-c
+dataset: <dataset_name> # should be consistent with your json file name
template: qwen
cutoff_len: 2048
-max_samples: 5000
+max_samples: <expected sample>
overwrite_cache: true
preprocessing_num_workers: 16

### output
-output_dir: saves/common_sense_reasoning/ARC-c
+output_dir: saves/<task_name>/<dataset_name>

################# line9-21 of training_your_dataset.yaml #################



################# line28-33 of training_your_dataset.yaml #################
-per_device_train_batch_size: 1
-gradient_accumulation_steps: 8
-learning_rate: 1.0e-4
-num_train_epochs: 1.0
-lr_scheduler_type: cosine
-warmup_ratio: 0.01
#you can modify the training settings
+per_device_train_batch_size:
+gradient_accumulation_steps:
+learning_rate:
+num_train_epochs:
+lr_scheduler_type:
+warmup_ratio:

################# line28-33 of training_your_dataset.yaml #################
```
- After training, you need to do the following to get checkpoint collections.
  1. You need to observe the loss curve, and decide the starting point of fine-tuning for checkpoint collection.
  2. The trainer_state.json in the checkpoint folder (usually named checkpoint-xxx) needs to be modified, setting "save_steps"=1.
  3. You can follow the scripts in [./prepare/training_scripts](https://github.com/jerryliang24/Drag-and-Drop-LLMs/tree/main/prepare/training_scripts) folder that end with "finetune" to design your fine-tuning process.
  4. After running the scripts and obtaining multiple checkpoints, you can enter the saving directory, run [./workspace/datasets/process_datasets/post_process_ckpts.sh](https://github.com/jerryliang24/Drag-and-Drop-LLMs/blob/main/workspace/datasets/process_datasets/post_process_ckpts.sh) to clean your checkpoint folder, deleting config files and rename checkpoints to ease the process of data loading.


3. Calculate importance scores for the collected checkpoints.

DnD utilizes a weighted MSE for training, assigning different importance to different layers' weights. The specific importance is calculated by the channel-wise variance and we provide scripts in [./workspace/datasets](https://github.com/jerryliang24/Drag-and-Drop-LLMs/tree/main/workspace/datasets), like : criterion_weight_for_<model_type>.py. You need to select a script and adjust it accordingly.

```diff

######################## on line 26-28 in ...<dataset_name>.py ########################
-DATASET_ROOT = "./data/common_sense_reasoning"
-CONFIG_ROOT = f"./workspace/datasets/common_sense_reasoning"
+DATASET_ROOT = "./data/<task_name>"
+CONFIG_ROOT = f"./workspace/datasets/<task_name>"
######################## on line 26-28 in ...<dataset_name>.py ########################



###################### on line 24 in ...<dataset_name>.py #######################

-dataset_tag = "ARC-c"
+dataset_tag = <your dataset_tag>

###################### on line 24 in ...<dataset_name>.py #######################




###################### on line 37 in ...<dataset_name>.py #######################

-datasets = ["ARC-e","OBQA","BoolQ","WinoGrande","PIQA","HellaSwag"]
+datasets = ["<dataset_name_1>","<dataset_name_2>",...,"<dataset_name_n>"]
# All datasets you collect for the target task

###################### on line 37 in ...<dataset_name>.py #######################

4. Create your training script. (<dataset_name> is decided by yourself. And we strongly recommend keeping this name in data registration, checkpoint collection, and DnD training consistent, since it can save much trouble.)

We use ./workspace/main/tasks/common_sense_reasoning/train_qwen0.5lora_ARC-c.py to give an example. You need to create your training script like ./workspace/main/tasks/<task_name>/train_<model_type>_<dataset_name>.py:


```diff

######################## on line 26-28 in ...<dataset_name>.py ########################
-DATASET_ROOT = "./data/common_sense_reasoning"
-CONFIG_ROOT = f"./workspace/datasets/common_sense_reasoning"
+DATASET_ROOT = "./data/<task_name>"
+CONFIG_ROOT = f"./workspace/datasets/<task_name>"
######################## on line 26-28 in ...<dataset_name>.py ########################



###################### on line 24 in ...<dataset_name>.py #######################

-dataset_tag = "ARC-c"
+dataset_tag = <your dataset_tag>

###################### on line 24 in ...<dataset_name>.py #######################




###################### on line 37 in ...<dataset_name>.py #######################

-datasets = ["ARC-e","OBQA","BoolQ","WinoGrande","PIQA","HellaSwag"]
+datasets = ["<dataset_name_1>","<dataset_name_2>",...,"<dataset_name_n>"]
# All datasets you collect for the target task

###################### on line 37 in ...<dataset_name>.py #######################


###################### on line 47 in ...<dataset_name>.py #######################

-max_text_length = xxx
+max_text_length = <The max prompt length in your dataset>

###################### on line 47 in ...<dataset_name>.py #######################



###################### on line 42-90 in ...<dataset_name>.py #######################
  config: dict[str, [float, int, str, dict]] = {
    # global setting
    "seed": SEED,
    "model_tag": os.path.basename(__file__)[:-3].split("_")[1],
    "need_test": False,
    "use_wandb": True,
    # data setting
-    "token_size": (10, 130),
+    "token_size": <suitable token size>
-    "real_length": 50,
+    "real_length": <number of checkpoints you like to use>
    "train_checkpoint_folders": [f"{DATASET_ROOT}/{dataset}" for dataset in datasets],
    "test_checkpoint_folder": "",
    "dataset_tag": dataset_tag,
    "generated_file": f"{CONFIG_ROOT}/{dataset_tag}/",
    # train setting
    "max_num_gpus": 8,
-    "batch_size": 64,
+    "batch_size": <suitable batch_size>
-    "num_workers": 8,
+    "num_workers": <suitable num_workers>
    "prefetch_factor": 1,
    "warmup_steps": 1,
-    "total_steps": 4000,
-    "learning_rate": 3e-5,
+    "total_steps": <your preferred training setting>
+    "learning_rate":
    "weight_decay": 0.1,
    "max_grad_norm": 1.0,
    "save_every": 100,
    "print_every": 20,
-    "num_texts": 128,
+    "num_texts": <suitable length of prompt batch>
    "save_folder": "./checkpoints",
    "noise_enhance": 0.0001,
    "criterion_weight": calculate_mean_criterion_weight([f"{CONFIG_ROOT}/{dataset}/criterion_weight.pt" for dataset in datasets]),
    "extractor_type":"BERT",
    "text_tokenizer":AutoTokenizer.from_pretrained(extractor),
    "extra_condition_module":
        AutoModel.from_pretrained(extractor,
        torch_dtype="auto").to(accelerator.device),
    "max_text_length":max_text_length,

-    "model_config": {
-        "features": [
-            (128, max_text_length, 384), (128, 200, 300),
-            (128, 100, 256), (256, 50, 200),
-            (512, 50, 200),
-            (1024, 25, 200), (1024, 10, 200), (2048, 10, 200),
-            (4296, 10, 130),
-        ],
-        "condition_dim": (128, max_text_length, 384),
-        "kernel_size": 9,
-    },
+     <your desired model size (the features actually denotes the shape transition of input embeddings, pretty convenient isn't it?)>
}
###################### on line 42-90 in ...<dataset_name>.py #######################
```

5. Train DnD model.
```shell
cd ./workspace/main
bash launch_multi.sh tasks/<task_name>/train_<model_type>_<dataset_name>.py <number_of_gpus>
```

Note that the adjustment of generation scripts is similar.

</details>

## üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Acknowledgment
We sincerely appreciate
Yuxiang Li,
Jiaxin Wu,
Zhiheng Chen,
Lei Feng,
Jingle Fu,
Hesen Yang,
[Bohan Zhuang](https://bohanzhuang.github.io/),
[Ziheng Qin](https://henryqin1997.github.io/ziheng_qin/),
[Zangwei Zheng](https://zhengzangw.github.io/),
[Zihan Qiu](https://www.linkedin.com/in/zihan-qiu-33a172249/),
[Zexi Li](https://zexilee.github.io/about-zexili//),
[Gongfan Fang](https://fangggf.github.io/),
[Xinyin Ma](https://horseee.github.io/),
and [Qinglin Lu](https://openreview.net/profile?id=~Qinglin_Lu2) for valuable discussions and feedbacks during this work.
<!-- This research is supported by the National Research Foundation,
Singapore under its AI Singapore Programme
(AISG Award No: AISG2-PhD-2021-08-008). -->


## üìÑ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.


## üéì Citation
```
@misc{liang2025draganddropllmszeroshotprompttoweights,
      title={Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights},
      author={Zhiyuan Liang and Dongwen Tang and Yuhao Zhou and Xuanlei Zhao and Mingjia Shi and Wangbo Zhao and Zekai Li and Peihao Wang and Konstantin Sch√ºrholt and Damian Borth and Michael M. Bronstein and Yang You and Zhangyang Wang and Kai Wang},
      year={2025},
      eprint={2506.16406},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.16406},
}
```

---

[![Star History Chart](https://api.star-history.com/svg?repos=jerryliang24/Drag-and-Drop-LLMs&type=Date)](https://star-history.com/#jerryliang24/Drag-and-Drop-LLMs&Date)

<div align="center">
  <p><strong>üåü Star us on GitHub if you find DnD helpful! üåü</strong></p>
</div>
